#############################################################################
#                                                                           #
# U.S. National Center for Health Statistics                                #
# Rmkf: R prototype of the SAS eMKF v2.4 macro                              #
# (requires R 'nimble' version >= 1.3.0; see https://r-nimble.org)          #
#                                                                           #  
# Suggested citation:                                                       #
# Talih M, Patel P, Rossen LM. An R-NIMBLE implementation of the            #
# enhanced modified Kalman filter (eMKF) tool for small domain              #
# estimation (version 2.4 2026-01-30). National Center for                  # 
# Health Statistics. 2026. https://github.com/CDCgov/eMKF.                  #
#                                                                           #
#                                                                           #  
# See the eMKF Guidance Report (DOI: 10.15620/cdc/157496) for background    #
# and how to use the SAS eMKF macro. Main differences between the R and SAS #
# implementation of the eMKF macro are the following:                       #
#                                                                           #
# - Only one outcome variable can be specified in each Rmkf call            #
# - Only Bayesian estimation is available in Rmkf; MLE option not available #
# - Only one Bayesian model can be specified in each Rmkf call              #
# - Bayesian model averaging (BMA) can only be requested via the keywords   #
#   bma_*** in the Rmkf call                                                #
# - Fully-Bayesian model (from RAND's MKF macro) is not implemented in Rmkf #
# - The slice sampler option is not currently implemented in Rmkf           #
# - Rmkf uses a reflective random walk sampler for bounded variables        #
# - Rmkf uses a target acceptance rate of 0.44 for the random walk sampler  #
#   (this is hard-coded into nimble::sampler_RW using optimalAR <- 0.44)    #
# - Rmkf implements standalone samplers for the intercepts in BMA cases     #
# - Rmkf does not reverse-transform the regression coefficients after       #
#   orthogonal polynomials are requested (default); interpret with care!    #
# - Rmkf does not require a maximum length for dataset or column names      #
# - Rmkf does not require a maximum number of groups or data points         #
# - Rmkf gives its user the option to calculate the HPD credible intervals  #
#   for all scalar variables, in addition to their posterior means and SDs  #
# - Rmkf does not implement graphical diagnostics directly; however, users  #
#   can call 'coda' diagnostic plots on the saved posterior sample;         #
#   see examples provided in emkf_macro_v24_tutorial.R                      #
#                                                                           #  
#                                                                           #  
# Last modified: 30-Jan-2025                                                #
# Makram Talih, Ph.D.                                                       #
#                                                                           #
#############################################################################

##################################################################
# Check for required packages and attach 'nimble' library...     #
# Actual package installation depends on user's environment and  #
# is assumed to have been completed prior to sourcing this file. #
##################################################################

if (!is.element("nimble", installed.packages()[, "Package"])) {
  stop("Error: Please install 'nimble' version 1.3.0 or later.")
} else {
  if (packageVersion("nimble") < "1.3.0") {
    stop("Error: Please install 'nimble' version 1.3.0 or later.")
  }
}
if (!is.element("coda", installed.packages()[, "Package"])) {
  stop("Error: Please install the 'coda' package.")
}
if (!is.element("msm", installed.packages()[, "Package"])) {
  stop("Error: Please install the 'msm' package.")
}
if (!is.element("stargazer", installed.packages()[, "Package"])) {
  stop("Error: Please install the 'stargazer' package.")
}

library(nimble)

#################################################################################
# eMKF: R nimble functions to calculate AR(1) covariance matrix and its inverse #
#################################################################################

# Create an AR(1) covariance matrix from input vector of timepoints
ar1_cov_matrix <- nimbleFunction(
  run = function(rtimes = double(1), 
                 rhoval = double(0, default = 0),
                 nuval = double(0, default = 1)) {
    returnType(double(2))
    
    j <- 0L
    k <- 0L
    jp1 <- 0L
    ntsm1 <- 0L
    nts <- 0L
    nts <- length(rtimes)
    covmat <- nimMatrix(nrow = nts, ncol = nts, init = FALSE)
    
    if (nts > 0) {
      for (j in 1:nts) {
        covmat[j,j] <- nuval
      }
    }
    if (nts > 1) {
      if (any_na(rtimes)) {
        nimStop("Cannot construct AR(1) covariance matrix of dimension 2 or more with missing timepoints.")
      }
      ntsm1 <- nts - 1
      for (j in 1:ntsm1) {
        jp1 <- j + 1
        for (k in jp1:nts) {
          covmat[j,k] <- nuval*pow(rhoval, rtimes[k]-rtimes[j])
          covmat[k,j] <- covmat[j,k]
        }
      }
    } 
    return(covmat)
  } 
) # ar1_cov_matrix

# Calculate the tri-diagonal inverse of an AR(1) covariance matrix from input vector of timepoints
ar1_inv_matrix <- nimbleFunction(
  run = function(rtimes = double(1), 
                 rhoval = double(0, default = 0),
                 nuval = double(0, default = 1)) {
    returnType(double(2))
    
    j <- 0L
    ntsm1 <- 0L
    nts <- 0L
    nts <- length(rtimes)
    imat <- nimMatrix(0, nrow = nts, ncol = nts, init = TRUE) # zero matrix
    
    if (nts == 0) {
      nimStop("Inverse of 0-dimensional matrix is undefined.")
    }  
    if (nts == 1) {
      imat[1,1] <- 1/nuval
    } else {
      ntsm1 <- nts - 1
      if (any_na(rtimes)) {
        nimStop("Cannot evaluate inverse of AR(1) covariance matrix of dimension 2 or more with missing timepoints.")
      }
      # First diagonal entry
      imat[1,1] <- 1/((1-pow(rhoval, 2*(rtimes[2]-rtimes[1])))*nuval)
      # Intermediate diagonal entries
      if (nts > 2) {
        for (j in 2:ntsm1) {
          imat[j,j] <- (1-pow(rhoval, 2*(rtimes[j+1]-rtimes[j-1])))/((1-pow(rhoval, 2*(rtimes[j+1]-rtimes[j])))*(1-pow(rhoval, 2*(rtimes[j]-rtimes[j-1])))*nuval) 
        } 
      }
      # Last diagonal entry
      imat[nts,nts] <- 1/((1-pow(rhoval, 2*(rtimes[nts]-rtimes[nts-1])))*nuval)
      # Nonzero off-diagonal entries
      for (j in 1:ntsm1) {
        imat[j,j+1] <- -pow(rhoval, rtimes[j+1]-rtimes[j])/((1-pow(rhoval, 2*(rtimes[j+1]-rtimes[j])))*nuval)
        imat[j+1,j] <- imat[j,j+1] 
      }
    }
    return(imat)
  }   
) # ar1_inv_matrix

# Calculate the upper triangular Cholesky factor for an AR(1) covariance matrix from input vector of timepoints
ar1_chl_matrix <- nimbleFunction(
  run = function(rtimes = double(1),
                 rhoval = double(0, default = 0),
                 nuval = double(0, default = 1)) {
    returnType(double(2))

    j <- 0L
    k <- 0L
    nts <- 0L
    nts <- length(rtimes)
    cmat <- nimMatrix(0, nrow = nts, ncol = nts, init = TRUE) # zero matrix

    if (nts == 0) {
      nimStop("Cholesky decomposition of 0-dimensional matrix is undefined.")
    }
    if (nts == 1) {
      cmat[1,1] <- sqrt(nuval)
    } else {
      if (any_na(rtimes)) {
        nimStop("Cannot evaluate Cholesky decomposition of AR(1) covariance matrix of dimension 2 or more with missing timepoints.")
      }
      # first row
      for (j in 1:nts) {
        cmat[1,j] <- sqrt(nuval)*pow(rhoval, rtimes[j]-rtimes[1])
      }
      # subsequent rows
      for (j in 2:nts) {
        for (k in j:nts) {
          cmat[j,k] <- sqrt(nuval)*pow(rhoval, rtimes[k]-rtimes[j])*sqrt(1-pow(rhoval, 2*(rtimes[j]-rtimes[j-1])))
        }
      }
    }
    return(cmat)
  }
) # ar1_chl_matrix

# Check determinant of AR(1) covariance matrix is neither NaN nor negative or zero
ar1_det_check <- nimbleFunction(
  run = function(rtimes = double(1),
                 rhoval = double(0, default = 0),
                 nuval = double(0, default = 1)) {
    returnType(logical(0))
    
    j <- 0L
    nts <- 0L
    nts <- length(rtimes)
    is_ok <- FALSE
    
    if (is.na(rhoval)) {
      return(nts <= 1)
    } else {
      if (abs(rhoval) < 1) {
        logDet_value <- nts*log(nuval)
        if (nts > 1) {
          for (j in 2:nts) {
            logDet_value <- logDet_value + log1p(-pow(rhoval, 2*(rtimes[j] - rtimes[j-1])))
          }
        }
        is_ok <- !is.na(logDet_value)
      }
      if (!is_ok) {
        return(is_ok)
      } else {
        return(exp(logDet_value) > 0)
      }
    }
  }
) # ar1_det_check

##################################################################################
# eMKF: Function to build a polynomial design matrix X from vector of timepoints #
##################################################################################

polyDesignMatrix <- nimbleFunction(
  run = function(rtimes = double(1), 
                 xpos = integer(0, default = 0),
                 xmod = integer(0, default = 0),
                 degree = integer(0, default = 3), 
                 orpol = integer(0, default = 0)) {
    returnType(double(2))
    
    # Initial declarations
    j <- 0L
    k <- 0L
    pts <- 0L
    pts1 <- 0L
    pts2 <- 0L
    nseg1 <- 0L
    nseg2 <- 0L
    nts <- 0L
    nts <- length(rtimes)

    # Error checks (!! Interpreting 'xpos' as the starting index for segment 2 !!)
    if (degree < 0) {
      nimStop("'degree' must be a nonnegative integer.")
    } 
    if (any_na(rtimes)) {
      nimStop("Cannot construct polynomial design matrix with missing timepoints.")
    }
    if ((xpos > 1 & xpos <= nts) & (xmod < 0 | xmod > 2)) {
      nimStop("Trend break specification should be either 'xmod = 0' (default) for no break, 'xmod = 1' for a level-shift, or 'xmod = 2' for a full break.")
    }
    if ((xpos <= 1 | xpos > nts) & (xmod > 0 & xmod <= 2)) {
      nimPrint("Warning: no valid breakpoint is specified in 'polyDesignMatrix'; trend break specification will be ignored.")
      xmod <- 0L
    }
    
    # Dimensionality
    if (xmod == 0) {
      if (degree >= nts) {
        nimStop("Number of timepoints must be larger than 'degree' to avoid computationally singular X'X matrix.")
      }
      pts <- degree + 1
    } else {
      nseg1 <- xpos - 1
      nseg2 <- nts - nseg1
      if (degree >= nseg1 | nseg2 < 2) {
        nimStop("To avoid a computationally singular X'X matrix, the number of timepoints in segment 1 must be larger than 'degree', and there must be at least 2 timepoints in segment 2.")        
      }
      if (xmod == 1) {
        pts <- degree + 2
      } else {
        pts1 <- degree + 1
        pts2 <-  min(pts1, nseg2 - 1)
        pts <- pts1 + pts2
      }
    }
    
    # Matrix allocation
    Xmat <- nimMatrix(nrow = nts, ncol = pts, init = FALSE)
    tXmat <- nimMatrix(nrow = pts, ncol = nts, init = FALSE)
    tXX <- nimMatrix(nrow = pts, ncol = pts, init = FALSE)
    cXX <- nimMatrix(nrow = pts, ncol = pts, init = FALSE)
    iXX <- nimMatrix(nrow = pts, ncol = pts, init = FALSE)
    XiXX <- nimMatrix(nrow = nts, ncol = pts, init = FALSE)
    
    if (xmod == 0) {
      for (k in 1:pts) {
        for (j in 1:nts) {
          Xmat[j,k] <- pow(rtimes[j], k-1)
        }
      }
    } else {
      if (xmod == 1) {
        for (j in 1:nseg1) {
          Xmat[j,1] <- 1
          Xmat[j,2] <- 0
        }
        for (j in xpos:nts) {
          Xmat[j,1] <- 0
          Xmat[j,2] <- 1
        }
        if (pts > 2) {
          for (k in 3:pts) {
            for (j in 1:nts) {
              Xmat[j,k] <- pow(rtimes[j], k-2)
            }
          }
        }
      } else {
        for (k in 1:pts1) {
          for (j in 1:nseg1) {
            Xmat[j,k] <- pow(rtimes[j], k-1)
          }
          for (j in xpos:nts) {
            Xmat[j,k] <- 0
          }
        }
        for (k in 1:pts2) {
          for (j in 1:nseg1) {
            Xmat[j,k+pts1] <- 0
          }
          for (j in xpos:nts) {
            Xmat[j,k+pts1] <- pow(rtimes[j], k-1)
          }
        }
      }
    }
    if (orpol) { # orthogonal transformation: individual steps for C++ compilation
      tXmat <- t(Xmat)
      tXX <- tXmat %*% Xmat
      cXX <- chol(tXX)
      iXX <- inverse(cXX)
      XiXX <- Xmat %*% iXX
      Xmat <- XiXX
    }
    return(Xmat)
  }
) # polyDesignMatrix

##########################################################################
# eMKF: Custom spike-and-slab distributions for use in NIMBLE model code #
##########################################################################

# Prior density function for cubic coefficients conditional on model flag
dmixbeta3 <- nimbleFunction(
  run = function(x = double(1), 
                 lg = integer(0), 
                 pmn = double(0, default = 0), 
                 psd = double(0, default = 1), 
                 ind = integer(0), 
                 bma = integer(0, default = 3), 
                 SINGDEN = double(0, default = 0),
                 NUMINF = double(0, default = Inf),
                 log = integer(0, default = 0)) {
    returnType(double(0))

    i <- 0L
    lx <- 0L
    lx <- length(x)
    lgp1 <- 0L
    lgp1 <- lg + 1
    xtemp <- 0
    xmean <- 0
    logLike <- 0
    
    if (SINGDEN > 1e-6 | SINGDEN < 0) {
      nimStop("'SINGDEN' should be a nonnegative value no greater than 1e-11.")
    }
    if (NUMINF < 1e10) {
      nimStop("'NUMINF' should be greater than or equal to 1e15.")
    }
    if (lx <= 1) {
      nimStop("Length of 'x' should be at least 2.")
    } else {
      if (lg != (lx - 1)) {
        nimStop("Parameter 'lg' is misspecified; it should be equal to length(x) - 1.")
      } else {
        if (bma > 3 | bma < 1) {
          nimStop("Input 'bma' value must be either 3 for cubic_bma, 2 for quad_bma, or 1 for linear_bma.")
        } else {
          if (bma == 3 & (ind > 7 | ind < 1)) {
            nimStop("Model flag 'ind' in cubic_bma case must be an integer between 1 and 7.")
          } else {
            if (bma == 2 & (ind > 5 | ind < 1)) {
              nimStop("Model flag 'ind' in quad_bma case must be an integer between 1 and 5.")
            } else {
              if (bma == 1 & (ind > 3 | ind < 1)) {
                nimStop("Model flag 'ind' in linear_bma case must be an integer between 1 and 3.")
              }
            }
          }
        }
      }
    }
    
    if (bma == 3) {
      if (ind == 1) {
        for (i in 1:lg) {
          xtemp <- x[i]
          logLike <- logLike + dnorm(xtemp, pmn, sd = psd, log = TRUE)
          xmean <- xmean + xtemp
        }
        xmean <- xmean/lg
        if (abs(x[lgp1] - xmean) > SINGDEN) {logLike <- logLike - NUMINF}
      } else {
        if (ind == 2 | ind == 3 | ind == 7) {
          for (i in 1:lg) {
            if (abs(x[i] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
          }
          if (abs(x[lgp1] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
        } else {
          if (ind == 4) {
            xtemp <- x[lgp1]
            logLike <- logLike + dnorm(xtemp, pmn, sd = psd, log = TRUE)
            for (i in 1:lg) {
              if (abs(x[i] - x[lgp1]) > SINGDEN) {logLike <- logLike - NUMINF}
            }
          } else {
            if (abs(x[lgp1] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
            for (i in 1:lg) {
              if (abs(x[i] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
            }
          }
        }
      }
    } else {
      nimStop("Density function 'dmixbeta3' is undefined for 'bma' values 1 or 2.")
    }
    
    if (log) {
      return(logLike)
    } else {
      return(exp(logLike))
    }
  }
) # dmixbeta3

# Random generation from prior for cubic coefficients conditional on model flag
rmixbeta3 <- nimbleFunction(
  run = function(n = integer(0), 
                 lg = integer(0), 
                 pmn = double(0, default = 0), 
                 psd = double(0, default = 1), 
                 ind = integer(0), 
                 bma = integer(0, default = 3),
                 SINGDEN = double(0, default = 0),
                 NUMINF = double(0, default = Inf)) {
    returnType(double(1))
    
    i <- 0L
    lgp1 <- 0L
    lgp1 <- lg + 1
    x <- nimNumeric(lgp1, init = FALSE)
    xmean <- 0
    
    if (SINGDEN > 1e-6 | SINGDEN < 0) {
      nimStop("'SINGDEN' should be a nonnegative value no greater than 1e-11.")
    }
    if (NUMINF < 1e10) {
      nimStop("'NUMINF' should be greater than or equal to 1e15.")
    }
    if (lg < 1) {
      nimStop("There should be at least one group to work with (i.e., 'lg' = 1+).")
    } else {
      if (bma > 3 | bma < 1) {
        nimStop("Input 'bma' value must be either 3 for cubic_bma, 2 for quad_bma, or 1 for linear_bma.")
      } else {
        if (bma == 3 & (ind > 7 | ind < 1)) {
          nimStop("Model flag 'ind' in cubic_bma case must be an integer between 1 and 7.")
        } else {
          if (bma == 2 & (ind > 5 | ind < 1)) {
            nimStop("Model flag 'ind' in quad_bma case must be an integer between 1 and 5.")
          } else {
            if (bma == 1 & (ind > 3 | ind < 1)) {
              nimStop("Model flag 'ind' in linear_bma case must be an integer between 1 and 3.")
            }
          }
        }
      }
    }
    if (n != 1) {
      nimPrint("Warning: 'rmixbeta3' only allows n = 1; using n = 1.")
    }
    
    if (bma == 3) {
      if (ind == 1) {
        for (i in 1:lg) {
          x[i] <- rnorm(1, pmn, sd = psd)
          xmean <- xmean + x[i]
        }
        xmean <- xmean/lg
        x[lgp1] <- xmean
      } else {
        if (ind == 2 | ind == 3 | ind == 7) {
          for (i in 1:lg) {
            x[i] <- 0
          }
          x[lgp1] <- 0
        } else {
          if (ind == 4) {
            x[lgp1] <- rnorm(1, pmn, sd = psd)
            for (i in 1:lg) {
              x[i] <- x[lgp1]
            }
          } else {
            x[lgp1] <- 0
            for (i in 1:lg) {
              x[i] <- 0
            }
          }
        }
      }
    } else {
      nimStop("Random generation function 'rmixbeta3' is undefined for 'bma' values 1 or 2.")
    }
    
    return(x)
  }
) # rmixbeta3

# Prior density function for quadratic coefficients conditional on model flag
dmixbeta2 <- nimbleFunction(
  run = function(x = double(1), 
                 lg = integer(0), 
                 pmn = double(0, default = 0), 
                 psd = double(0, default = 1), 
                 ind = integer(0), 
                 bma = integer(0, default = 3), 
                 SINGDEN = double(0, default = 0),
                 NUMINF = double(0, default = Inf),
                 log = integer(0, default = 0)) {
    returnType(double(0))

    i <- 0L
    lx <- 0L
    lx <- length(x)
    lgp1 <- 0L
    lgp1 <- lg + 1
    xtemp <- 0
    xmean <- 0
    logLike <- 0
    
    if (SINGDEN > 1e-6 | SINGDEN < 0) {
      nimStop("'SINGDEN' should be a nonnegative value no greater than 1e-11.")
    }
    if (NUMINF < 1e10) {
      nimStop("'NUMINF' should be greater than or equal to 1e15.")
    }
    if (lx <= 1) {
      nimStop("Length of 'x' should be at least 2.")
    } else {
      if (lg != (lx - 1)) {
        nimStop("Parameter 'lg' is misspecified; it should be equal to length(x) - 1.")
      } else {
        if (bma > 3 | bma < 1) {
          nimStop("Input 'bma' value must be either 3 for cubic_bma, 2 for quad_bma, or 1 for linear_bma.")
        } else {
          if (bma == 3 & (ind > 7 | ind < 1)) {
            nimStop("Model flag 'ind' in cubic_bma case must be an integer between 1 and 7.")
          } else {
            if (bma == 2 & (ind > 5 | ind < 1)) {
              nimStop("Model flag 'ind' in quad_bma case must be an integer between 1 and 5.")
            } else {
              if (bma == 1 & (ind > 3 | ind < 1)) {
                nimStop("Model flag 'ind' in linear_bma case must be an integer between 1 and 3.")
              }
            }
          }
        }
      }
    }
    
    if (bma == 3) {
      if (ind == 1 | ind == 2) {
        for (i in 1:lg) {
          xtemp <- x[i]
          logLike <- logLike + dnorm(xtemp, pmn, sd = psd, log = TRUE)
          xmean <- xmean + xtemp
        }
        xmean <- xmean/lg
        if (abs(x[lgp1] - xmean) > SINGDEN) {logLike <- logLike - NUMINF}
      } else {
        if (ind == 3 | ind == 7) {
          for (i in 1:lg) {
            if (abs(x[i] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
          }
          if (abs(x[lgp1] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
        } else {
          if (ind == 4 | ind == 5) {
            xtemp <- x[lgp1]
            logLike <- logLike + dnorm(xtemp, pmn, sd = psd, log = TRUE)
            for (i in 1:lg) {
              if (abs(x[i] - x[lgp1]) > SINGDEN) {logLike <- logLike - NUMINF}
            }
          } else {
            if (abs(x[lgp1] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
            for (i in 1:lg) {
              if (abs(x[i] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
            }
          }
        }
      }
    } else {
      if (bma == 2) {
        if (ind == 1) {
          for (i in 1:lg) {
            xtemp <- x[i]
            logLike <- logLike + dnorm(xtemp, pmn, sd = psd, log = TRUE)
            xmean <- xmean + xtemp
          }
          xmean <- xmean/lg
          if (abs(x[lgp1] - xmean) > SINGDEN) {logLike <- logLike - NUMINF}
        } else {           
          if (ind == 2 | ind == 5) {
            for (i in 1:lg) {
              if (abs(x[i] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
            }
            if (abs(x[lgp1] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
          } else {
            if (ind == 3) {
              xtemp <- x[lgp1]
              logLike <- logLike + dnorm(xtemp, pmn, sd = psd, log = TRUE)
              for (i in 1:lg) {
                if (abs(x[i] - x[lgp1]) > SINGDEN) {logLike <- logLike - NUMINF}
              }
            } else {
              if (abs(x[lgp1] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
              for (i in 1:lg) {
                if (abs(x[i] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
              }
            }
          }
        }
      } else {
        nimStop("Density function 'dmixbeta2' is undefined for 'bma' value 1.")
      }
    }
    
    if (log) {
      return(logLike)
    } else {
      return(exp(logLike))
    }
  }
) # dmixbeta2

# Random generation from prior for quadratic coefficients conditional on model flag
rmixbeta2 <- nimbleFunction(
  run = function(n = integer(0), 
                 lg = integer(0), 
                 pmn = double(0, default = 0), 
                 psd = double(0, default = 1), 
                 ind = integer(0), 
                 bma = integer(0, default = 3),
                 SINGDEN = double(0, default = 0),
                 NUMINF = double(0, default = Inf)) {
    returnType(double(1))
    
    i <- 0L
    lgp1 <- 0L
    lgp1 <- lg + 1
    x <- nimNumeric(lgp1, init = FALSE)
    xmean <- 0
    
    if (SINGDEN > 1e-6 | SINGDEN < 0) {
      nimStop("'SINGDEN' should be a nonnegative value no greater than 1e-11.")
    }
    if (NUMINF < 1e10) {
      nimStop("'NUMINF' should be greater than or equal to 1e15.")
    }
    if (lg < 1) {
      nimStop("There should be at least one group to work with (i.e., 'lg' = 1+).")
    } else {
      if (bma > 3 | bma < 1) {
        nimStop("Input 'bma' value must be either 3 for cubic_bma, 2 for quad_bma, or 1 for linear_bma.")
      } else {
        if (bma == 3 & (ind > 7 | ind < 1)) {
          nimStop("Model flag 'ind' in cubic_bma case must be an integer between 1 and 7.")
        } else {
          if (bma == 2 & (ind > 5 | ind < 1)) {
            nimStop("Model flag 'ind' in quad_bma case must be an integer between 1 and 5.")
          } else {
            if (bma == 1 & (ind > 3 | ind < 1)) {
              nimStop("Model flag 'ind' in linear_bma case must be an integer between 1 and 3.")
            }
          }
        }
      }
    }
    if (n != 1) {
      nimPrint("Warning: 'rmixbeta2' only allows n = 1; using n = 1.")
    }
    
    if (bma == 3) {
      if (ind == 1 | ind == 2) {
        for (i in 1:lg) {
          x[i] <- rnorm(1, pmn, sd = psd)
          xmean <- xmean + x[i]
        }
        xmean <- xmean/lg
        x[lgp1] <- xmean
      } else {
        if (ind == 3 | ind == 7) {
          for (i in 1:lg) {
            x[i] <- 0
          }
          x[lgp1] <- 0
        } else {
          if (ind == 4 | ind == 5) {
            x[lgp1] <- rnorm(1, pmn, sd = psd)
            for (i in 1:lg) {
              x[i] <- x[lgp1]
            }
          } else {
            x[lgp1] <- 0
            for (i in 1:lg) {
              x[i] <- 0
            }
          }
        }
      }
    } else {
      if (bma == 2) {
        if (ind == 1) {
          for (i in 1:lg) {
            x[i] <- rnorm(1, pmn, sd = psd)
            xmean <- xmean + x[i]
          }
          xmean <- xmean/lg
          x[lgp1] <- xmean
        } else {
          if (ind == 2 | ind == 5) {
            for (i in 1:lg) {
              x[i] <- 0
            }
            x[lgp1] <- 0
          } else {
            if (ind == 3) {
              x[lgp1] <- rnorm(1, pmn, sd = psd)
              for (i in 1:lg) {
                x[i] <- x[lgp1]
              }
            } else {
              x[lgp1] <- 0
              for (i in 1:lg) {
                x[i] <- 0
              }
            }
          }
        }
      } else {
        nimStop("Random generation function 'rmixbeta2' is undefined for 'bma' value 1.")
      } 
    }
    
    return(x)
  }
) # rmixbeta2

# Prior density function for linear coefficients conditional on model flag
dmixbeta1 <- nimbleFunction(
  run = function(x = double(1), 
                 lg = integer(0), 
                 pmn = double(0, default = 0), 
                 psd = double(0, default = 1), 
                 ind = integer(0), 
                 bma = integer(0, default = 3), 
                 SINGDEN = double(0, default = 0),
                 NUMINF = double(0, default = Inf),
                 log = integer(0, default = 0)) {
    returnType(double(0))

    i <- 0L
    lx <- 0L
    lx <- length(x)
    lgp1 <- 0L
    lgp1 <- lg + 1
    xtemp <- 0
    xmean <- 0
    logLike <- 0
    
    if (SINGDEN > 1e-6 | SINGDEN < 0) {
      nimStop("'SINGDEN' should be a nonnegative value no greater than 1e-11.")
    }
    if (NUMINF < 1e10) {
      nimStop("'NUMINF' should be greater than or equal to 1e15.")
    }
    if (lx <= 1) {
      nimStop("Length of 'x' should be at least 2.")
    } else {
      if (lg != (lx - 1)) {
        nimStop("Parameter 'lg' is misspecified; it should be equal to length(x) - 1.")
      } else {
        if (bma > 3 | bma < 1) {
          nimStop("Input 'bma' value must be either 3 for cubic_bma, 2 for quad_bma, or 1 for linear_bma.")
        } else {
          if (bma == 3 & (ind > 7 | ind < 1)) {
            nimStop("Model flag 'ind' in cubic_bma case must be an integer between 1 and 7.")
          } else {
            if (bma == 2 & (ind > 5 | ind < 1)) {
              nimStop("Model flag 'ind' in quad_bma case must be an integer between 1 and 5.")
            } else {
              if (bma == 1 & (ind > 3 | ind < 1)) {
                nimStop("Model flag 'ind' in linear_bma case must be an integer between 1 and 3.")
              }
            }
          }
        }
      }
    }
    
    if (bma == 3) {
      if (ind == 1 | ind == 2 | ind == 3) {
        for (i in 1:lg) {
          xtemp <- x[i]
          logLike <- logLike + dnorm(xtemp, pmn, sd = psd, log = TRUE)
          xmean <- xmean + xtemp
        }
        xmean <- xmean/lg
        if (abs(x[lgp1] - xmean) > SINGDEN) {logLike <- logLike - NUMINF}
      } else {
        if (ind == 7) {
          for (i in 1:lg) {
            if (abs(x[i] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
          }
          if (abs(x[lgp1] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
        } else {
          xtemp <- x[lgp1]
          logLike <- logLike + dnorm(xtemp, pmn, sd = psd, log = TRUE)
          for (i in 1:lg) {
            if (abs(x[i] - x[lgp1]) > SINGDEN) {logLike <- logLike - NUMINF}
          }
        }
      }
    } else {
      if (bma == 2) {
        if (ind == 1 | ind == 2) {
          for (i in 1:lg) {
            xtemp <- x[i]
            logLike <- logLike + dnorm(xtemp, pmn, sd = psd, log = TRUE)
            xmean <- xmean + xtemp
          }
          xmean <- xmean/lg
          if (abs(x[lgp1] - xmean) > SINGDEN) {logLike <- logLike - NUMINF}
        } else {
          if (ind == 5) {
            for (i in 1:lg) {
              if (abs(x[i] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
            }
            if (abs(x[lgp1] - 0) > SINGDEN) {logLike <-  logLike - NUMINF}
          } else {
            xtemp <- x[lgp1]
            logLike <- logLike + dnorm(xtemp, pmn, sd = psd, log = TRUE)
            for (i in 1:lg) {
              if (abs(x[i] - x[lgp1]) > SINGDEN) {logLike <- logLike - NUMINF}
            }
          }
        }
      } else {
        if (ind == 1) {
          for (i in 1:lg) {
            xtemp <- x[i]
            logLike <- logLike + dnorm(xtemp, pmn, sd = psd, log = TRUE)
            xmean <- xmean + xtemp
          }
          xmean <- xmean/lg
          if (abs(x[lgp1] - xmean) > SINGDEN) {logLike <- logLike - NUMINF}
        } else {
          if (ind == 3) {
            for (i in 1:lg) {
              if (abs(x[i] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
            }
            if (abs(x[lgp1] - 0) > SINGDEN) {logLike <- logLike - NUMINF}
          } else {
            xtemp <- x[lgp1]
            logLike <- logLike + dnorm(xtemp, pmn, sd = psd, log = TRUE)
            for (i in 1:lg) {
              if (abs(x[i] - x[lgp1]) > SINGDEN) {logLike <- logLike - NUMINF}
            }
          }
        }
      }
    }
    
    if (log) {
      return(logLike)
    } else {
      return(exp(logLike))
    }
  }
) # dmixbeta1

# Random generation from prior for linear coefficients conditional on model flag
rmixbeta1 <- nimbleFunction(
  run = function(n = integer(0), 
                 lg = integer(0), 
                 pmn = double(0, default = 0), 
                 psd = double(0, default = 1), 
                 ind = integer(0), 
                 bma = integer(0, default = 3),
                 SINGDEN = double(0, default = 0),
                 NUMINF = double(0, default = Inf)) {
    returnType(double(1))
    
    i <- 0L
    lgp1 <- 0L
    lgp1 <- lg + 1
    x <- nimNumeric(lgp1, init = FALSE)
    xmean <- 0
    
    if (SINGDEN > 1e-6 | SINGDEN < 0) {
      nimStop("'SINGDEN' should be a nonnegative value no greater than 1e-11.")
    }
    if (NUMINF < 1e10) {
      nimStop("'NUMINF' should be greater than or equal to 1e15.")
    }
    if (lg < 1) {
      nimStop("There should be at least one group to work with (i.e., 'lg' = 1+).")
    } else {
      if (bma > 3 | bma < 1) {
        nimStop("Input 'bma' value must be either 3 for cubic_bma, 2 for quad_bma, or 1 for linear_bma.")
      } else {
        if (bma == 3 & (ind > 7 | ind < 1)) {
          nimStop("Model flag 'ind' in cubic_bma case must be an integer between 1 and 7.")
        } else {
          if (bma == 2 & (ind > 5 | ind < 1)) {
            nimStop("Model flag 'ind' in quad_bma case must be an integer between 1 and 5.")
          } else {
            if (bma == 1 & (ind > 3 | ind < 1)) {
              nimStop("Model flag 'ind' in linear_bma case must be an integer between 1 and 3.")
            }
          }
        }
      }
    }
    if (n != 1) {
      nimPrint("Warning: 'rmixbeta1' only allows n = 1; using n = 1.")
    }
    
    if (bma == 3) {
      if (ind == 1 | ind == 2 | ind == 3) {
        for (i in 1:lg) {
          x[i] <- rnorm(1, pmn, sd = psd)
          xmean <- xmean + x[i]
        }
        xmean <- xmean/lg
        x[lgp1] <- xmean
      } else {
        if (ind == 7) {
          for (i in 1:lg) {
            x[i] <- 0
          }
          x[lgp1] <- 0
        } else {
          x[lgp1] <- rnorm(1, pmn, sd = psd)
          for (i in 1:lg) {
            x[i] <- x[lgp1]
          }
        }
      }
    } else {
      if (bma == 2) {
        if (ind == 1 | ind == 2) {
          for (i in 1:lg) {
            x[i] <- rnorm(1, pmn, sd = psd)
            xmean <- xmean + x[i]
          }
          xmean <- xmean/lg
          x[lgp1] <- xmean
        } else {
          if (ind == 5) {
            for (i in 1:lg) {
              x[i] <- 0
            }
            x[lgp1] <- 0
          } else {
            x[lgp1] <- rnorm(1, pmn, sd = psd)
            for (i in 1:lg) {
              x[i] <- x[lgp1]
            }
          }
        }
      } else {
        if (ind == 1) {
          for (i in 1:lg) {
            x[i] <- rnorm(1, pmn, sd = psd)
            xmean <- xmean + x[i]
          }
          xmean <- xmean/lg
          x[lgp1] <- xmean
        } else {
          if (ind == 3) {
            for (i in 1:lg) {
              x[i] <- 0
            }
            x[lgp1] <- 0
          } else {
            x[lgp1] <- rnorm(1, pmn, sd = psd)
            for (i in 1:lg) {
              x[i] <- x[lgp1]
            }
          }
        }
      }
    }
    
    return(x)
  }
) # rmixbeta1

############################################################
# eMKF: Custom Gibbs sampler for random sampling variances #
############################################################

sampler_RP <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Make sure the 'target' value is correctly specified
    if (lt != 1) {
      nimStop("Only the 'varr' model node can be sampled using 'sampler_RP'.")
    } else {
      if (target[1] != 'varr') {
        nimStop("Target node must be the vector (length g) of variance parameters 'varr'.")
      }
    }
    
    # Get constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n

    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Narr <- model[['Narr']]
    S2arr <- model[['S2arr']]
    vshape <- model[['vshape']] 
    vscale <- model[['vscale']] 
    
    # Node values to be replaced
    varr <- model[['varr']]
    
    i <- 0L
    j <- 0L
    ivshape <- 0
    ivscale <- 0
    ivrate <- 0
    
    for (i in 1:g) {
      ivshape <- vshape
      ivscale <- vscale
      for (j in 1:n) {
        ivshape <- ivshape + (Narr[(i-1)*n+j]-1)/2
        ivscale <- ivscale + (Narr[(i-1)*n+j]-1)*S2arr[(i-1)*n+j]/2
      }
      ivrate <- 1/ivscale  # rate is the canonical parameter in NIMBLE for dinvgamma
      varr[i] <- rinvgamma(1, ivshape, rate=ivrate) 
    }
    
    # Update model with new node values
    model[['varr']] <<- varr
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_RP 

#########################################################
# eMKF: Custom Gibbs sampler for true state predictions #
#########################################################

sampler_EP <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {

    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Make sure the 'target' value is correctly specified
    if (lt != 1) {
      nimStop("Only the 'etaarr' model node can be sampled using 'sampler_EP'.")
    } else {
      if (target[1] != 'etaarr') {
        nimStop("Target node must be the vector (length g*n) of true states 'etaarr'.")
      }
    }
    
    # Get constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    rts <- model$getConstants()$rts
    
    # Pre-allocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    Xbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    Wgamma <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    petas <- nimMatrix(nrow=n, ncol=1L, init = FALSE)  
    yetas <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    etas <- nimMatrix(nrow=n, ncol=1L, init = FALSE)  
    EE <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    EI <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    tEI <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {

    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    
    # Get current regression predictions and AR(1) parameters (draws are conditional on these values)
    etamnarr <- model[['etamnarr']]
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Nodes values to be replaced
    etaarr <- model[['etaarr']]
    
    # Working variables
    i <- 0L
    j <- 0L
    irho <- 0
    inu <- 0
    
    for (i in 1:g) {                                                # cycle through each group independently
      for (j in 1:n) {
        Xbeta[j,1] <<- etamnarr[(i-1)*n+j]                          # predictions from regression for group k
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Wgamma <<- ar1_inv_matrix(rts, irho, inu)                     # Wgamma = Vgamma^{-1} is a tridiagonal matrix
      petas <<- Wgamma %*% Xbeta                                    # contribution to posterior mean from random effects prior
      Vg <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        Vg[j,j] <<- 1/S2arr[(i-1)*n+j]                              # populate diagonal entries of Vg using sampling precisions
      }
      yetas <<- Vg %*% Yvec                                         # contribution to posterior mean from data
      petas <<- yetas + petas                                       # sum of prior and data contributions
      Wg <<- Wgamma + Vg                                            # precision matrix Wg = sum of Wgamma and diagonal of sampling precisions
      for (j in 1:n) {
        etas[j,1] <<- rnorm(1, 0, sd=1)                             # sample from univariate standard normal
      }
      EE <<- chol(Wg)                                               # Cholesky decomposition for precision matrix (R returns upper triangular)
      EI <<- inverse(EE)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tEI <<- t(EI)                                                 # transpose
      petas <<- tEI %*% petas                                       # rescale
      etas <<- petas + etas                                         # center
      etas <<- EI %*% etas                                          # rescale
      for (j in 1:n) {
        etaarr[(i-1)*n+j] <- etas[j,1]
      }
    }
    
    # Update model with new node values
    model[['etaarr']] <<- etaarr
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_EP 

#########################################################################################
# eMKF: Custom Gibbs samplers for regression coefficients in the supported trend models #
#########################################################################################

# Custom Gibbs sampler for regression coefficients in the group-specific cubic trend model
sampler_CP_bgc <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)

    # Overall dimensionality (p == 4 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 4 | lt != p) {
      nimStop("The intercepts and three sets of regression coefficients are all expected to be sampled in 'sampler_CP_bgc'.")
    } else {
      if (target[1] != 'ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 'ag'.")
      } else {
        if (target[2] != 'b1g') {
          nimStop("2nd target node must be the vector (length g) of linear coefficients 'b1g'.")
        } else {
          if (target[3] != 'b2g') {
            nimStop("3rd target node must be the vector (length g) of quadratic coefficients 'b2g'.")
          } else {
            if (target[4] != 'b3g') {
              nimStop("4th target node must be the vector (length g) of cubic coefficients 'b3g'.")
            }
          }
        }
      }
    }

    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Xt <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtW <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    DXtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    prbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    psbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    ybeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    qsbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    CC <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    CI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    tCI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {

    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    ag  <- model[['ag']]
    b1g <- model[['b1g']]
    b2g <- model[['b2g']]
    b3g <- model[['b3g']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    Xt <<- t(X)                                                     # transpose X
    prbeta <<- Dbetag %*% mbetag                                    # contribution to posterior mean from prior
    
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      XtW <<- Xt %*% Wg                                             # multiply Xt and Wg
      XtWX <<- XtW %*% X                                            # calculate XtWX, the precision matrix from WLS
      DXtWX <<- Dbetag + XtWX                                       # posterior precision matrix for beta is Dbetag + XtWX
      ybeta <<- XtW %*% Yvec                                        # contribution to posterior mean from WLS
      psbeta <<- prbeta + ybeta                                     # sum of prior and WLS contributions
      for (k in 1:p) {
        qsbeta[k,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
      }
      CC <<- chol(DXtWX)                                            # Cholesky decomposition for precision matrix (R returns upper triangular)
      CI <<- inverse(CC)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tCI <<- t(CI)                                                 # transpose
      psbeta <<- tCI %*% psbeta                                     # rescale
      qsbeta <<- psbeta + qsbeta                                    # center
      qsbeta <<- CI %*% qsbeta                                      # rescale
      
      ag[i]  <- qsbeta[1,1]
      b1g[i] <- qsbeta[2,1] 
      b2g[i] <- qsbeta[3,1] 
      b3g[i] <- qsbeta[4,1] 
    }

    # Update model with new node values
    model[['ag']]  <<- ag
    model[['b1g']] <<- b1g
    model[['b2g']] <<- b2g
    model[['b3g']] <<- b3g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_bgc

# Custom Gibbs sampler for regression coefficients in the group-specific cubic trend model with a level shift
sampler_CP_xptl_bgc <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 5 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 5 | lt != p) {
      nimStop("The two intercepts and three sets of regression coefficients are all expected to be sampled in 'sampler_CP_xptl_bgc'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's2ag') {
          nimStop("2nd target node must be the vector (length g) of intercepts 's2ag'.")
        } else {
          if (target[3] != 'b1g') {
            nimStop("3rd target node must be the vector (length g) of linear coefficients 'b1g'.")
          } else {
            if (target[4] != 'b2g') {
              nimStop("4th target node must be the vector (length g) of quadratic coefficients 'b2g'.")
            } else {
              if (target[5] != 'b3g') {
                nimStop("5th target node must be the vector (length g) of cubic coefficients 'b3g'.")
              }
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Xt <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtW <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    DXtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    prbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    psbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    ybeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    qsbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    CC <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    CI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    tCI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    b1g  <- model[['b1g']]
    b2g  <- model[['b2g']]
    b3g  <- model[['b3g']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    Xt <<- t(X)                                                     # transpose X
    prbeta <<- Dbetag %*% mbetag                                    # contribution to posterior mean from prior
    
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      XtW <<- Xt %*% Wg                                             # multiply Xt and Wg
      XtWX <<- XtW %*% X                                            # calculate XtWX, the precision matrix from WLS
      DXtWX <<- Dbetag + XtWX                                       # posterior precision matrix for beta is Dbetag + XtWX
      ybeta <<- XtW %*% Yvec                                        # contribution to posterior mean from WLS
      psbeta <<- prbeta + ybeta                                     # sum of prior and WLS contributions
      for (k in 1:p) {
        qsbeta[k,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
      }
      CC <<- chol(DXtWX)                                            # Cholesky decomposition for precision matrix (R returns upper triangular)
      CI <<- inverse(CC)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tCI <<- t(CI)                                                 # transpose
      psbeta <<- tCI %*% psbeta                                     # rescale
      qsbeta <<- psbeta + qsbeta                                    # center
      qsbeta <<- CI %*% qsbeta                                      # rescale
      
      s1ag[i] <- qsbeta[1,1]
      s2ag[i] <- qsbeta[2,1]
      b1g[i]  <- qsbeta[3,1] 
      b2g[i]  <- qsbeta[4,1] 
      b3g[i]  <- qsbeta[5,1] 
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    model[['b1g']]  <<- b1g
    model[['b2g']]  <<- b2g
    model[['b3g']]  <<- b3g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptl_bgc

# Custom Gibbs sampler for regression coefficients in the group-specific cubic-cubic trend model with a full break
sampler_CP_xptf_bgc_bgc <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 8 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 8 | lt != p) {
      nimStop("The segment-specific intercepts and sets of regression coefficients are all expected to be sampled in 'sampler_CP_xptf_bgc_bgc'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's1b1g') {
          nimStop("2nd target node must be the vector (length g) of linear coefficients 's1b1g'.")
        } else {
          if (target[3] != 's1b2g') {
            nimStop("3rd target node must be the vector (length g) of quadratic coefficients 's1b2g'.")
          } else {
            if (target[4] != 's1b3g') {
              nimStop("4th target node must be the vector (length g) of cubic coefficients 's1b3g'.")
            } else {
              if (target[5] != 's2ag') {
                nimStop("5th target node must be the vector (length g) of intercepts 's2ag'.")
              } else {
                if (target[6] != 's2b1g') {
                  nimStop("6th target node must be the vector (length g) of linear coefficients 's2b1g'.")
                } else {
                  if (target[7] != 's2b2g') {
                    nimStop("7th target node must be the vector (length g) of quadratic coefficients 's2b2g'.")
                  } else {
                    if (target[8] != 's2b3g') {
                      nimStop("8th target node must be the vector (length g) of cubic coefficients 's2b3g'.")
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Xt <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtW <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    DXtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    prbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    psbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    ybeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    qsbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    CC <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    CI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    tCI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag  <- model[['s1ag']]
    s1b1g <- model[['s1b1g']]
    s1b2g <- model[['s1b2g']]
    s1b3g <- model[['s1b3g']]
    s2ag  <- model[['s2ag']]
    s2b1g <- model[['s2b1g']]
    s2b2g <- model[['s2b2g']]
    s2b3g <- model[['s2b3g']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    Xt <<- t(X)                                                     # transpose X
    prbeta <<- Dbetag %*% mbetag                                    # contribution to posterior mean from prior
    
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      XtW <<- Xt %*% Wg                                             # multiply Xt and Wg
      XtWX <<- XtW %*% X                                            # calculate XtWX, the precision matrix from WLS
      DXtWX <<- Dbetag + XtWX                                       # posterior precision matrix for beta is Dbetag + XtWX
      ybeta <<- XtW %*% Yvec                                        # contribution to posterior mean from WLS
      psbeta <<- prbeta + ybeta                                     # sum of prior and WLS contributions
      for (k in 1:p) {
        qsbeta[k,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
      }
      CC <<- chol(DXtWX)                                            # Cholesky decomposition for precision matrix (R returns upper triangular)
      CI <<- inverse(CC)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tCI <<- t(CI)                                                 # transpose
      psbeta <<- tCI %*% psbeta                                     # rescale
      qsbeta <<- psbeta + qsbeta                                    # center
      qsbeta <<- CI %*% qsbeta                                      # rescale
      
      s1ag[i]  <- qsbeta[1,1]
      s1b1g[i] <- qsbeta[2,1] 
      s1b2g[i] <- qsbeta[3,1] 
      s1b3g[i] <- qsbeta[4,1] 
      s2ag[i]  <- qsbeta[5,1]
      s2b1g[i] <- qsbeta[6,1] 
      s2b2g[i] <- qsbeta[7,1] 
      s2b3g[i] <- qsbeta[8,1] 
    }
    
    # Update model with new node values
    model[['s1ag']]  <<- s1ag
    model[['s1b1g']] <<- s1b1g
    model[['s1b2g']] <<- s1b2g
    model[['s1b3g']] <<- s1b3g
    model[['s2ag']]  <<- s2ag
    model[['s2b1g']] <<- s2b1g
    model[['s2b2g']] <<- s2b2g
    model[['s2b3g']] <<- s2b3g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptf_bgc_bgc

# Custom Gibbs sampler for regression coefficients in the group-specific cubic-quadratic trend model with a full break
sampler_CP_xptf_bgc_bgq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 7 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 7 | lt != p) {
      nimStop("The segment-specific intercepts and sets of regression coefficients are all expected to be sampled in 'sampler_CP_xptf_bgc_bgq'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's1b1g') {
          nimStop("2nd target node must be the vector (length g) of linear coefficients 's1b1g'.")
        } else {
          if (target[3] != 's1b2g') {
            nimStop("3rd target node must be the vector (length g) of quadratic coefficients 's1b2g'.")
          } else {
            if (target[4] != 's1b3g') {
              nimStop("4th target node must be the vector (length g) of cubic coefficients 's1b3g'.")
            } else {
              if (target[5] != 's2ag') {
                nimStop("5th target node must be the vector (length g) of intercepts 's2ag'.")
              } else {
                if (target[6] != 's2b1g') {
                  nimStop("6th target node must be the vector (length g) of linear coefficients 's2b1g'.")
                } else {
                  if (target[7] != 's2b2g') {
                    nimStop("7th target node must be the vector (length g) of quadratic coefficients 's2b2g'.")
                  }
                }
              }
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Xt <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtW <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    DXtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    prbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    psbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    ybeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    qsbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    CC <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    CI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    tCI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag  <- model[['s1ag']]
    s1b1g <- model[['s1b1g']]
    s1b2g <- model[['s1b2g']]
    s1b3g <- model[['s1b3g']]
    s2ag  <- model[['s2ag']]
    s2b1g <- model[['s2b1g']]
    s2b2g <- model[['s2b2g']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    Xt <<- t(X)                                                     # transpose X
    prbeta <<- Dbetag %*% mbetag                                    # contribution to posterior mean from prior
    
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      XtW <<- Xt %*% Wg                                             # multiply Xt and Wg
      XtWX <<- XtW %*% X                                            # calculate XtWX, the precision matrix from WLS
      DXtWX <<- Dbetag + XtWX                                       # posterior precision matrix for beta is Dbetag + XtWX
      ybeta <<- XtW %*% Yvec                                        # contribution to posterior mean from WLS
      psbeta <<- prbeta + ybeta                                     # sum of prior and WLS contributions
      for (k in 1:p) {
        qsbeta[k,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
      }
      CC <<- chol(DXtWX)                                            # Cholesky decomposition for precision matrix (R returns upper triangular)
      CI <<- inverse(CC)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tCI <<- t(CI)                                                 # transpose
      psbeta <<- tCI %*% psbeta                                     # rescale
      qsbeta <<- psbeta + qsbeta                                    # center
      qsbeta <<- CI %*% qsbeta                                      # rescale
      
      s1ag[i]  <- qsbeta[1,1]
      s1b1g[i] <- qsbeta[2,1] 
      s1b2g[i] <- qsbeta[3,1] 
      s1b3g[i] <- qsbeta[4,1] 
      s2ag[i]  <- qsbeta[5,1]
      s2b1g[i] <- qsbeta[6,1] 
      s2b2g[i] <- qsbeta[7,1] 
    }
    
    # Update model with new node values
    model[['s1ag']]  <<- s1ag
    model[['s1b1g']] <<- s1b1g
    model[['s1b2g']] <<- s1b2g
    model[['s1b3g']] <<- s1b3g
    model[['s2ag']]  <<- s2ag
    model[['s2b1g']] <<- s2b1g
    model[['s2b2g']] <<- s2b2g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptf_bgc_bgq

# Custom Gibbs sampler for regression coefficients in the group-specific cubic-linear trend model with a full break
sampler_CP_xptf_bgc_bgl <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 6 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 6 | lt != p) {
      nimStop("The segment-specific intercepts and sets of regression coefficients are all expected to be sampled in 'sampler_CP_xptf_bgc_bgl'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's1b1g') {
          nimStop("2nd target node must be the vector (length g) of linear coefficients 's1b1g'.")
        } else {
          if (target[3] != 's1b2g') {
            nimStop("3rd target node must be the vector (length g) of quadratic coefficients 's1b2g'.")
          } else {
            if (target[4] != 's1b3g') {
              nimStop("4th target node must be the vector (length g) of cubic coefficients 's1b3g'.")
            } else {
              if (target[5] != 's2ag') {
                nimStop("5th target node must be the vector (length g) of intercepts 's2ag'.")
              } else {
                if (target[6] != 's2b1g') {
                  nimStop("6th target node must be the vector (length g) of linear coefficients 's2b1g'.")
                } 
              }
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Xt <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtW <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    DXtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    prbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    psbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    ybeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    qsbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    CC <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    CI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    tCI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag  <- model[['s1ag']]
    s1b1g <- model[['s1b1g']]
    s1b2g <- model[['s1b2g']]
    s1b3g <- model[['s1b3g']]
    s2ag  <- model[['s2ag']]
    s2b1g <- model[['s2b1g']]

    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    Xt <<- t(X)                                                     # transpose X
    prbeta <<- Dbetag %*% mbetag                                    # contribution to posterior mean from prior
    
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      XtW <<- Xt %*% Wg                                             # multiply Xt and Wg
      XtWX <<- XtW %*% X                                            # calculate XtWX, the precision matrix from WLS
      DXtWX <<- Dbetag + XtWX                                       # posterior precision matrix for beta is Dbetag + XtWX
      ybeta <<- XtW %*% Yvec                                        # contribution to posterior mean from WLS
      psbeta <<- prbeta + ybeta                                     # sum of prior and WLS contributions
      for (k in 1:p) {
        qsbeta[k,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
      }
      CC <<- chol(DXtWX)                                            # Cholesky decomposition for precision matrix (R returns upper triangular)
      CI <<- inverse(CC)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tCI <<- t(CI)                                                 # transpose
      psbeta <<- tCI %*% psbeta                                     # rescale
      qsbeta <<- psbeta + qsbeta                                    # center
      qsbeta <<- CI %*% qsbeta                                      # rescale
      
      s1ag[i]  <- qsbeta[1,1]
      s1b1g[i] <- qsbeta[2,1] 
      s1b2g[i] <- qsbeta[3,1] 
      s1b3g[i] <- qsbeta[4,1] 
      s2ag[i]  <- qsbeta[5,1]
      s2b1g[i] <- qsbeta[6,1] 
    }
    
    # Update model with new node values
    model[['s1ag']]  <<- s1ag
    model[['s1b1g']] <<- s1b1g
    model[['s1b2g']] <<- s1b2g
    model[['s1b3g']] <<- s1b3g
    model[['s2ag']]  <<- s2ag
    model[['s2b1g']] <<- s2b1g

    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptf_bgc_bgl

# Custom Gibbs sampler for regression coefficients in the group-specific quadratic trend model
sampler_CP_bgq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 3 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 3 | lt != p) {
      nimStop("The intercepts and both sets of regression coefficients are all expected to be sampled in 'sampler_CP_bgq'.")
    } else {
      if (target[1] != 'ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 'ag'.")
      } else {
        if (target[2] != 'b1g') {
          nimStop("2nd target node must be the vector (length g) of linear coefficients 'b1g'.")
        } else {
          if (target[3] != 'b2g') {
            nimStop("3rd target node must be the vector (length g) of quadratic coefficients 'b2g'.")
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Xt <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtW <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    DXtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    prbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    psbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    ybeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    qsbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    CC <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    CI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    tCI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {

    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    ag  <- model[['ag']]
    b1g <- model[['b1g']]
    b2g <- model[['b2g']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    Xt <<- t(X)                                                     # transpose X
    prbeta <<- Dbetag %*% mbetag                                    # contribution to posterior mean from prior
    
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      XtW <<- Xt %*% Wg                                             # multiply Xt and Wg
      XtWX <<- XtW %*% X                                            # calculate XtWX, the precision matrix from WLS
      DXtWX <<- Dbetag + XtWX                                       # posterior precision matrix for beta is Dbetag + XtWX
      ybeta <<- XtW %*% Yvec                                        # contribution to posterior mean from WLS
      psbeta <<- prbeta + ybeta                                     # sum of prior and WLS contributions
      for (k in 1:p) {
        qsbeta[k,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
      }
      CC <<- chol(DXtWX)                                            # Cholesky decomposition for precision matrix (R returns upper triangular)
      CI <<- inverse(CC)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tCI <<- t(CI)                                                 # transpose
      psbeta <<- tCI %*% psbeta                                     # rescale
      qsbeta <<- psbeta + qsbeta                                    # center
      qsbeta <<- CI %*% qsbeta                                      # rescale
      
      ag[i]  <- qsbeta[1,1]
      b1g[i] <- qsbeta[2,1] 
      b2g[i] <- qsbeta[3,1] 
    }
    
    # Update model with new node values
    model[['ag']]  <<- ag
    model[['b1g']] <<- b1g
    model[['b2g']] <<- b2g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_bgq

# Custom Gibbs sampler for regression coefficients in the group-specific quadratic trend model with a level shift
sampler_CP_xptl_bgq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 4 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 4 | lt != p) {
      nimStop("The two intercepts and two sets of regression coefficients are all expected to be sampled in 'sampler_CP_xptl_bgq'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's2ag') {
          nimStop("2nd target node must be the vector (length g) of intercepts 's2ag'.")
        } else {
          if (target[3] != 'b1g') {
            nimStop("3rd target node must be the vector (length g) of linear coefficients 'b1g'.")
          } else {
            if (target[4] != 'b2g') {
              nimStop("4th target node must be the vector (length g) of quadratic coefficients 'b2g'.")
            } 
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Xt <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtW <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    DXtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    prbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    psbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    ybeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    qsbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    CC <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    CI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    tCI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    b1g  <- model[['b1g']]
    b2g  <- model[['b2g']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    Xt <<- t(X)                                                     # transpose X
    prbeta <<- Dbetag %*% mbetag                                    # contribution to posterior mean from prior
    
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      XtW <<- Xt %*% Wg                                             # multiply Xt and Wg
      XtWX <<- XtW %*% X                                            # calculate XtWX, the precision matrix from WLS
      DXtWX <<- Dbetag + XtWX                                       # posterior precision matrix for beta is Dbetag + XtWX
      ybeta <<- XtW %*% Yvec                                        # contribution to posterior mean from WLS
      psbeta <<- prbeta + ybeta                                     # sum of prior and WLS contributions
      for (k in 1:p) {
        qsbeta[k,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
      }
      CC <<- chol(DXtWX)                                            # Cholesky decomposition for precision matrix (R returns upper triangular)
      CI <<- inverse(CC)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tCI <<- t(CI)                                                 # transpose
      psbeta <<- tCI %*% psbeta                                     # rescale
      qsbeta <<- psbeta + qsbeta                                    # center
      qsbeta <<- CI %*% qsbeta                                      # rescale
      
      s1ag[i] <- qsbeta[1,1]
      s2ag[i] <- qsbeta[2,1]
      b1g[i]  <- qsbeta[3,1] 
      b2g[i]  <- qsbeta[4,1] 
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    model[['b1g']]  <<- b1g
    model[['b2g']]  <<- b2g

    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptl_bgq

# Custom Gibbs sampler for regression coefficients in the group-specific quadratic-quadratic trend model with a full break
sampler_CP_xptf_bgq_bgq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 6 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 6 | lt != p) {
      nimStop("The segment-specific intercepts and sets of regression coefficients are all expected to be sampled in 'sampler_CP_xptf_bgq_bgq'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's1b1g') {
          nimStop("2nd target node must be the vector (length g) of linear coefficients 's1b1g'.")
        } else {
          if (target[3] != 's1b2g') {
            nimStop("3rd target node must be the vector (length g) of quadratic coefficients 's1b2g'.")
          } else {
            if (target[4] != 's2ag') {
              nimStop("4th target node must be the vector (length g) of intercepts 's2ag'.")
            } else {
              if (target[5] != 's2b1g') {
                nimStop("5th target node must be the vector (length g) of linear coefficients 's2b1g'.")
              } else {
                if (target[6] != 's2b2g') {
                  nimStop("6th target node must be the vector (length g) of quadratic coefficients 's2b2g'.")
                } 
              }
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Xt <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtW <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    DXtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    prbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    psbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    ybeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    qsbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    CC <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    CI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    tCI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag  <- model[['s1ag']]
    s1b1g <- model[['s1b1g']]
    s1b2g <- model[['s1b2g']]
    s2ag  <- model[['s2ag']]
    s2b1g <- model[['s2b1g']]
    s2b2g <- model[['s2b2g']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    Xt <<- t(X)                                                     # transpose X
    prbeta <<- Dbetag %*% mbetag                                    # contribution to posterior mean from prior
    
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      XtW <<- Xt %*% Wg                                             # multiply Xt and Wg
      XtWX <<- XtW %*% X                                            # calculate XtWX, the precision matrix from WLS
      DXtWX <<- Dbetag + XtWX                                       # posterior precision matrix for beta is Dbetag + XtWX
      ybeta <<- XtW %*% Yvec                                        # contribution to posterior mean from WLS
      psbeta <<- prbeta + ybeta                                     # sum of prior and WLS contributions
      for (k in 1:p) {
        qsbeta[k,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
      }
      CC <<- chol(DXtWX)                                            # Cholesky decomposition for precision matrix (R returns upper triangular)
      CI <<- inverse(CC)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tCI <<- t(CI)                                                 # transpose
      psbeta <<- tCI %*% psbeta                                     # rescale
      qsbeta <<- psbeta + qsbeta                                    # center
      qsbeta <<- CI %*% qsbeta                                      # rescale
      
      s1ag[i]  <- qsbeta[1,1]
      s1b1g[i] <- qsbeta[2,1] 
      s1b2g[i] <- qsbeta[3,1] 
      s2ag[i]  <- qsbeta[4,1]
      s2b1g[i] <- qsbeta[5,1] 
      s2b2g[i] <- qsbeta[6,1] 
    }
    
    # Update model with new node values
    model[['s1ag']]  <<- s1ag
    model[['s1b1g']] <<- s1b1g
    model[['s1b2g']] <<- s1b2g
    model[['s2ag']]  <<- s2ag
    model[['s2b1g']] <<- s2b1g
    model[['s2b2g']] <<- s2b2g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptf_bgq_bgq

# Custom Gibbs sampler for regression coefficients in the group-specific quadratic-linear trend model with a full break
sampler_CP_xptf_bgq_bgl <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 5 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 5 | lt != p) {
      nimStop("The segment-specific intercepts and sets of regression coefficients are all expected to be sampled in 'sampler_CP_xptf_bgq_bgl'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's1b1g') {
          nimStop("2nd target node must be the vector (length g) of linear coefficients 's1b1g'.")
        } else {
          if (target[3] != 's1b2g') {
            nimStop("3rd target node must be the vector (length g) of quadratic coefficients 's1b2g'.")
          } else {
            if (target[4] != 's2ag') {
              nimStop("4th target node must be the vector (length g) of intercepts 's2ag'.")
            } else {
              if (target[5] != 's2b1g') {
                nimStop("5th target node must be the vector (length g) of linear coefficients 's2b1g'.")
              } 
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Xt <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtW <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    DXtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    prbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    psbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    ybeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    qsbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    CC <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    CI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    tCI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag  <- model[['s1ag']]
    s1b1g <- model[['s1b1g']]
    s1b2g <- model[['s1b2g']]
    s2ag  <- model[['s2ag']]
    s2b1g <- model[['s2b1g']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    Xt <<- t(X)                                                     # transpose X
    prbeta <<- Dbetag %*% mbetag                                    # contribution to posterior mean from prior
    
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      XtW <<- Xt %*% Wg                                             # multiply Xt and Wg
      XtWX <<- XtW %*% X                                            # calculate XtWX, the precision matrix from WLS
      DXtWX <<- Dbetag + XtWX                                       # posterior precision matrix for beta is Dbetag + XtWX
      ybeta <<- XtW %*% Yvec                                        # contribution to posterior mean from WLS
      psbeta <<- prbeta + ybeta                                     # sum of prior and WLS contributions
      for (k in 1:p) {
        qsbeta[k,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
      }
      CC <<- chol(DXtWX)                                            # Cholesky decomposition for precision matrix (R returns upper triangular)
      CI <<- inverse(CC)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tCI <<- t(CI)                                                 # transpose
      psbeta <<- tCI %*% psbeta                                     # rescale
      qsbeta <<- psbeta + qsbeta                                    # center
      qsbeta <<- CI %*% qsbeta                                      # rescale
      
      s1ag[i]  <- qsbeta[1,1]
      s1b1g[i] <- qsbeta[2,1] 
      s1b2g[i] <- qsbeta[3,1] 
      s2ag[i]  <- qsbeta[4,1]
      s2b1g[i] <- qsbeta[5,1] 
    }
    
    # Update model with new node values
    model[['s1ag']]  <<- s1ag
    model[['s1b1g']] <<- s1b1g
    model[['s1b2g']] <<- s1b2g
    model[['s2ag']]  <<- s2ag
    model[['s2b1g']] <<- s2b1g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptf_bgq_bgl

# Custom Gibbs sampler for regression coefficients in the group-specific linear trend model
sampler_CP_bgl <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 2 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 2 | lt != p) {
      nimStop("The intercepts and linear regression coefficients are all expected to be sampled in 'sampler_CP_bgl'.")
    } else {
      if (target[1] != 'ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 'ag'.")
      } else {
        if (target[2] != 'b1g') {
          nimStop("2nd target node must be the vector (length g) of linear coefficients 'b1g'.")
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Xt <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtW <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    DXtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    prbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    psbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    ybeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    qsbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    CC <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    CI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    tCI <- nimMatrix(nrow=p, ncol=p, init = FALSE)

    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {

    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    ag  <- model[['ag']]
    b1g <- model[['b1g']]

    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    Xt <<- t(X)                                                     # transpose X
    prbeta <<- Dbetag %*% mbetag                                    # contribution to posterior mean from prior
    
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      XtW <<- Xt %*% Wg                                             # multiply Xt and Wg
      XtWX <<- XtW %*% X                                            # calculate XtWX, the precision matrix from WLS
      DXtWX <<- Dbetag + XtWX                                       # posterior precision matrix for beta is Dbetag + XtWX
      ybeta <<- XtW %*% Yvec                                        # contribution to posterior mean from WLS
      psbeta <<- prbeta + ybeta                                     # sum of prior and WLS contributions
      for (k in 1:p) {
        qsbeta[k,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
      }
      CC <<- chol(DXtWX)                                            # Cholesky decomposition for precision matrix (R returns upper triangular)
      CI <<- inverse(CC)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tCI <<- t(CI)                                                 # transpose
      psbeta <<- tCI %*% psbeta                                     # rescale
      qsbeta <<- psbeta + qsbeta                                    # center
      qsbeta <<- CI %*% qsbeta                                      # rescale
      
      ag[i]  <- qsbeta[1,1]
      b1g[i] <- qsbeta[2,1] 
    }
    
    # Update model with new node values
    model[['ag']]  <<- ag
    model[['b1g']] <<- b1g

    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_bgl

# Custom Gibbs sampler for regression coefficients in the group-specific linear trend model with a level shift
sampler_CP_xptl_bgl <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 3 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 3 | lt != p) {
      nimStop("The two intercepts and the set of regression coefficients are all expected to be sampled in 'sampler_CP_xptl_bgl'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's2ag') {
          nimStop("2nd target node must be the vector (length g) of intercepts 's2ag'.")
        } else {
          if (target[3] != 'b1g') {
            nimStop("3rd target node must be the vector (length g) of linear coefficients 'b1g'.")
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Xt <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtW <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    DXtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    prbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    psbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    ybeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    qsbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    CC <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    CI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    tCI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    b1g  <- model[['b1g']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    Xt <<- t(X)                                                     # transpose X
    prbeta <<- Dbetag %*% mbetag                                    # contribution to posterior mean from prior
    
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      XtW <<- Xt %*% Wg                                             # multiply Xt and Wg
      XtWX <<- XtW %*% X                                            # calculate XtWX, the precision matrix from WLS
      DXtWX <<- Dbetag + XtWX                                       # posterior precision matrix for beta is Dbetag + XtWX
      ybeta <<- XtW %*% Yvec                                        # contribution to posterior mean from WLS
      psbeta <<- prbeta + ybeta                                     # sum of prior and WLS contributions
      for (k in 1:p) {
        qsbeta[k,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
      }
      CC <<- chol(DXtWX)                                            # Cholesky decomposition for precision matrix (R returns upper triangular)
      CI <<- inverse(CC)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tCI <<- t(CI)                                                 # transpose
      psbeta <<- tCI %*% psbeta                                     # rescale
      qsbeta <<- psbeta + qsbeta                                    # center
      qsbeta <<- CI %*% qsbeta                                      # rescale
      
      s1ag[i] <- qsbeta[1,1]
      s2ag[i] <- qsbeta[2,1]
      b1g[i]  <- qsbeta[3,1] 
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    model[['b1g']]  <<- b1g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptl_bgl

# Custom Gibbs sampler for regression coefficients in the group-specific linear-linear trend model with a full break
sampler_CP_xptf_bgl_bgl <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 4 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 4 | lt != p) {
      nimStop("The segment-specific intercepts and sets of regression coefficients are all expected to be sampled in 'sampler_CP_xptf_bgl_bgl'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's1b1g') {
          nimStop("2nd target node must be the vector (length g) of linear coefficients 's1b1g'.")
        } else {
          if (target[3] != 's2ag') {
            nimStop("3rd target node must be the vector (length g) of intercepts 's2ag'.")
          } else {
            if (target[4] != 's2b1g') {
              nimStop("4th target node must be the vector (length g) of linear coefficients 's2b1g'.")
            } 
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Xt <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtW <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    DXtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    prbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    psbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    ybeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    qsbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    CC <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    CI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    tCI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag  <- model[['s1ag']]
    s1b1g <- model[['s1b1g']]
    s2ag  <- model[['s2ag']]
    s2b1g <- model[['s2b1g']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    Xt <<- t(X)                                                     # transpose X
    prbeta <<- Dbetag %*% mbetag                                    # contribution to posterior mean from prior
    
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      XtW <<- Xt %*% Wg                                             # multiply Xt and Wg
      XtWX <<- XtW %*% X                                            # calculate XtWX, the precision matrix from WLS
      DXtWX <<- Dbetag + XtWX                                       # posterior precision matrix for beta is Dbetag + XtWX
      ybeta <<- XtW %*% Yvec                                        # contribution to posterior mean from WLS
      psbeta <<- prbeta + ybeta                                     # sum of prior and WLS contributions
      for (k in 1:p) {
        qsbeta[k,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
      }
      CC <<- chol(DXtWX)                                            # Cholesky decomposition for precision matrix (R returns upper triangular)
      CI <<- inverse(CC)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tCI <<- t(CI)                                                 # transpose
      psbeta <<- tCI %*% psbeta                                     # rescale
      qsbeta <<- psbeta + qsbeta                                    # center
      qsbeta <<- CI %*% qsbeta                                      # rescale
      
      s1ag[i]  <- qsbeta[1,1]
      s1b1g[i] <- qsbeta[2,1] 
      s2ag[i]  <- qsbeta[3,1]
      s2b1g[i] <- qsbeta[4,1] 
    }
    
    # Update model with new node values
    model[['s1ag']]  <<- s1ag
    model[['s1b1g']] <<- s1b1g
    model[['s2ag']]  <<- s2ag
    model[['s2b1g']] <<- s2b1g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptf_bgl_bgl

# Custom Gibbs sampler for regression coefficients in the intercepts-only trend model
sampler_CP_b0 <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 1 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 1 | lt != p) {
      nimStop("The intercepts are expected to be sampled in 'sampler_CP_b0'.")
    } else {
      if (target[1] != 'ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 'ag'.")
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Xt <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtW <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    DXtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    prbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    psbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    ybeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    qsbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    CC <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    CI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    tCI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]

    # Node values to be replaced
    ag <- model[['ag']]

    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    Xt <<- t(X)                                                     # transpose X
    prbeta <<- Dbetag %*% mbetag                                    # contribution to posterior mean from prior
    
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      XtW <<- Xt %*% Wg                                             # multiply Xt and Wg
      XtWX <<- XtW %*% X                                            # calculate XtWX, the precision matrix from WLS
      DXtWX <<- Dbetag + XtWX                                       # posterior precision matrix for beta is Dbetag + XtWX
      ybeta <<- XtW %*% Yvec                                        # contribution to posterior mean from WLS
      psbeta <<- prbeta + ybeta                                     # sum of prior and WLS contributions
      for (k in 1:p) {
        qsbeta[k,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
      }
      CC <<- chol(DXtWX)                                            # Cholesky decomposition for precision matrix (R returns upper triangular)
      CI <<- inverse(CC)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tCI <<- t(CI)                                                 # transpose
      psbeta <<- tCI %*% psbeta                                     # rescale
      qsbeta <<- psbeta + qsbeta                                    # center
      qsbeta <<- CI %*% qsbeta                                      # rescale
      
      ag[i] <- qsbeta[1,1]
    }
    
    # Update model with new node values
    model[['ag']] <<- ag

    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_b0

# Custom Gibbs sampler for regression coefficients in the group-specific linear trend model with a level shift
sampler_CP_xptl_b0 <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 2 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 2 | lt != p) {
      nimStop("The two intercepts are both expected to be sampled in 'sampler_CP_xptl_b0'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's2ag') {
          nimStop("2nd target node must be the vector (length g) of intercepts 's2ag'.")
        } 
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Xt <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtW <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    DXtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    prbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    psbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    ybeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    qsbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    CC <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    CI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    tCI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    Xt <<- t(X)                                                     # transpose X
    prbeta <<- Dbetag %*% mbetag                                    # contribution to posterior mean from prior
    
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      XtW <<- Xt %*% Wg                                             # multiply Xt and Wg
      XtWX <<- XtW %*% X                                            # calculate XtWX, the precision matrix from WLS
      DXtWX <<- Dbetag + XtWX                                       # posterior precision matrix for beta is Dbetag + XtWX
      ybeta <<- XtW %*% Yvec                                        # contribution to posterior mean from WLS
      psbeta <<- prbeta + ybeta                                     # sum of prior and WLS contributions
      for (k in 1:p) {
        qsbeta[k,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
      }
      CC <<- chol(DXtWX)                                            # Cholesky decomposition for precision matrix (R returns upper triangular)
      CI <<- inverse(CC)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tCI <<- t(CI)                                                 # transpose
      psbeta <<- tCI %*% psbeta                                     # rescale
      qsbeta <<- psbeta + qsbeta                                    # center
      qsbeta <<- CI %*% qsbeta                                      # rescale
      
      s1ag[i] <- qsbeta[1,1]
      s2ag[i] <- qsbeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptl_b0 (note that this is identical to sampler_CP_xptf_b0_b0)

# Custom Gibbs sampler for regression coefficients in the group-specific dropped-dropped trend model with a full break
sampler_CP_xptf_b0_b0 <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 2 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 2 | lt != p) {
      nimStop("The segment-specific intercepts are both expected to be sampled in 'sampler_CP_xptf_b0_b0'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's2ag') {
          nimStop("2nd target node must be the vector (length g) of intercepts 's2ag'.")
        } 
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Yvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Xt <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtW <- nimMatrix(nrow=p, ncol=n, init = FALSE)
    XtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    DXtWX <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    prbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    psbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    ybeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)
    qsbeta <- nimMatrix(nrow=p, ncol=1L, init = FALSE)  
    CC <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    CI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    tCI <- nimMatrix(nrow=p, ncol=p, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag  <- model[['s1ag']]
    s2ag  <- model[['s2ag']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    Xt <<- t(X)                                                     # transpose X
    prbeta <<- Dbetag %*% mbetag                                    # contribution to posterior mean from prior
    
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Yvec[j,1] <<- Yarr[(i-1)*n+j]                               # populate nx1 data vector Yvec
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      XtW <<- Xt %*% Wg                                             # multiply Xt and Wg
      XtWX <<- XtW %*% X                                            # calculate XtWX, the precision matrix from WLS
      DXtWX <<- Dbetag + XtWX                                       # posterior precision matrix for beta is Dbetag + XtWX
      ybeta <<- XtW %*% Yvec                                        # contribution to posterior mean from WLS
      psbeta <<- prbeta + ybeta                                     # sum of prior and WLS contributions
      for (k in 1:p) {
        qsbeta[k,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
      }
      CC <<- chol(DXtWX)                                            # Cholesky decomposition for precision matrix (R returns upper triangular)
      CI <<- inverse(CC)                                            # inverse of upper triangular matrix from Cholesky decomposition
      tCI <<- t(CI)                                                 # transpose
      psbeta <<- tCI %*% psbeta                                     # rescale
      qsbeta <<- psbeta + qsbeta                                    # center
      qsbeta <<- CI %*% qsbeta                                      # rescale
      
      s1ag[i]  <- qsbeta[1,1]
      s2ag[i]  <- qsbeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']]  <<- s1ag
    model[['s2ag']]  <<- s2ag
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptf_b0_b0 (note that this identical to sampler_CP_xptl_b0)

# Custom Gibbs sampler for regression coefficients in the common cubic trend model
sampler_CP_b1c <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 4 here)
    pm1 <- 0L
    p <- model$getConstants()$p
    pm1 <- p-1
    
    # Make sure the 'target' values are correctly specified
    if (p != 4 | lt != p) {
      nimStop("The intercepts and three regression coefficients are all expected to be sampled in 'sampler_CP_b1c'.")
    } else {
      if (target[1] != 'ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 'ag'.")
      } else {
        if (target[2] != 'b1') {
          nimStop("2nd target node must be the common linear coefficient 'b1'.")
        } else {
          if (target[3] != 'b2') {
            nimStop("3rd target node must be the common quadratic coefficient 'b2'.")
          } else {
            if (target[4] != 'b3') {
              nimStop("4th target node must be the common cubic coefficient 'b3'.")
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bX <- nimMatrix(nrow=n, ncol=pm1, init = FALSE)
    ambetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bmbetag <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bDbetag <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    sumbXtWX <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    sumbzbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    bXt <- nimMatrix(nrow=pm1, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    bXtW <- nimMatrix(nrow=pm1, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bXtWX <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    aDXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bDXtWX <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    aprbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bprbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)  
    bpbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)  
    azbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bzbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)  
    bbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)  
    aCC <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bCC <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    aCI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    taCI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bCI <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    tbCI <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    ag <- model[['ag']]
    b1 <- model[['b1']]
    b2 <- model[['b2']]
    b3 <- model[['b3']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
    }
    ambetag[1,1] <<- mbetag[1,1]
    aDbetag[1,1] <<- Dbetag[1,1]
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    for (j in 1:n) {
      for (k in 2:p) {
        bX[j,k-1] <<- X[j,k]
      }
    }
    bDbetag <<- nimMatrix(0, pm1, pm1)
    for (k in 2:p) {
      bmbetag[k-1,1] <<- mbetag[k,1]
      bDbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    bXt <<- t(bX)                                                   # transpose bX
    bprbeta <<- bDbetag %*% bmbetag                                 # contribution to posterior mean from prior
    
    sumbXtWX <<- nimMatrix(0, pm1, pm1)                             # initialize cumulative sums
    sumbzbeta <<- nimMatrix(0, pm1, 1L)
    
    # Update common coefficient(s) conditional on intercepts
    for (i in 1:g) {                                                # cycle through each group independently
      abeta[1,1] <<- ag[i]                                          # group-specific abeta vector
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      bXtW <<- bXt %*% Wg                                           # multiply bXt and Wg
      bXtWX <<- bXtW %*% bX                                         # calculate bXtWX
      sumbXtWX <<- sumbXtWX + bXtWX                                 # cumulative matrix sum
      bzbeta <<- bXtW %*% Zvec                                      # contributions to posterior mean from WLS
      sumbzbeta <<- sumbzbeta + bzbeta                              # cumulative matrix sum
    }                                                               # end cycle through groups
    bDXtWX <<- bDbetag + sumbXtWX                                   # posterior precision matrix for common beta is bDbetag + sumbXtWX
    bpbeta <<- bprbeta + sumbzbeta                                  # sum of prior and the cumulative WLS contributions
    
    for (k in 1:pm1) {
      bbeta[k,1] <<- rnorm(1, 0, sd = 1)                            # sample from univariate standard normal(s)
    }
    bCC <<- chol(bDXtWX)                                            # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
    bCI <<- inverse(bCC)                                            # inverse of upper triangular matrix from Cholesky decomposition
    tbCI <<- t(bCI)                                                 # transpose
    bpbeta <<- tbCI %*% bpbeta                                      # rescale
    bbeta <<- bpbeta + bbeta                                        # center
    bbeta <<- bCI %*% bbeta                                         # rescale
    
    bXbeta <<- bX %*% bbeta                                         # updated vector bX (to use below)
    
    b1 <- bbeta[1,1]
    b2 <- bbeta[2,1]
    b3 <- bbeta[3,1]
    
    # Update intercepts conditional on the common coefficient(s)
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      ag[i] <- abeta[1,1]
    }
 
    # Update model with new node values
    model[['ag']] <<- ag
    model[['b1']] <<- b1
    model[['b2']] <<- b2
    model[['b3']] <<- b3
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_b1c 

# Custom Gibbs sampler for regression coefficients in the common cubic trend model with a level shift
sampler_CP_xptl_b1c <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 5 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2
    
    # Make sure the 'target' values are correctly specified
    if (p != 5 | lt != p) {
      nimStop("The two intercepts and three regression coefficients are all expected to be sampled in 'sampler_CP_xptl_b1c'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's2ag') {
          nimStop("2nd target node must be the vector (length g) of intercepts 's2ag'.")
        } else {
          if (target[3] != 'b1') {
            nimStop("3rd target node must be the common linear coefficient 'b1'.")
          } else {
            if (target[4] != 'b2') {
              nimStop("4th target node must be the common quadratic coefficient 'b2'.")
            } else {
              if (target[5] != 'b3') {
                nimStop("5th target node must be the common cubic coefficient 'b3'.")
              }
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bmbetag <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDbetag <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXt <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXtW <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bprbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bpbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCC <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    tbCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    b1   <- model[['b1']]
    b2   <- model[['b2']]
    b3   <- model[['b3']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,2]
    }
    aDbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 1:2) {
      ambetag[k,1] <<- mbetag[k,1]
      aDbetag[k,k] <<- Dbetag[k,k]
    }
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    for (j in 1:n) {
      for (k in 3:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    bDbetag <<- nimMatrix(0, pm2, pm2)
    for (k in 3:p) {
      bmbetag[k-2,1] <<- mbetag[k,1]
      bDbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    bXt <<- t(bX)                                                   # transpose bX
    bprbeta <<- bDbetag %*% bmbetag                                 # contribution to posterior mean from prior
    
    sumbXtWX <<- nimMatrix(0, pm2, pm2)                             # initialize cumulative sums
    sumbzbeta <<- nimMatrix(0, pm2, 1L)
    
    # Update common coefficient(s) conditional on intercepts
    for (i in 1:g) {                                                # cycle through each group independently
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector segment 2
      aXbeta <<- aX %*% abeta                                       # contribution from intercepts
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      bXtW <<- bXt %*% Wg                                           # multiply bXt and Wg
      bXtWX <<- bXtW %*% bX                                         # calculate bXtWX
      sumbXtWX <<- sumbXtWX + bXtWX                                 # cumulative matrix sum
      bzbeta <<- bXtW %*% Zvec                                      # contributions to posterior mean from WLS
      sumbzbeta <<- sumbzbeta + bzbeta                              # cumulative matrix sum
    }                                                               # end cycle through groups
    bDXtWX <<- bDbetag + sumbXtWX                                   # posterior precision matrix for common beta is bDbetag + sumbXtWX
    bpbeta <<- bprbeta + sumbzbeta                                  # sum of prior and the cumulative WLS contributions
    
    for (k in 1:pm2) {
      bbeta[k,1] <<- rnorm(1, 0, sd = 1)                            # sample from univariate standard normal(s)
    }
    bCC <<- chol(bDXtWX)                                            # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
    bCI <<- inverse(bCC)                                            # inverse of upper triangular matrix from Cholesky decomposition
    tbCI <<- t(bCI)                                                 # transpose
    bpbeta <<- tbCI %*% bpbeta                                      # rescale
    bbeta <<- bpbeta + bbeta                                        # center
    bbeta <<- bCI %*% bbeta                                         # rescale
    
    bXbeta <<- bX %*% bbeta                                         # updated vector bX (to use below)
    
    b1 <- bbeta[1,1]
    b2 <- bbeta[2,1]
    b3 <- bbeta[3,1]
    
    # Update intercepts conditional on the common coefficient(s)
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    model[['b1']]   <<- b1
    model[['b2']]   <<- b2
    model[['b3']]   <<- b3
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptl_b1c 

# Custom Gibbs sampler for regression coefficients in the common cubic-cubic trend model with a full break
sampler_CP_xptf_b1c_b1c <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 4 here)
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    
    # Overall dimensionality (p == 8 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2
    
    # Make sure the 'target' values are correctly specified
    if (p != 8 | lt != p) {
      nimStop("All segment-specific intercepts and regression coefficients are all expected to be sampled in 'sampler_CP_xptf_b1c_b1c'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's1b1') {
          nimStop("2nd target node must be the common linear coefficient 's1b1'.")
        } else {
          if (target[3] != 's1b2') {
            nimStop("3rd target node must be the common quadratic coefficient 's1b2'.")
          } else {
            if (target[4] != 's1b3') {
              nimStop("4th target node must be the common cubic coefficient 's1b3'.")
            } else {
              if (target[5] != 's2ag') {
                nimStop("5th target node must be the vector (length g) of intercepts 's2ag'.")
              } else {
                if (target[6] != 's2b1') {
                  nimStop("6th target node must be the common linear coefficient 's2b1'.")
                } else {
                  if (target[7] != 's2b2') {
                    nimStop("7th target node must be the common quadratic coefficient 's2b2'.")
                  } else {
                    if (target[8] != 's2b3') {
                      nimStop("8th target node must be the common cubic coefficient 's2b3'.")
                    }
                  }
                }
              }
            }
          }
        }
      }
    }

    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bmbetag <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDbetag <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXt <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXtW <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bprbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bpbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCC <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    tbCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s1b1 <- model[['s1b1']]
    s1b2 <- model[['s1b2']]
    s1b3 <- model[['s1b3']]
    s2ag <- model[['s2ag']]
    s2b1 <- model[['s2b1']]
    s2b2 <- model[['s2b2']]
    s2b3 <- model[['s2b3']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    ambetag[1,1] <<- mbetag[1,1]
    ambetag[2,1] <<- mbetag[1+s1p,1]
    aDbetag <<- nimMatrix(0, 2L, 2L)
    aDbetag[1,1] <<- Dbetag[1,1]
    aDbetag[2,2] <<- Dbetag[1+s1p,1+s1p]
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    for (j in 1:n) {
      for (k in 2:s1p) {
        bX[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    bDbetag <<- nimMatrix(0, pm2, pm2)
    for (k in 2:s1p) {
      bmbetag[k-1,1] <<- mbetag[k,1]
      bDbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      bmbetag[k-2,1] <<- mbetag[k,1]
      bDbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    bXt <<- t(bX)                                                   # transpose bX
    bprbeta <<- bDbetag %*% bmbetag                                 # contribution to posterior mean from prior
    
    sumbXtWX <<- nimMatrix(0, pm2, pm2)                             # initialize cumulative sums
    sumbzbeta <<- nimMatrix(0, pm2, 1L)
    
    # Update common coefficient(s) conditional on intercepts
    for (i in 1:g) {                                                # cycle through each group independently
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector segment 2
      aXbeta <<- aX %*% abeta                                       # contribution from intercepts
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      bXtW <<- bXt %*% Wg                                           # multiply bXt and Wg
      bXtWX <<- bXtW %*% bX                                         # calculate bXtWX
      sumbXtWX <<- sumbXtWX + bXtWX                                 # cumulative matrix sum
      bzbeta <<- bXtW %*% Zvec                                      # contributions to posterior mean from WLS
      sumbzbeta <<- sumbzbeta + bzbeta                              # cumulative matrix sum
    }                                                               # end cycle through groups
    bDXtWX <<- bDbetag + sumbXtWX                                   # posterior precision matrix for common beta is bDbetag + sumbXtWX
    bpbeta <<- bprbeta + sumbzbeta                                  # sum of prior and the cumulative WLS contributions
    
    for (k in 1:pm2) {
      bbeta[k,1] <<- rnorm(1, 0, sd = 1)                            # sample from univariate standard normal(s)
    }
    bCC <<- chol(bDXtWX)                                            # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
    bCI <<- inverse(bCC)                                            # inverse of upper triangular matrix from Cholesky decomposition
    tbCI <<- t(bCI)                                                 # transpose
    bpbeta <<- tbCI %*% bpbeta                                      # rescale
    bbeta <<- bpbeta + bbeta                                        # center
    bbeta <<- bCI %*% bbeta                                         # rescale
    
    bXbeta <<- bX %*% bbeta                                         # updated vector bX (to use below)
    
    s1b1 <- bbeta[1,1]
    s1b2 <- bbeta[2,1]
    s1b3 <- bbeta[3,1]
    s2b1 <- bbeta[4,1]
    s2b2 <- bbeta[5,1]
    s2b3 <- bbeta[6,1]
    
    # Update intercepts conditional on the common coefficient(s)
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s1b1']] <<- s1b1
    model[['s1b2']] <<- s1b2
    model[['s1b3']] <<- s1b3
    model[['s2ag']] <<- s2ag
    model[['s2b1']] <<- s2b1
    model[['s2b2']] <<- s2b2
    model[['s2b3']] <<- s2b3
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptf_b1c_b1c 

# Custom Gibbs sampler for regression coefficients in the common cubic-quadratic trend model with a full break
sampler_CP_xptf_b1c_b1q <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 4 here)
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    
    # Overall dimensionality (p == 7 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2
    
    # Make sure the 'target' values are correctly specified
    if (p != 7 | lt != p) {
      nimStop("All segment-specific intercepts and regression coefficients are all expected to be sampled in 'sampler_CP_xptf_b1c_b1q'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's1b1') {
          nimStop("2nd target node must be the common linear coefficient 's1b1'.")
        } else {
          if (target[3] != 's1b2') {
            nimStop("3rd target node must be the common quadratic coefficient 's1b2'.")
          } else {
            if (target[4] != 's1b3') {
              nimStop("4th target node must be the common cubic coefficient 's1b3'.")
            } else {
              if (target[5] != 's2ag') {
                nimStop("5th target node must be the vector (length g) of intercepts 's2ag'.")
              } else {
                if (target[6] != 's2b1') {
                  nimStop("6th target node must be the common linear coefficient 's2b1'.")
                } else {
                  if (target[7] != 's2b2') {
                    nimStop("7th target node must be the common quadratic coefficient 's2b2'.")
                  } 
                }
              }
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bmbetag <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDbetag <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXt <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXtW <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bprbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bpbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCC <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    tbCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s1b1 <- model[['s1b1']]
    s1b2 <- model[['s1b2']]
    s1b3 <- model[['s1b3']]
    s2ag <- model[['s2ag']]
    s2b1 <- model[['s2b1']]
    s2b2 <- model[['s2b2']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    ambetag[1,1] <<- mbetag[1,1]
    ambetag[2,1] <<- mbetag[1+s1p,1]
    aDbetag <<- nimMatrix(0, 2L, 2L)
    aDbetag[1,1] <<- Dbetag[1,1]
    aDbetag[2,2] <<- Dbetag[1+s1p,1+s1p]
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    for (j in 1:n) {
      for (k in 2:s1p) {
        bX[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    bDbetag <<- nimMatrix(0, pm2, pm2)
    for (k in 2:s1p) {
      bmbetag[k-1,1] <<- mbetag[k,1]
      bDbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      bmbetag[k-2,1] <<- mbetag[k,1]
      bDbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    bXt <<- t(bX)                                                   # transpose bX
    bprbeta <<- bDbetag %*% bmbetag                                 # contribution to posterior mean from prior
    
    sumbXtWX <<- nimMatrix(0, pm2, pm2)                             # initialize cumulative sums
    sumbzbeta <<- nimMatrix(0, pm2, 1L)
    
    # Update common coefficient(s) conditional on intercepts
    for (i in 1:g) {                                                # cycle through each group independently
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector segment 2
      aXbeta <<- aX %*% abeta                                       # contribution from intercepts
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      bXtW <<- bXt %*% Wg                                           # multiply bXt and Wg
      bXtWX <<- bXtW %*% bX                                         # calculate bXtWX
      sumbXtWX <<- sumbXtWX + bXtWX                                 # cumulative matrix sum
      bzbeta <<- bXtW %*% Zvec                                      # contributions to posterior mean from WLS
      sumbzbeta <<- sumbzbeta + bzbeta                              # cumulative matrix sum
    }                                                               # end cycle through groups
    bDXtWX <<- bDbetag + sumbXtWX                                   # posterior precision matrix for common beta is bDbetag + sumbXtWX
    bpbeta <<- bprbeta + sumbzbeta                                  # sum of prior and the cumulative WLS contributions
    
    for (k in 1:pm2) {
      bbeta[k,1] <<- rnorm(1, 0, sd = 1)                            # sample from univariate standard normal(s)
    }
    bCC <<- chol(bDXtWX)                                            # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
    bCI <<- inverse(bCC)                                            # inverse of upper triangular matrix from Cholesky decomposition
    tbCI <<- t(bCI)                                                 # transpose
    bpbeta <<- tbCI %*% bpbeta                                      # rescale
    bbeta <<- bpbeta + bbeta                                        # center
    bbeta <<- bCI %*% bbeta                                         # rescale
    
    bXbeta <<- bX %*% bbeta                                         # updated vector bX (to use below)
    
    s1b1 <- bbeta[1,1]
    s1b2 <- bbeta[2,1]
    s1b3 <- bbeta[3,1]
    s2b1 <- bbeta[4,1]
    s2b2 <- bbeta[5,1]
    
    # Update intercepts conditional on the common coefficient(s)
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s1b1']] <<- s1b1
    model[['s1b2']] <<- s1b2
    model[['s1b3']] <<- s1b3
    model[['s2ag']] <<- s2ag
    model[['s2b1']] <<- s2b1
    model[['s2b2']] <<- s2b2
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptf_b1c_b1q 

# Custom Gibbs sampler for regression coefficients in the common cubic-linear trend model with a full break
sampler_CP_xptf_b1c_b1l <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 4 here)
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    
    # Overall dimensionality (p == 6 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2
    
    # Make sure the 'target' values are correctly specified
    if (p != 6 | lt != p) {
      nimStop("All segment-specific intercepts and regression coefficients are all expected to be sampled in 'sampler_CP_xptf_b1c_b1l'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's1b1') {
          nimStop("2nd target node must be the common linear coefficient 's1b1'.")
        } else {
          if (target[3] != 's1b2') {
            nimStop("3rd target node must be the common quadratic coefficient 's1b2'.")
          } else {
            if (target[4] != 's1b3') {
              nimStop("4th target node must be the common cubic coefficient 's1b3'.")
            } else {
              if (target[5] != 's2ag') {
                nimStop("5th target node must be the vector (length g) of intercepts 's2ag'.")
              } else {
                if (target[6] != 's2b1') {
                  nimStop("6th target node must be the common linear coefficient 's2b1'.")
                } 
              }
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bmbetag <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDbetag <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXt <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXtW <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bprbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bpbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCC <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    tbCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s1b1 <- model[['s1b1']]
    s1b2 <- model[['s1b2']]
    s1b3 <- model[['s1b3']]
    s2ag <- model[['s2ag']]
    s2b1 <- model[['s2b1']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    ambetag[1,1] <<- mbetag[1,1]
    ambetag[2,1] <<- mbetag[1+s1p,1]
    aDbetag <<- nimMatrix(0, 2L, 2L)
    aDbetag[1,1] <<- Dbetag[1,1]
    aDbetag[2,2] <<- Dbetag[1+s1p,1+s1p]
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    for (j in 1:n) {
      for (k in 2:s1p) {
        bX[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    bDbetag <<- nimMatrix(0, pm2, pm2)
    for (k in 2:s1p) {
      bmbetag[k-1,1] <<- mbetag[k,1]
      bDbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      bmbetag[k-2,1] <<- mbetag[k,1]
      bDbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    bXt <<- t(bX)                                                   # transpose bX
    bprbeta <<- bDbetag %*% bmbetag                                 # contribution to posterior mean from prior
    
    sumbXtWX <<- nimMatrix(0, pm2, pm2)                             # initialize cumulative sums
    sumbzbeta <<- nimMatrix(0, pm2, 1L)
    
    # Update common coefficient(s) conditional on intercepts
    for (i in 1:g) {                                                # cycle through each group independently
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector segment 2
      aXbeta <<- aX %*% abeta                                       # contribution from intercepts
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      bXtW <<- bXt %*% Wg                                           # multiply bXt and Wg
      bXtWX <<- bXtW %*% bX                                         # calculate bXtWX
      sumbXtWX <<- sumbXtWX + bXtWX                                 # cumulative matrix sum
      bzbeta <<- bXtW %*% Zvec                                      # contributions to posterior mean from WLS
      sumbzbeta <<- sumbzbeta + bzbeta                              # cumulative matrix sum
    }                                                               # end cycle through groups
    bDXtWX <<- bDbetag + sumbXtWX                                   # posterior precision matrix for common beta is bDbetag + sumbXtWX
    bpbeta <<- bprbeta + sumbzbeta                                  # sum of prior and the cumulative WLS contributions
    
    for (k in 1:pm2) {
      bbeta[k,1] <<- rnorm(1, 0, sd = 1)                            # sample from univariate standard normal(s)
    }
    bCC <<- chol(bDXtWX)                                            # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
    bCI <<- inverse(bCC)                                            # inverse of upper triangular matrix from Cholesky decomposition
    tbCI <<- t(bCI)                                                 # transpose
    bpbeta <<- tbCI %*% bpbeta                                      # rescale
    bbeta <<- bpbeta + bbeta                                        # center
    bbeta <<- bCI %*% bbeta                                         # rescale
    
    bXbeta <<- bX %*% bbeta                                         # updated vector bX (to use below)
    
    s1b1 <- bbeta[1,1]
    s1b2 <- bbeta[2,1]
    s1b3 <- bbeta[3,1]
    s2b1 <- bbeta[4,1]
    
    # Update intercepts conditional on the common coefficient(s)
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s1b1']] <<- s1b1
    model[['s1b2']] <<- s1b2
    model[['s1b3']] <<- s1b3
    model[['s2ag']] <<- s2ag
    model[['s2b1']] <<- s2b1
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptf_b1c_b1l 

# Custom Gibbs sampler for regression coefficients in the common quadratic trend model
sampler_CP_b1q <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 3 here)
    pm1 <- 0L
    p <- model$getConstants()$p
    pm1 <- p-1
    
    # Make sure the 'target' values are correctly specified
    if (p != 3 | lt != p) {
      nimStop("The intercepts and both regression coefficients are all expected to be sampled in 'sampler_CP_b1q'.")
    } else {
      if (target[1] != 'ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 'ag'.")
      } else {
        if (target[2] != 'b1') {
          nimStop("2nd target node must be the common linear coefficient 'b1'.")
        } else {
          if (target[3] != 'b2') {
            nimStop("3rd target node must be the common quadratic coefficient 'b2'.")
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bX <- nimMatrix(nrow=n, ncol=pm1, init = FALSE)
    ambetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bmbetag <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bDbetag <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    sumbXtWX <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    sumbzbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    bXt <- nimMatrix(nrow=pm1, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    bXtW <- nimMatrix(nrow=pm1, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bXtWX <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    aDXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bDXtWX <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    aprbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bprbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)  
    bpbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)  
    azbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bzbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)  
    bbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)  
    aCC <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bCC <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    aCI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    taCI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bCI <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    tbCI <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    ag <- model[['ag']]
    b1 <- model[['b1']]
    b2 <- model[['b2']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
    }
    ambetag[1,1] <<- mbetag[1,1]
    aDbetag[1,1] <<- Dbetag[1,1]
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    for (j in 1:n) {
      for (k in 2:p) {
        bX[j,k-1] <<- X[j,k]
      }
    }
    bDbetag <<- nimMatrix(0, pm1, pm1)
    for (k in 2:p) {
      bmbetag[k-1,1] <<- mbetag[k,1]
      bDbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    bXt <<- t(bX)                                                   # transpose bX
    bprbeta <<- bDbetag %*% bmbetag                                 # contribution to posterior mean from prior
    
    sumbXtWX <<- nimMatrix(0, pm1, pm1)                             # initialize cumulative sums
    sumbzbeta <<- nimMatrix(0, pm1, 1L)
    
    # Update common coefficient(s) conditional on intercepts
    for (i in 1:g) {                                                # cycle through each group independently
      abeta[1,1] <<- ag[i]                                          # group-specific abeta vector
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      bXtW <<- bXt %*% Wg                                           # multiply bXt and Wg
      bXtWX <<- bXtW %*% bX                                         # calculate bXtWX
      sumbXtWX <<- sumbXtWX + bXtWX                                 # cumulative matrix sum
      bzbeta <<- bXtW %*% Zvec                                      # contributions to posterior mean from WLS
      sumbzbeta <<- sumbzbeta + bzbeta                              # cumulative matrix sum
    }                                                               # end cycle through groups
    bDXtWX <<- bDbetag + sumbXtWX                                   # posterior precision matrix for common beta is bDbetag + sumbXtWX
    bpbeta <<- bprbeta + sumbzbeta                                  # sum of prior and the cumulative WLS contributions
    
    for (k in 1:pm1) {
      bbeta[k,1] <<- rnorm(1, 0, sd = 1)                            # sample from univariate standard normal(s)
    }
    bCC <<- chol(bDXtWX)                                            # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
    bCI <<- inverse(bCC)                                            # inverse of upper triangular matrix from Cholesky decomposition
    tbCI <<- t(bCI)                                                 # transpose
    bpbeta <<- tbCI %*% bpbeta                                      # rescale
    bbeta <<- bpbeta + bbeta                                        # center
    bbeta <<- bCI %*% bbeta                                         # rescale
    
    bXbeta <<- bX %*% bbeta                                         # updated vector bX (to use below)
    
    b1 <- bbeta[1,1]
    b2 <- bbeta[2,1]
    
    # Update intercepts conditional on the common coefficient(s)
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      ag[i] <- abeta[1,1]
    }
    
    # Update model with new node values
    model[['ag']] <<- ag
    model[['b1']] <<- b1
    model[['b2']] <<- b2
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_b1q 

# Custom Gibbs sampler for regression coefficients in the common quadratic trend model with a level shift
sampler_CP_xptl_b1q <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 4 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2
    
    # Make sure the 'target' values are correctly specified
    if (p != 4 | lt != p) {
      nimStop("The two intercepts and two regression coefficients are all expected to be sampled in 'sampler_CP_xptl_b1q'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's2ag') {
          nimStop("2nd target node must be the vector (length g) of intercepts 's2ag'.")
        } else {
          if (target[3] != 'b1') {
            nimStop("3rd target node must be the common linear coefficient 'b1'.")
          } else {
            if (target[4] != 'b2') {
              nimStop("4th target node must be the common quadratic coefficient 'b2'.")
            } 
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bmbetag <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDbetag <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXt <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXtW <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bprbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bpbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCC <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    tbCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    b1   <- model[['b1']]
    b2   <- model[['b2']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,2]
    }
    aDbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 1:2) {
      ambetag[k,1] <<- mbetag[k,1]
      aDbetag[k,k] <<- Dbetag[k,k]
    }
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    for (j in 1:n) {
      for (k in 3:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    bDbetag <<- nimMatrix(0, pm2, pm2)
    for (k in 3:p) {
      bmbetag[k-2,1] <<- mbetag[k,1]
      bDbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    bXt <<- t(bX)                                                   # transpose bX
    bprbeta <<- bDbetag %*% bmbetag                                 # contribution to posterior mean from prior
    
    sumbXtWX <<- nimMatrix(0, pm2, pm2)                             # initialize cumulative sums
    sumbzbeta <<- nimMatrix(0, pm2, 1L)
    
    # Update common coefficient(s) conditional on intercepts
    for (i in 1:g) {                                                # cycle through each group independently
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector segment 2
      aXbeta <<- aX %*% abeta                                       # contribution from intercepts
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      bXtW <<- bXt %*% Wg                                           # multiply bXt and Wg
      bXtWX <<- bXtW %*% bX                                         # calculate bXtWX
      sumbXtWX <<- sumbXtWX + bXtWX                                 # cumulative matrix sum
      bzbeta <<- bXtW %*% Zvec                                      # contributions to posterior mean from WLS
      sumbzbeta <<- sumbzbeta + bzbeta                              # cumulative matrix sum
    }                                                               # end cycle through groups
    bDXtWX <<- bDbetag + sumbXtWX                                   # posterior precision matrix for common beta is bDbetag + sumbXtWX
    bpbeta <<- bprbeta + sumbzbeta                                  # sum of prior and the cumulative WLS contributions
    
    for (k in 1:pm2) {
      bbeta[k,1] <<- rnorm(1, 0, sd = 1)                            # sample from univariate standard normal(s)
    }
    bCC <<- chol(bDXtWX)                                            # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
    bCI <<- inverse(bCC)                                            # inverse of upper triangular matrix from Cholesky decomposition
    tbCI <<- t(bCI)                                                 # transpose
    bpbeta <<- tbCI %*% bpbeta                                      # rescale
    bbeta <<- bpbeta + bbeta                                        # center
    bbeta <<- bCI %*% bbeta                                         # rescale
    
    bXbeta <<- bX %*% bbeta                                         # updated vector bX (to use below)
    
    b1 <- bbeta[1,1]
    b2 <- bbeta[2,1]
    
    # Update intercepts conditional on the common coefficient(s)
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    model[['b1']]   <<- b1
    model[['b2']]   <<- b2
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptl_b1q 

# Custom Gibbs sampler for regression coefficients in the common quadratic-quadratic trend model with a full break
sampler_CP_xptf_b1q_b1q <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 3 here)
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    
    # Overall dimensionality (p == 6 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2
    
    # Make sure the 'target' values are correctly specified
    if (p != 6 | lt != p) {
      nimStop("All segment-specific intercepts and regression coefficients are all expected to be sampled in 'sampler_CP_xptf_b1q_b1q'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's1b1') {
          nimStop("2nd target node must be the common linear coefficient 's1b1'.")
        } else {
          if (target[3] != 's1b2') {
            nimStop("3rd target node must be the common quadratic coefficient 's1b2'.")
          } else {
            if (target[4] != 's2ag') {
              nimStop("4th target node must be the vector (length g) of intercepts 's2ag'.")
            } else {
              if (target[5] != 's2b1') {
                nimStop("5th target node must be the common linear coefficient 's2b1'.")
              } else {
                if (target[6] != 's2b2') {
                  nimStop("6th target node must be the common quadratic coefficient 's2b2'.")
                } 
              }
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bmbetag <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDbetag <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXt <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXtW <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bprbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bpbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCC <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    tbCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s1b1 <- model[['s1b1']]
    s1b2 <- model[['s1b2']]
    s2ag <- model[['s2ag']]
    s2b1 <- model[['s2b1']]
    s2b2 <- model[['s2b2']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    ambetag[1,1] <<- mbetag[1,1]
    ambetag[2,1] <<- mbetag[1+s1p,1]
    aDbetag <<- nimMatrix(0, 2L, 2L)
    aDbetag[1,1] <<- Dbetag[1,1]
    aDbetag[2,2] <<- Dbetag[1+s1p,1+s1p]
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    for (j in 1:n) {
      for (k in 2:s1p) {
        bX[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    bDbetag <<- nimMatrix(0, pm2, pm2)
    for (k in 2:s1p) {
      bmbetag[k-1,1] <<- mbetag[k,1]
      bDbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      bmbetag[k-2,1] <<- mbetag[k,1]
      bDbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    bXt <<- t(bX)                                                   # transpose bX
    bprbeta <<- bDbetag %*% bmbetag                                 # contribution to posterior mean from prior
    
    sumbXtWX <<- nimMatrix(0, pm2, pm2)                             # initialize cumulative sums
    sumbzbeta <<- nimMatrix(0, pm2, 1L)
    
    # Update common coefficient(s) conditional on intercepts
    for (i in 1:g) {                                                # cycle through each group independently
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector segment 2
      aXbeta <<- aX %*% abeta                                       # contribution from intercepts
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      bXtW <<- bXt %*% Wg                                           # multiply bXt and Wg
      bXtWX <<- bXtW %*% bX                                         # calculate bXtWX
      sumbXtWX <<- sumbXtWX + bXtWX                                 # cumulative matrix sum
      bzbeta <<- bXtW %*% Zvec                                      # contributions to posterior mean from WLS
      sumbzbeta <<- sumbzbeta + bzbeta                              # cumulative matrix sum
    }                                                               # end cycle through groups
    bDXtWX <<- bDbetag + sumbXtWX                                   # posterior precision matrix for common beta is bDbetag + sumbXtWX
    bpbeta <<- bprbeta + sumbzbeta                                  # sum of prior and the cumulative WLS contributions
    
    for (k in 1:pm2) {
      bbeta[k,1] <<- rnorm(1, 0, sd = 1)                            # sample from univariate standard normal(s)
    }
    bCC <<- chol(bDXtWX)                                            # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
    bCI <<- inverse(bCC)                                            # inverse of upper triangular matrix from Cholesky decomposition
    tbCI <<- t(bCI)                                                 # transpose
    bpbeta <<- tbCI %*% bpbeta                                      # rescale
    bbeta <<- bpbeta + bbeta                                        # center
    bbeta <<- bCI %*% bbeta                                         # rescale
    
    bXbeta <<- bX %*% bbeta                                         # updated vector bX (to use below)
    
    s1b1 <- bbeta[1,1]
    s1b2 <- bbeta[2,1]
    s2b1 <- bbeta[3,1]
    s2b2 <- bbeta[4,1]
    
    # Update intercepts conditional on the common coefficient(s)
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s1b1']] <<- s1b1
    model[['s1b2']] <<- s1b2
    model[['s2ag']] <<- s2ag
    model[['s2b1']] <<- s2b1
    model[['s2b2']] <<- s2b2
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptf_b1q_b1q 

# Custom Gibbs sampler for regression coefficients in the common quadratic-linear trend model with a full break
sampler_CP_xptf_b1q_b1l <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 3 here)
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    
    # Overall dimensionality (p == 5 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2
    
    # Make sure the 'target' values are correctly specified
    if (p != 5 | lt != p) {
      nimStop("All segment-specific intercepts and regression coefficients are all expected to be sampled in 'sampler_CP_xptf_b1q_b1l'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's1b1') {
          nimStop("2nd target node must be the common linear coefficient 's1b1'.")
        } else {
          if (target[3] != 's1b2') {
            nimStop("3rd target node must be the common quadratic coefficient 's1b2'.")
          } else {
            if (target[4] != 's2ag') {
              nimStop("4th target node must be the vector (length g) of intercepts 's2ag'.")
            } else {
              if (target[5] != 's2b1') {
                nimStop("5th target node must be the common linear coefficient 's2b1'.")
              }
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bmbetag <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDbetag <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXt <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXtW <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bprbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bpbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCC <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    tbCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s1b1 <- model[['s1b1']]
    s1b2 <- model[['s1b2']]
    s2ag <- model[['s2ag']]
    s2b1 <- model[['s2b1']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    ambetag[1,1] <<- mbetag[1,1]
    ambetag[2,1] <<- mbetag[1+s1p,1]
    aDbetag <<- nimMatrix(0, 2L, 2L)
    aDbetag[1,1] <<- Dbetag[1,1]
    aDbetag[2,2] <<- Dbetag[1+s1p,1+s1p]
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    for (j in 1:n) {
      for (k in 2:s1p) {
        bX[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    bDbetag <<- nimMatrix(0, pm2, pm2)
    for (k in 2:s1p) {
      bmbetag[k-1,1] <<- mbetag[k,1]
      bDbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      bmbetag[k-2,1] <<- mbetag[k,1]
      bDbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    bXt <<- t(bX)                                                   # transpose bX
    bprbeta <<- bDbetag %*% bmbetag                                 # contribution to posterior mean from prior
    
    sumbXtWX <<- nimMatrix(0, pm2, pm2)                             # initialize cumulative sums
    sumbzbeta <<- nimMatrix(0, pm2, 1L)
    
    # Update common coefficient(s) conditional on intercepts
    for (i in 1:g) {                                                # cycle through each group independently
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector segment 2
      aXbeta <<- aX %*% abeta                                       # contribution from intercepts
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      bXtW <<- bXt %*% Wg                                           # multiply bXt and Wg
      bXtWX <<- bXtW %*% bX                                         # calculate bXtWX
      sumbXtWX <<- sumbXtWX + bXtWX                                 # cumulative matrix sum
      bzbeta <<- bXtW %*% Zvec                                      # contributions to posterior mean from WLS
      sumbzbeta <<- sumbzbeta + bzbeta                              # cumulative matrix sum
    }                                                               # end cycle through groups
    bDXtWX <<- bDbetag + sumbXtWX                                   # posterior precision matrix for common beta is bDbetag + sumbXtWX
    bpbeta <<- bprbeta + sumbzbeta                                  # sum of prior and the cumulative WLS contributions
    
    for (k in 1:pm2) {
      bbeta[k,1] <<- rnorm(1, 0, sd = 1)                            # sample from univariate standard normal(s)
    }
    bCC <<- chol(bDXtWX)                                            # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
    bCI <<- inverse(bCC)                                            # inverse of upper triangular matrix from Cholesky decomposition
    tbCI <<- t(bCI)                                                 # transpose
    bpbeta <<- tbCI %*% bpbeta                                      # rescale
    bbeta <<- bpbeta + bbeta                                        # center
    bbeta <<- bCI %*% bbeta                                         # rescale
    
    bXbeta <<- bX %*% bbeta                                         # updated vector bX (to use below)
    
    s1b1 <- bbeta[1,1]
    s1b2 <- bbeta[2,1]
    s2b1 <- bbeta[3,1]
    
    # Update intercepts conditional on the common coefficient(s)
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s1b1']] <<- s1b1
    model[['s1b2']] <<- s1b2
    model[['s2ag']] <<- s2ag
    model[['s2b1']] <<- s2b1
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptf_b1q_b1l 

# Custom Gibbs sampler for regression coefficients in the common linear trend model
sampler_CP_b1l <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 2 here)
    pm1 <- 0L
    p <- model$getConstants()$p
    pm1 <- p-1
    
    # Make sure the 'target' values are correctly specified
    if (p != 2 | lt != p) {
      nimStop("The intercepts and linear regression coefficient are all expected to be sampled in 'sampler_CP_b1l'.")
    } else {
      if (target[1] != 'ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 'ag'.")
      } else {
        if (target[2] != 'b1') {
          nimStop("2nd target node must be the common linear coefficient 'b1'.")
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bX <- nimMatrix(nrow=n, ncol=pm1, init = FALSE)
    ambetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bmbetag <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bDbetag <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    sumbXtWX <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    sumbzbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    bXt <- nimMatrix(nrow=pm1, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    bXtW <- nimMatrix(nrow=pm1, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bXtWX <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    aDXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bDXtWX <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    aprbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bprbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)  
    bpbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)  
    azbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bzbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)  
    bbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)  
    aCC <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bCC <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    aCI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    taCI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    bCI <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    tbCI <- nimMatrix(nrow=pm1, ncol=pm1, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    ag <- model[['ag']]
    b1 <- model[['b1']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
    }
    ambetag[1,1] <<- mbetag[1,1]
    aDbetag[1,1] <<- Dbetag[1,1]
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    for (j in 1:n) {
      for (k in 2:p) {
        bX[j,k-1] <<- X[j,k]
      }
    }
    bDbetag <<- nimMatrix(0, pm1, pm1)
    for (k in 2:p) {
      bmbetag[k-1,1] <<- mbetag[k,1]
      bDbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    bXt <<- t(bX)                                                   # transpose bX
    bprbeta <<- bDbetag %*% bmbetag                                 # contribution to posterior mean from prior
    
    sumbXtWX <<- nimMatrix(0, pm1, pm1)                             # initialize cumulative sums
    sumbzbeta <<- nimMatrix(0, pm1, 1L)
    
    # Update common coefficient(s) conditional on intercepts
    for (i in 1:g) {                                                # cycle through each group independently
      abeta[1,1] <<- ag[i]                                          # group-specific abeta vector
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      bXtW <<- bXt %*% Wg                                           # multiply bXt and Wg
      bXtWX <<- bXtW %*% bX                                         # calculate bXtWX
      sumbXtWX <<- sumbXtWX + bXtWX                                 # cumulative matrix sum
      bzbeta <<- bXtW %*% Zvec                                      # contributions to posterior mean from WLS
      sumbzbeta <<- sumbzbeta + bzbeta                              # cumulative matrix sum
    }                                                               # end cycle through groups
    bDXtWX <<- bDbetag + sumbXtWX                                   # posterior precision matrix for common beta is bDbetag + sumbXtWX
    bpbeta <<- bprbeta + sumbzbeta                                  # sum of prior and the cumulative WLS contributions
    
    for (k in 1:pm1) {
      bbeta[k,1] <<- rnorm(1, 0, sd = 1)                            # sample from univariate standard normal(s)
    }
    bCC <<- chol(bDXtWX)                                            # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
    bCI <<- inverse(bCC)                                            # inverse of upper triangular matrix from Cholesky decomposition
    tbCI <<- t(bCI)                                                 # transpose
    bpbeta <<- tbCI %*% bpbeta                                      # rescale
    bbeta <<- bpbeta + bbeta                                        # center
    bbeta <<- bCI %*% bbeta                                         # rescale
    
    bXbeta <<- bX %*% bbeta                                         # updated vector bX (to use below)
    
    b1 <- bbeta[1,1]
    
    # Update intercepts conditional on the common coefficient(s)
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      ag[i] <- abeta[1,1]
    }
    
    # Update model with new node values
    model[['ag']] <<- ag
    model[['b1']] <<- b1
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_b1l 

# Custom Gibbs sampler for regression coefficients in the common linear trend model with a level shift
sampler_CP_xptl_b1l <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 3 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2
    
    # Make sure the 'target' values are correctly specified
    if (p != 3 | lt != p) {
      nimStop("The two intercepts and the regression coefficient are all expected to be sampled in 'sampler_CP_xptl_b1l'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's2ag') {
          nimStop("2nd target node must be the vector (length g) of intercepts 's2ag'.")
        } else {
          if (target[3] != 'b1') {
            nimStop("3rd target node must be the common linear coefficient 'b1'.")
          } 
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bmbetag <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDbetag <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXt <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXtW <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bprbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bpbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCC <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    tbCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    b1   <- model[['b1']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,2]
    }
    aDbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 1:2) {
      ambetag[k,1] <<- mbetag[k,1]
      aDbetag[k,k] <<- Dbetag[k,k]
    }
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    for (j in 1:n) {
      for (k in 3:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    bDbetag <<- nimMatrix(0, pm2, pm2)
    for (k in 3:p) {
      bmbetag[k-2,1] <<- mbetag[k,1]
      bDbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    bXt <<- t(bX)                                                   # transpose bX
    bprbeta <<- bDbetag %*% bmbetag                                 # contribution to posterior mean from prior
    
    sumbXtWX <<- nimMatrix(0, pm2, pm2)                             # initialize cumulative sums
    sumbzbeta <<- nimMatrix(0, pm2, 1L)
    
    # Update common coefficient(s) conditional on intercepts
    for (i in 1:g) {                                                # cycle through each group independently
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector segment 2
      aXbeta <<- aX %*% abeta                                       # contribution from intercepts
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      bXtW <<- bXt %*% Wg                                           # multiply bXt and Wg
      bXtWX <<- bXtW %*% bX                                         # calculate bXtWX
      sumbXtWX <<- sumbXtWX + bXtWX                                 # cumulative matrix sum
      bzbeta <<- bXtW %*% Zvec                                      # contributions to posterior mean from WLS
      sumbzbeta <<- sumbzbeta + bzbeta                              # cumulative matrix sum
    }                                                               # end cycle through groups
    bDXtWX <<- bDbetag + sumbXtWX                                   # posterior precision matrix for common beta is bDbetag + sumbXtWX
    bpbeta <<- bprbeta + sumbzbeta                                  # sum of prior and the cumulative WLS contributions
    
    for (k in 1:pm2) {
      bbeta[k,1] <<- rnorm(1, 0, sd = 1)                            # sample from univariate standard normal(s)
    }
    bCC <<- chol(bDXtWX)                                            # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
    bCI <<- inverse(bCC)                                            # inverse of upper triangular matrix from Cholesky decomposition
    tbCI <<- t(bCI)                                                 # transpose
    bpbeta <<- tbCI %*% bpbeta                                      # rescale
    bbeta <<- bpbeta + bbeta                                        # center
    bbeta <<- bCI %*% bbeta                                         # rescale
    
    bXbeta <<- bX %*% bbeta                                         # updated vector bX (to use below)
    
    b1 <- bbeta[1,1]
    
    # Update intercepts conditional on the common coefficient(s)
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    model[['b1']]   <<- b1
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptl_b1l 

# Custom Gibbs sampler for regression coefficients in the common linear-linear trend model with a full break
sampler_CP_xptf_b1l_b1l <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 2 here)
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    
    # Overall dimensionality (p == 4 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2
    
    # Make sure the 'target' values are correctly specified
    if (p != 4 | lt != p) {
      nimStop("All segment-specific intercepts and regression coefficients are all expected to be sampled in 'sampler_CP_xptf_b1l_b1l'.")
    } else {
      if (target[1] != 's1ag') {
        nimStop("1st target node must be the vector (length g) of intercepts 's1ag'.")
      } else {
        if (target[2] != 's1b1') {
          nimStop("2nd target node must be the common linear coefficient 's1b1'.")
        } else {
          if (target[3] != 's2ag') {
            nimStop("3rd target node must be the vector (length g) of intercepts 's2ag'.")
          } else {
            if (target[4] != 's2b1') {
              nimStop("4th target node must be the common linear coefficient 's2b1'.")
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    # Preallocate storage for matrix calculations
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bmbetag <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDbetag <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    sumbzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXt <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    bXtW <- nimMatrix(nrow=pm2, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bDXtWX <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bprbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bpbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    bzbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)  
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)  
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCC <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    bCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    tbCI <- nimMatrix(nrow=pm2, ncol=pm2, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE)
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s1b1 <- model[['s1b1']]
    s2ag <- model[['s2ag']]
    s2b1 <- model[['s2b1']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    ambetag[1,1] <<- mbetag[1,1]
    ambetag[2,1] <<- mbetag[1+s1p,1]
    aDbetag <<- nimMatrix(0, 2L, 2L)
    aDbetag[1,1] <<- Dbetag[1,1]
    aDbetag[2,2] <<- Dbetag[1+s1p,1+s1p]
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    for (j in 1:n) {
      for (k in 2:s1p) {
        bX[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    bDbetag <<- nimMatrix(0, pm2, pm2)
    for (k in 2:s1p) {
      bmbetag[k-1,1] <<- mbetag[k,1]
      bDbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      bmbetag[k-2,1] <<- mbetag[k,1]
      bDbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    bXt <<- t(bX)                                                   # transpose bX
    bprbeta <<- bDbetag %*% bmbetag                                 # contribution to posterior mean from prior
    
    sumbXtWX <<- nimMatrix(0, pm2, pm2)                             # initialize cumulative sums
    sumbzbeta <<- nimMatrix(0, pm2, 1L)
    
    # Update common coefficient(s) conditional on intercepts
    for (i in 1:g) {                                                # cycle through each group independently
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector segment 2
      aXbeta <<- aX %*% abeta                                       # contribution from intercepts
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      bXtW <<- bXt %*% Wg                                           # multiply bXt and Wg
      bXtWX <<- bXtW %*% bX                                         # calculate bXtWX
      sumbXtWX <<- sumbXtWX + bXtWX                                 # cumulative matrix sum
      bzbeta <<- bXtW %*% Zvec                                      # contributions to posterior mean from WLS
      sumbzbeta <<- sumbzbeta + bzbeta                              # cumulative matrix sum
    }                                                               # end cycle through groups
    bDXtWX <<- bDbetag + sumbXtWX                                   # posterior precision matrix for common beta is bDbetag + sumbXtWX
    bpbeta <<- bprbeta + sumbzbeta                                  # sum of prior and the cumulative WLS contributions
    
    for (k in 1:pm2) {
      bbeta[k,1] <<- rnorm(1, 0, sd = 1)                            # sample from univariate standard normal(s)
    }
    bCC <<- chol(bDXtWX)                                            # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
    bCI <<- inverse(bCC)                                            # inverse of upper triangular matrix from Cholesky decomposition
    tbCI <<- t(bCI)                                                 # transpose
    bpbeta <<- tbCI %*% bpbeta                                      # rescale
    bbeta <<- bpbeta + bbeta                                        # center
    bbeta <<- bCI %*% bbeta                                         # rescale
    
    bXbeta <<- bX %*% bbeta                                         # updated vector bX (to use below)
    
    s1b1 <- bbeta[1,1]
    s2b1 <- bbeta[2,1]
    
    # Update intercepts conditional on the common coefficient(s)
    for (i in 1:g) {                                                # cycle through each group independently
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s1b1']] <<- s1b1
    model[['s2ag']] <<- s2ag
    model[['s2b1']] <<- s2b1
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CP_xptf_b1l_b1l 

############################################################################################################
# eMKF: Custom Gibbs samplers for intercepts conditional on trend coefficients in the supported BMA models #
############################################################################################################

# Custom Gibbs sampler for intercepts conditional on trend coefficients in the cubic BMA model
sampler_CPa_bmac <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 4 here)
    pm1 <- 0L
    p <- model$getConstants()$p
    pm1 <- p-1 
    
    # Make sure the 'target' values are correctly specified
    if (p != 4 | lt != 1) {
      nimStop("Dimensionality 'p' must be 4 in 'sampler_CPa_bmac' and only node 'ag' can be sampled.")
    } else {
      if (target[1] != 'ag') {
        nimStop("Target node must be the vector (length g) of intercepts 'ag'.")
      } 
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    bX <- nimMatrix(nrow=n, ncol=pm1, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)
    
    ambetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aDXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aprbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    azbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aCC <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aCI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    taCI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and trend coefficients (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    b1g <- model[['b1g']]
    b2g <- model[['b2g']]
    b3g <- model[['b3g']]
    
    # Node values to be replaced
    ag <- model[['ag']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      for (k in 2:p) {
        bX[j,k-1] <<- X[j,k]
      }
    }
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]                                            # intercepts column
    }
    ambetag[1,1] <<- mbetag[1,1]                                    # prior mean for intercepts
    aDbetag[1,1] <<- Dbetag[1,1]                                    # prior precision for intercepts
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    # Generate proposal for intercepts conditional on the betas
    for (i in 1:g) {                                                # cycle through each group independently
      bbeta[1,1] <<- b1g[i]
      bbeta[2,1] <<- b2g[i]
      bbeta[3,1] <<- b3g[i]
      bXbeta <<- bX %*% bbeta
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      ag[i] <- abeta[1,1]
    }
    
    # Update model with new node values
    model[['ag']] <<- ag
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPa_bmac 

# Custom Gibbs sampler for intercepts conditional on trend coefficients in the cubic BMA model with a level shift
sampler_CPa_xptl_bmac <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 5 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2 
    
    # Make sure the 'target' values are correctly specified
    if (p != 5 | lt != 2) {
      nimStop("Dimensionality 'p' must be 5 in 'sampler_CPa_xptl_bmac' and only nodes 's1ag' and 's2ag' can be sampled.")
    } else {
      if (target[1] != 's1ag' | target[2] != 's2ag') {
        nimStop("Target nodes must be the vectors (length g each) of intercepts 's1ag' and 's2ag'.")
      } 
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and trend coefficients (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    b1g <- model[['b1g']]
    b2g <- model[['b2g']]
    b3g <- model[['b3g']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      for (k in 3:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]                                          # intercepts columns
      aX[j,2] <<- X[j,2]
    }
    aDbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 1:2) {
      ambetag[k,1] <<- mbetag[k,1]
      aDbetag[k,k] <<- Dbetag[k,k]
    }
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    # Generate proposal for intercepts conditional on the betas
    for (i in 1:g) {                                                # cycle through each group independently
      bbeta[1,1] <<- b1g[i]
      bbeta[2,1] <<- b2g[i]
      bbeta[3,1] <<- b3g[i]
      bXbeta <<- bX %*% bbeta
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPa_xptl_bmac

# Custom Gibbs sampler for intercepts conditional on trend coefficients in the cubic-cubic BMA model with a full trend break
sampler_CPa_xptf_bmac_bmac <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 4 here)
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    
    # Overall dimensionality (p == 8 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2 
    
    # Make sure the 'target' values are correctly specified
    if (p != 8 | lt != 2) {
      nimStop("Dimensionality 'p' must be 8 in 'sampler_CPa_xptf_bmac_bmac' and only nodes 's1ag' and 's2ag' can be sampled.")
    } else {
      if (target[1] != 's1ag' | target[2] != 's2ag') {
        nimStop("Target nodes must be the vectors (length g each) of intercepts 's1ag' and 's2ag'.")
      } 
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and trend coefficients (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    s1b1g <- model[['s1b1g']]
    s1b2g <- model[['s1b2g']]
    s1b3g <- model[['s1b3g']]
    s2b1g <- model[['s2b1g']]
    s2b2g <- model[['s2b2g']]
    s2b3g <- model[['s2b3g']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      for (k in 2:s1p) {
        bX[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    ambetag[1,1] <<- mbetag[1,1]
    ambetag[2,1] <<- mbetag[1+s1p,1]
    aDbetag <<- nimMatrix(0, 2L, 2L)
    aDbetag[1,1] <<- Dbetag[1,1]
    aDbetag[2,2] <<- Dbetag[1+s1p,1+s1p]
    
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    # Generate proposal for intercepts conditional on the betas
    for (i in 1:g) {                                                # cycle through each group independently
      bbeta[1,1] <<- s1b1g[i]
      bbeta[2,1] <<- s1b2g[i]
      bbeta[3,1] <<- s1b3g[i]
      bbeta[4,1] <<- s2b1g[i]
      bbeta[5,1] <<- s2b2g[i]
      bbeta[6,1] <<- s2b3g[i]
      bXbeta <<- bX %*% bbeta
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPa_xptf_bmac_bmac

# Custom Gibbs sampler for intercepts conditional on trend coefficients in the cubic-quadratic BMA model with a full trend break
sampler_CPa_xptf_bmac_bmaq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 4 here)
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    
    # Overall dimensionality (p == 7 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2 
    
    # Make sure the 'target' values are correctly specified
    if (p != 7 | lt != 2) {
      nimStop("Dimensionality 'p' must be 7 in 'sampler_CPa_xptf_bmac_bmaq' and only nodes 's1ag' and 's2ag' can be sampled.")
    } else {
      if (target[1] != 's1ag' | target[2] != 's2ag') {
        nimStop("Target nodes must be the vectors (length g each) of intercepts 's1ag' and 's2ag'.")
      } 
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and trend coefficients (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    s1b1g <- model[['s1b1g']]
    s1b2g <- model[['s1b2g']]
    s1b3g <- model[['s1b3g']]
    s2b1g <- model[['s2b1g']]
    s2b2g <- model[['s2b2g']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      for (k in 2:s1p) {
        bX[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    ambetag[1,1] <<- mbetag[1,1]
    ambetag[2,1] <<- mbetag[1+s1p,1]
    aDbetag <<- nimMatrix(0, 2L, 2L)
    aDbetag[1,1] <<- Dbetag[1,1]
    aDbetag[2,2] <<- Dbetag[1+s1p,1+s1p]
    
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    # Generate proposal for intercepts conditional on the betas
    for (i in 1:g) {                                                # cycle through each group independently
      bbeta[1,1] <<- s1b1g[i]
      bbeta[2,1] <<- s1b2g[i]
      bbeta[3,1] <<- s1b3g[i]
      bbeta[4,1] <<- s2b1g[i]
      bbeta[5,1] <<- s2b2g[i]
      bXbeta <<- bX %*% bbeta
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPa_xptf_bmac_bmaq

# Custom Gibbs sampler for intercepts conditional on trend coefficients in the cubic-linear BMA model with a full trend break
sampler_CPa_xptf_bmac_bmal <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 4 here)
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    
    # Overall dimensionality (p == 6 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2 
    
    # Make sure the 'target' values are correctly specified
    if (p != 6 | lt != 2) {
      nimStop("Dimensionality 'p' must be 6 in 'sampler_CPa_xptf_bmac_bmal' and only nodes 's1ag' and 's2ag' can be sampled.")
    } else {
      if (target[1] != 's1ag' | target[2] != 's2ag') {
        nimStop("Target nodes must be the vectors (length g each) of intercepts 's1ag' and 's2ag'.")
      } 
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and trend coefficients (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    s1b1g <- model[['s1b1g']]
    s1b2g <- model[['s1b2g']]
    s1b3g <- model[['s1b3g']]
    s2b1g <- model[['s2b1g']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      for (k in 2:s1p) {
        bX[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    ambetag[1,1] <<- mbetag[1,1]
    ambetag[2,1] <<- mbetag[1+s1p,1]
    aDbetag <<- nimMatrix(0, 2L, 2L)
    aDbetag[1,1] <<- Dbetag[1,1]
    aDbetag[2,2] <<- Dbetag[1+s1p,1+s1p]
    
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    # Generate proposal for intercepts conditional on the betas
    for (i in 1:g) {                                                # cycle through each group independently
      bbeta[1,1] <<- s1b1g[i]
      bbeta[2,1] <<- s1b2g[i]
      bbeta[3,1] <<- s1b3g[i]
      bbeta[4,1] <<- s2b1g[i]
      bXbeta <<- bX %*% bbeta
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPa_xptf_bmac_bmal

# Custom Gibbs sampler for intercepts conditional on trend coefficients in the quadratic BMA model
sampler_CPa_bmaq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 3 here)
    pm1 <- 0L
    p <- model$getConstants()$p
    pm1 <- p-1
    
    # Make sure the 'target' values are correctly specified
    if (p != 3 | lt != 1) {
      nimStop("Dimensionality 'p' must be 3 in 'sampler_CPa_bmaq' and only node 'ag' can be sampled.")
    } else {
      if (target[1] != 'ag') {
        nimStop("Target node must be the vector (length g) of intercepts 'ag'.")
      } 
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    bX <- nimMatrix(nrow=n, ncol=pm1, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)
    
    ambetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aDXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aprbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    azbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aCC <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aCI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    taCI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and trend coefficients (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    b1g <- model[['b1g']]
    b2g <- model[['b2g']]
    
    # Node values to be replaced
    ag <- model[['ag']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      for (k in 2:p) {
        bX[j,k-1] <<- X[j,k]
      }
    }
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]                                            # intercepts column
    }
    ambetag[1,1] <<- mbetag[1,1]                                    # prior mean for intercepts
    aDbetag[1,1] <<- Dbetag[1,1]                                    # prior precision for intercepts
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    # Generate proposal for intercepts conditional on the betas
    for (i in 1:g) {                                                # cycle through each group independently
      bbeta[1,1] <<- b1g[i]
      bbeta[2,1] <<- b2g[i]
      bXbeta <<- bX %*% bbeta
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      ag[i] <- abeta[1,1]
    }
    
    # Update model with new node values
    model[['ag']] <<- ag
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPa_bmaq 

# Custom Gibbs sampler for intercepts conditional on trend coefficients in the quadratic BMA model with a level shift
sampler_CPa_xptl_bmaq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 4 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2 
    
    # Make sure the 'target' values are correctly specified
    if (p != 4 | lt != 2) {
      nimStop("Dimensionality 'p' must be 4 in 'sampler_CPa_xptl_bmaq' and only nodes 's1ag' and 's2ag' can be sampled.")
    } else {
      if (target[1] != 's1ag' | target[2] != 's2ag') {
        nimStop("Target nodes must be the vectors (length g each) of intercepts 's1ag' and 's2ag'.")
      } 
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and trend coefficients (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    b1g <- model[['b1g']]
    b2g <- model[['b2g']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      for (k in 3:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]                                            # intercepts columns
      aX[j,2] <<- X[j,2]
    }
    aDbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 1:2) {
      ambetag[k,1] <<- mbetag[k,1]
      aDbetag[k,k] <<- Dbetag[k,k]
    }
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    # Generate proposal for intercepts conditional on the betas
    for (i in 1:g) {                                                # cycle through each group independently
      bbeta[1,1] <<- b1g[i]
      bbeta[2,1] <<- b2g[i]
      bXbeta <<- bX %*% bbeta
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPa_xptl_bmaq

# Custom Gibbs sampler for intercepts conditional on trend coefficients in the quadratic-quadratic BMA model with a full trend break
sampler_CPa_xptf_bmaq_bmaq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 3 here)
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    
    # Overall dimensionality (p == 6 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2 
    
    # Make sure the 'target' values are correctly specified
    if (p != 6 | lt != 2) {
      nimStop("Dimensionality 'p' must be 6 in 'sampler_CPa_xptf_bmaq_bmaq' and only nodes 's1ag' and 's2ag' can be sampled.")
    } else {
      if (target[1] != 's1ag' | target[2] != 's2ag') {
        nimStop("Target nodes must be the vectors (length g each) of intercepts 's1ag' and 's2ag'.")
      } 
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and trend coefficients (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    s1b1g <- model[['s1b1g']]
    s1b2g <- model[['s1b2g']]
    s2b1g <- model[['s2b1g']]
    s2b2g <- model[['s2b2g']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      for (k in 2:s1p) {
        bX[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    ambetag[1,1] <<- mbetag[1,1]
    ambetag[2,1] <<- mbetag[1+s1p,1]
    aDbetag <<- nimMatrix(0, 2L, 2L)
    aDbetag[1,1] <<- Dbetag[1,1]
    aDbetag[2,2] <<- Dbetag[1+s1p,1+s1p]
    
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    # Generate proposal for intercepts conditional on the betas
    for (i in 1:g) {                                                # cycle through each group independently
      bbeta[1,1] <<- s1b1g[i]
      bbeta[2,1] <<- s1b2g[i]
      bbeta[3,1] <<- s2b1g[i]
      bbeta[4,1] <<- s2b2g[i]
      bXbeta <<- bX %*% bbeta
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPa_xptf_bmaq_bmaq

# Custom Gibbs sampler for intercepts conditional on trend coefficients in the quadratic-linear BMA model with a full trend break
sampler_CPa_xptf_bmaq_bmal <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 3 here)
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    
    # Overall dimensionality (p == 5 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2 
    
    # Make sure the 'target' values are correctly specified
    if (p != 5 | lt != 2) {
      nimStop("Dimensionality 'p' must be 5 in 'sampler_CPa_xptf_bmaq_bmal' and only nodes 's1ag' and 's2ag' can be sampled.")
    } else {
      if (target[1] != 's1ag' | target[2] != 's2ag') {
        nimStop("Target nodes must be the vectors (length g each) of intercepts 's1ag' and 's2ag'.")
      } 
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and trend coefficients (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    s1b1g <- model[['s1b1g']]
    s1b2g <- model[['s1b2g']]
    s2b1g <- model[['s2b1g']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      for (k in 2:s1p) {
        bX[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    ambetag[1,1] <<- mbetag[1,1]
    ambetag[2,1] <<- mbetag[1+s1p,1]
    aDbetag <<- nimMatrix(0, 2L, 2L)
    aDbetag[1,1] <<- Dbetag[1,1]
    aDbetag[2,2] <<- Dbetag[1+s1p,1+s1p]
    
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    # Generate proposal for intercepts conditional on the betas
    for (i in 1:g) {                                                # cycle through each group independently
      bbeta[1,1] <<- s1b1g[i]
      bbeta[2,1] <<- s1b2g[i]
      bbeta[3,1] <<- s2b1g[i]
      bXbeta <<- bX %*% bbeta
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPa_xptf_bmaq_bmal

# Custom Gibbs sampler for intercepts conditional on trend coefficients in the linear BMA model
sampler_CPa_bmal <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 2 here)
    pm1 <- 0L
    p <- model$getConstants()$p
    pm1 <- p-1
    
    # Make sure the 'target' values are correctly specified
    if (p != 2 | lt != 1) {
      nimStop("Dimensionality 'p' must be 2 in 'sampler_CPa_bmal' and only node 'ag' can be sampled.")
    } else {
      if (target[1] != 'ag') {
        nimStop("Target node must be the vector (length g) of intercepts 'ag'.")
      } 
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    bX <- nimMatrix(nrow=n, ncol=pm1, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bbeta <- nimMatrix(nrow=pm1, ncol=1L, init = FALSE)
    
    ambetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aDXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aprbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    azbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aCC <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    aCI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    taCI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and trend coefficients (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    b1g <- model[['b1g']]
    
    # Node values to be replaced
    ag <- model[['ag']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      for (k in 2:p) {
        bX[j,k-1] <<- X[j,k]
      }
    }
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]                                            # intercepts column
    }
    ambetag[1,1] <<- mbetag[1,1]                                    # prior mean for intercepts
    aDbetag[1,1] <<- Dbetag[1,1]                                    # prior precision for intercepts
    
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    # Generate proposal for intercepts conditional on the betas
    for (i in 1:g) {                                                # cycle through each group independently
      bbeta[1,1] <<- b1g[i]
      bXbeta <<- bX %*% bbeta
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      ag[i] <- abeta[1,1]
    }
    
    # Update model with new node values
    model[['ag']] <<- ag
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPa_bmal

# Custom Gibbs sampler for intercepts conditional on trend coefficients in the linear BMA model with a level shift
sampler_CPa_xptl_bmal <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 3 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2 
    
    # Make sure the 'target' values are correctly specified
    if (p != 3 | lt != 2) {
      nimStop("Dimensionality 'p' must be 3 in 'sampler_CPa_xptl_bmal' and only nodes 's1ag' and 's2ag' can be sampled.")
    } else {
      if (target[1] != 's1ag' | target[2] != 's2ag') {
        nimStop("Target nodes must be the vectors (length g each) of intercepts 's1ag' and 's2ag'.")
      } 
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and trend coefficients (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    b1g <- model[['b1g']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      for (k in 3:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]                                            # intercepts columns
      aX[j,2] <<- X[j,2]
    }
    aDbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 1:2) {
      ambetag[k,1] <<- mbetag[k,1]
      aDbetag[k,k] <<- Dbetag[k,k]
    }
    
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    # Generate proposal for intercepts conditional on the betas
    for (i in 1:g) {                                                # cycle through each group independently
      bbeta[1,1] <<- b1g[i]
      bXbeta <<- bX %*% bbeta
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPa_xptl_bmal

# Custom Gibbs sampler for intercepts conditional on trend coefficients in the linear-linear BMA model with a full trend break
sampler_CPa_xptf_bmal_bmal <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 2 here)
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    
    # Overall dimensionality (p == 4 here)
    pm2 <- 0L
    p <- model$getConstants()$p
    pm2 <- p-2 
    
    # Make sure the 'target' values are correctly specified
    if (p != 4 | lt != 2) {
      nimStop("Dimensionality 'p' must be 4 in 'sampler_CPa_xptf_bmal_bmal' and only nodes 's1ag' and 's2ag' can be sampled.")
    } else {
      if (target[1] != 's1ag' | target[2] != 's2ag') {
        nimStop("Target nodes must be the vectors (length g each) of intercepts 's1ag' and 's2ag'.")
      } 
    }
    
    # Get other constants from model to use in calculations
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    bX <- nimMatrix(nrow=n, ncol=pm2, init = FALSE)
    bXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    bbeta <- nimMatrix(nrow=pm2, ncol=1L, init = FALSE)
    
    ambetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aDbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    aXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aDXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aprbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    apbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    azbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    aCC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    aCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    taCI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and trend coefficients (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    s1b1g <- model[['s1b1g']]
    s2b1g <- model[['s2b1g']]
    
    # Node values to be replaced
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    irho <- 0
    inu <- 0
    
    # Populate conformal matrices
    for (j in 1:n) {
      for (k in 2:s1p) {
        bX[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        bX[j,k-2] <<- X[j,k]
      }
    }
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    ambetag[1,1] <<- mbetag[1,1]
    ambetag[2,1] <<- mbetag[1+s1p,1]
    aDbetag <<- nimMatrix(0, 2L, 2L)
    aDbetag[1,1] <<- Dbetag[1,1]
    aDbetag[2,2] <<- Dbetag[1+s1p,1+s1p]
    
    aXt <<- t(aX)                                                   # transpose aX
    aprbeta <<- aDbetag %*% ambetag                                 # contribution to posterior mean from prior
    
    # Generate proposal for intercepts conditional on the betas
    for (i in 1:g) {                                                # cycle through each group independently
      bbeta[1,1] <<- s1b1g[i]
      bbeta[2,1] <<- s2b1g[i]
      bXbeta <<- bX %*% bbeta
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - bXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - bX
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      aXtW <<- aXt %*% Wg                                           # multiply aXt and Wg
      aXtWX <<- aXtW %*% aX                                         # calculate aXtWX, the precision matrix from WLS
      aDXtWX <<- aDbetag + aXtWX                                    # posterior precision matrix is aDbetag + XtWX
      azbeta <<- aXtW %*% Zvec                                      # contribution to posterior mean from WLS
      apbeta <<- aprbeta + azbeta                                   # sum of prior and WLS contributions
      
      abeta[1,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 1 from univariate normal
      abeta[2,1] <<- rnorm(1, 0, sd = 1)                            # sample intercept for segment 2 from univariate normal
      aCC <<- chol(aDXtWX)                                          # Cholesky decomposition for precision matrix (R returns upper triangular)
      aCI <<- inverse(aCC)                                          # inverse of upper triangular matrix from Cholesky decomposition
      taCI <<- t(aCI)                                               # transpose
      apbeta <<- taCI %*% apbeta                                    # rescale
      abeta <<- apbeta + abeta                                      # center
      abeta <<- aCI %*% abeta                                       # rescale
      
      s1ag[i] <- abeta[1,1]
      s2ag[i] <- abeta[2,1]
    }
    
    # Update model with new node values
    model[['s1ag']] <<- s1ag
    model[['s2ag']] <<- s2ag
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPa_xptf_bmal_bmal

################################################################################################################
# eMKF: Custom Gibbs samplers for trend coefficients conditional on the intercepts in the supported BMA models #
################################################################################################################

# Custom Gibbs sampler for trend coefficients conditional on the intercepts in the cubic BMA model
sampler_CPb_bmac <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {

    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 4 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 4 | lt != p-1) {
      nimStop("All three nodes 'b1g', 'b2g', and 'b3g' must be sampled, conditionally on 'ag', using 'sampler_CPb_bmac'.")
    } else {
      if (target[1] != 'b1g') {
        nimStop("1st target node must be the vector (length g+1) of linear coefficients 'b1g'.")
      } else {
        if (target[2] != 'b2g') {
          nimStop("2nd target node must be the vector (length g+1) of quadratic coefficients 'b2g'.")
        } else {
          if (target[3] != 'b3g') {
            nimStop("3rd target node must be the vector (length g+1) of cubic coefficients 'b3g'.")
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    gp1 <- 0L
    g <- model$getConstants()$g
    gp1 <- g+1
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
   
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb3XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    sumb2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb3zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    sumb2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3mbetag <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3Dbetag <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3X <- nimMatrix(nrow=n, ncol=3L, init = FALSE)
    b2X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    
    b3Xt <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b2Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b3XtW <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b2XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b3XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3DXtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3prbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3pbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3beta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2beta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1beta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3CC <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2CC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1CC <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    tb3CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    tb2CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    tb1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and intercepts (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    ag <- model[['ag']]
    
    # Get current integer-valued model flag (draws are conditional on the model flag)
    flg <- 0L
    flg <- as.integer(model[['flg']])
    if (flg > 7 | flg < 1) {
      nimStop("Expecting integer model flag values between 1 and 7 in 'sampler_CPb_bmac'.")
    }
    
    # Node values to be replaced
    b1g <- model[['b1g']]
    b2g <- model[['b2g']]
    b3g <- model[['b3g']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    ###########################################################
    # Populate conformal matrices depending on the model flag #
    ###########################################################
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]                                            # intercepts column from design matrix
    }

    if (flg == 1 | flg == 4) {                                      # cubic
      for (j in 1:n) {
        for (k in 2:4) {
          b3X[j,k-1] <<- X[j,k]
        }
      }
      b3Dbetag <<- nimMatrix(0, 3L, 3L)
      for (k in 2:4) {
        b3mbetag[k-1,1] <<- mbetag[k,1]
        b3Dbetag[k-1,k-1] <<- Dbetag[k,k]  
      }
      b3Xt <<- t(b3X)                                               # transpose bX
      b3prbeta <<- b3Dbetag %*% b3mbetag                            # contribution to posterior mean from prior
      
      if (flg == 4) {                                               # initialize cumulative sums in common case
        sumb3XtWX <<- nimMatrix(0, 3L, 3L)
        sumb3zbeta <<- nimMatrix(0, 3L, 1L)
      }
    } else {
      if (flg == 2 | flg == 5) {                                    # quadratic
        for (j in 1:n) {
          for (k in 2:3) {
            b2X[j,k-1] <<- X[j,k]
          }
        }
        b2Dbetag <<- nimMatrix(0, 2L, 2L)
        for (k in 2:3) {
          b2mbetag[k-1,1] <<- mbetag[k,1]
          b2Dbetag[k-1,k-1] <<- Dbetag[k,k]  
        }
        b2Xt <<- t(b2X)                                             # transpose bX
        b2prbeta <<- b2Dbetag %*% b2mbetag                          # contribution to posterior mean from prior
        
        if (flg == 5) {                                             # initialize cumulative sums in common case
          sumb2XtWX <<- nimMatrix(0, 2L, 2L)
          sumb2zbeta <<- nimMatrix(0, 2L, 1L)
        }
      } else { 
        if (flg == 3 | flg == 6) {                                  # linear
          for (j in 1:n) {
            b1X[j,1] <<- X[j,2]
          }
          b1mbetag[1,1] <<- mbetag[2,1]
          b1Dbetag[1,1] <<- Dbetag[2,2]  
          b1Xt <<- t(b1X)                                           # transpose bX
          b1prbeta <<- b1Dbetag %*% b1mbetag                        # contribution to posterior mean from prior
          
          if (flg == 6) {                                           # initialize cumulative sums in common case
            sumb1XtWX[1,1] <<- 0
            sumb1zbeta[1,1] <<- 0
          }
        }
      }
    }

    if (flg == 1 | flg == 2 | flg == 3) {

      #####################################################
      # Generate proposal for group-specific trend models #
      #####################################################
      
      # Update coefficients conditional on intercepts
      for (i in 1:g) {                                              # cycle through each group independently
        abeta[1,1] <<- ag[i]                                        # group-specific abeta vector
        aXbeta <<- aX %*% abeta                                     # vector of intercepts a
        
        VgD <<- nimMatrix(0, n, n)
        for (j in 1:n) {
          Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]               # populate nx1 data vector Zvec = Yvec - a
          VgD[j,j] <<- S2arr[(i-1)*n+j]                             # (Re-)set VgD to diagonal matrix of sampling variances
        }
        irho <- rhoarr[i]
        inu <- nuarr[i]
        Vg <<- ar1_cov_matrix(rts, irho, inu)                       # add AR covariance matrix Vgamma
        Vg <<- VgD + Vg
        Wg <<- inverse(Vg)                                          # Wg = Vg^{-1}
        
        if (flg == 1) {                                             # group-specific cubic trend model
          b3XtW <<- b3Xt %*% Wg                                     # multiply Xt and Wg
          b3XtWX <<- b3XtW %*% b3X                                  # calculate XtWX, the precision matrix from WLS
          b3DXtWX <<- b3Dbetag + b3XtWX                             # posterior precision matrix for beta is Dbetag + XtWX
          b3zbeta <<- b3XtW %*% Zvec                                # contribution to posterior mean from WLS
          b3pbeta <<- b3prbeta + b3zbeta                            # sum of prior and WLS contributions
          for (k in 1:3) {
            b3beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal
          }
          b3CC <<- chol(b3DXtWX)                                    # Cholesky decomposition for precision matrix (R returns upper triangular)
          b3CI <<- inverse(b3CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb3CI <<- t(b3CI)                                         # transpose
          b3pbeta <<- tb3CI %*% b3pbeta                             # rescale
          b3beta <<- b3pbeta + b3beta                               # center
          b3beta <<- b3CI %*% b3beta                                # rescale
          
          b1g[i] <- b3beta[1,1]
          b2g[i] <- b3beta[2,1]
          b3g[i] <- b3beta[3,1]
          
        } else {
          if (flg == 2) {                                           # group-specific quadratic trend model
            b2XtW <<- b2Xt %*% Wg                                   # multiply Xt and Wg
            b2XtWX <<- b2XtW %*% b2X                                # calculate XtWX, the precision matrix from WLS
            b2DXtWX <<- b2Dbetag + b2XtWX                           # posterior precision matrix for beta is Dbetag + XtWX
            b2zbeta <<- b2XtW %*% Zvec                              # contribution to posterior mean from WLS
            b2pbeta <<- b2prbeta + b2zbeta                          # sum of prior and WLS contributions
            for (k in 1:2) {
              b2beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal
            }
            b2CC <<- chol(b2DXtWX)                                  # Cholesky decomposition for precision matrix (R returns upper triangular)
            b2CI <<- inverse(b2CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb2CI <<- t(b2CI)                                       # transpose
            b2pbeta <<- tb2CI %*% b2pbeta                           # rescale
            b2beta <<- b2pbeta + b2beta                             # center
            b2beta <<- b2CI %*% b2beta                              # rescale
            
            b1g[i] <- b2beta[1,1]
            b2g[i] <- b2beta[2,1]
            b3g[i] <- 0
            
          } else { # (flg == 3)                                     # group-specific linear trend model
            b1XtW <<- b1Xt %*% Wg                                   # multiply Xt and Wg
            b1XtWX <<- b1XtW %*% b1X                                # calculate XtWX, the precision matrix from WLS
            b1DXtWX <<- b1Dbetag + b1XtWX                           # posterior precision matrix for beta is Dbetag + XtWX
            b1zbeta <<- b1XtW %*% Zvec                              # contribution to posterior mean from WLS
            b1pbeta <<- b1prbeta + b1zbeta                          # sum of prior and WLS contributions
            b1beta[1,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal
            b1CC <<- chol(b1DXtWX)                                  # Cholesky decomposition for precision matrix (R returns upper triangular)
            b1CI <<- inverse(b1CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb1CI <<- t(b1CI)                                       # transpose
            b1pbeta <<- tb1CI %*% b1pbeta                           # rescale
            b1beta <<- b1pbeta + b1beta                             # center
            b1beta <<- b1CI %*% b1beta                              # rescale
            
            b1g[i] <- b1beta[1,1]
            b2g[i] <- 0
            b3g[i] <- 0
          }
        }
      }                                                             # end cycle through groups
      
      # Common coefficients are averages of group-specific ones
      b1g[gp1] <- 0
      b2g[gp1] <- 0
      b3g[gp1] <- 0
      for (i in 1:g) {
        b1g[gp1] <- b1g[gp1] + b1g[i]
        b2g[gp1] <- b2g[gp1] + b2g[i] 
        b3g[gp1] <- b3g[gp1] + b3g[i]
      }
      b1g[gp1] <- b1g[gp1]/g
      b2g[gp1] <- b2g[gp1]/g
      b3g[gp1] <- b3g[gp1]/g
      
    } else { 
      
      if (flg == 4 | flg == 5 | flg == 6) {
      
        #############################################
        # Generate proposal for common trend models #
        #############################################
  
        # Update common coefficient(s) conditional on intercepts
        for (i in 1:g) {                                            # cycle through each group independently
          abeta[1,1] <<- ag[i]                                      # group-specific abeta vector
          aXbeta <<- aX %*% abeta                                   # vector of intercepts a
          
          VgD <<- nimMatrix(0, n, n)
          for (j in 1:n) {
            Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]             # populate nx1 data vector Zvec = Yvec - a
            VgD[j,j] <<- S2arr[(i-1)*n+j]                           # (Re-)set VgD to diagonal matrix of sampling variances
          }
          irho <- rhoarr[i]
          inu <- nuarr[i]
          Vg <<- ar1_cov_matrix(rts, irho, inu)                     # add AR covariance matrix Vgamma
          Vg <<- VgD + Vg
          Wg <<- inverse(Vg)                                        # Wg = Vg^{-1}
          
          if (flg == 4) {                                           # common cubic trend model
            b3XtW <<- b3Xt %*% Wg                                   # multiply bXt and Wg
            b3XtWX <<- b3XtW %*% b3X                                # calculate bXtWX
            sumb3XtWX <<- sumb3XtWX + b3XtWX                        # cumulative matrix sum
            b3zbeta <<- b3XtW %*% Zvec                              # contributions to posterior mean from WLS
            sumb3zbeta <<- sumb3zbeta + b3zbeta                     # cumulative matrix sum
            
          } else {
            if (flg == 5) {                                         # common quadratic trend model
              b2XtW <<- b2Xt %*% Wg                                 # multiply bXt and Wg
              b2XtWX <<- b2XtW %*% b2X                              # calculate bXtWX
              sumb2XtWX <<- sumb2XtWX + b2XtWX                      # cumulative matrix sum
              b2zbeta <<- b2XtW %*% Zvec                            # contributions to posterior mean from WLS
              sumb2zbeta <<- sumb2zbeta + b2zbeta                   # cumulative matrix sum
              
            } else { # (flg == 6)                                   # common linear trend model
              b1XtW <<- b1Xt %*% Wg                                 # multiply bXt and Wg
              b1XtWX <<- b1XtW %*% b1X                              # calculate bXtWX
              sumb1XtWX <<- sumb1XtWX + b1XtWX                      # cumulative matrix sum
              b1zbeta <<- b1XtW %*% Zvec                            # contributions to posterior mean from WLS
              sumb1zbeta <<- sumb1zbeta + b1zbeta                   # cumulative matrix sum
            }
          }
        }                                                           # end cycle through groups
        
        if (flg == 4) {                                             # common cubic trend model
          b3DXtWX <<- b3Dbetag + sumb3XtWX                          # posterior precision matrix for common beta is bDbetag + sumbXtWX
          b3pbeta <<- b3prbeta + sumb3zbeta                         # sum of prior and the cumulative WLS contributions
          for (k in 1:3) {
            b3beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal(s)
          }
          b3CC <<- chol(b3DXtWX)                                    # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
          b3CI <<- inverse(b3CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb3CI <<- t(b3CI)                                         # transpose
          b3pbeta <<- tb3CI %*% b3pbeta                             # rescale
          b3beta <<- b3pbeta + b3beta                               # center
          b3beta <<- b3CI %*% b3beta                                # rescale
          
          b1g[gp1] <- b3beta[1,1]
          b2g[gp1] <- b3beta[2,1]
          b3g[gp1] <- b3beta[3,1]
          
        } else {
          if (flg == 5) {                                           # common quadratic trend model
            b2DXtWX <<- b2Dbetag + sumb2XtWX                        # posterior precision matrix for common beta is bDbetag + sumbXtWX
            b2pbeta <<- b2prbeta + sumb2zbeta                       # sum of prior and the cumulative WLS contributions
            for (k in 1:2) {
              b2beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal(s)
            }
            b2CC <<- chol(b2DXtWX)                                  # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
            b2CI <<- inverse(b2CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb2CI <<- t(b2CI)                                       # transpose
            b2pbeta <<- tb2CI %*% b2pbeta                           # rescale
            b2beta <<- b2pbeta + b2beta                             # center
            b2beta <<- b2CI %*% b2beta                              # rescale
            
            b1g[gp1] <- b2beta[1,1]
            b2g[gp1] <- b2beta[2,1]
            b3g[gp1] <- 0
            
          } else { # (flg == 6)                                     # common linear trend model
            b1DXtWX <<- b1Dbetag + sumb1XtWX                        # posterior precision matrix for common beta is bDbetag + sumbXtWX
            b1pbeta <<- b1prbeta + sumb1zbeta                       # sum of prior and the cumulative WLS contributions
            b1beta[1,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal(s)
            b1CC <<- chol(b1DXtWX)                                  # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
            b1CI <<- inverse(b1CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb1CI <<- t(b1CI)                                       # transpose
            b1pbeta <<- tb1CI %*% b1pbeta                           # rescale
            b1beta <<- b1pbeta + b1beta                             # center
            b1beta <<- b1CI %*% b1beta                              # rescale
            
            b1g[gp1] <- b1beta[1,1]
            b2g[gp1] <- 0
            b3g[gp1] <- 0
          }
        }
        
        # Group-specific coefficient(s) reflect common value(s)
        for (i in 1:g) {
          b1g[i] <- b1g[gp1]
          b2g[i] <- b2g[gp1]
          b3g[i] <- b3g[gp1]
        }
        
      } else { # (flg == 7)
        
        ###############################################
        # Generate proposal for intercepts-only model #
        ###############################################
        
        for (i in 1:gp1) {
          b1g[i] <- 0
          b2g[i] <- 0
          b3g[i] <- 0
        }
        
      }
    }
  
    # Update model with new node values    
    model[['b1g']] <<- b1g
    model[['b2g']] <<- b2g
    model[['b3g']] <<- b3g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})

) # sampler_CPb_bmac 

# Custom Gibbs sampler for trend coefficients conditional on the intercepts in the cubic BMA model with a level shift
sampler_CPb_xptl_bmac <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 5 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 5 | lt != p-2) {
      nimStop("All three nodes 'b1g', 'b2g', and 'b3g' must be sampled, conditionally on 's1ag' and 's2ag, using 'sampler_CPb_xptl_bmac'.")
    } else {
      if (target[1] != 'b1g') {
        nimStop("1st target node must be the vector (length g+1) of linear coefficients 'b1g'.")
      } else {
        if (target[2] != 'b2g') {
          nimStop("2nd target node must be the vector (length g+1) of quadratic coefficients 'b2g'.")
        } else {
          if (target[3] != 'b3g') {
            nimStop("3rd target node must be the vector (length g+1) of cubic coefficients 'b3g'.")
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    gp1 <- 0L
    g <- model$getConstants()$g
    gp1 <- g+1
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb3XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    sumb2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb3zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    sumb2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3mbetag <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3Dbetag <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3X <- nimMatrix(nrow=n, ncol=3L, init = FALSE)
    b2X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    
    b3Xt <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b2Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b3XtW <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b2XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b3XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3DXtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3prbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3pbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3beta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2beta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1beta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3CC <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2CC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1CC <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    tb3CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    tb2CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    tb1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and intercepts (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Get current integer-valued model flag (draws are conditional on the model flag)
    flg <- 0L
    flg <- as.integer(model[['flg']])
    if (flg > 7 | flg < 1) {
      nimStop("Expecting integer model flag values between 1 and 7 in 'sampler_CPb_xptl_bmac'.")
    }
    
    # Node values to be replaced
    b1g <- model[['b1g']]
    b2g <- model[['b2g']]
    b3g <- model[['b3g']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    ###########################################################
    # Populate conformal matrices depending on the model flag #
    ###########################################################
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]                                            # intercepts columns
      aX[j,2] <<- X[j,2]
    }

    if (flg == 1 | flg == 4) {                                      # cubic
      for (j in 1:n) {
        for (k in 3:5) {
          b3X[j,k-2] <<- X[j,k]
        }
      }
      b3Dbetag <<- nimMatrix(0, 3L, 3L)
      for (k in 3:5) {
        b3mbetag[k-2,1] <<- mbetag[k,1]
        b3Dbetag[k-2,k-2] <<- Dbetag[k,k]  
      }
      b3Xt <<- t(b3X)                                               # transpose bX
      b3prbeta <<- b3Dbetag %*% b3mbetag                            # contribution to posterior mean from prior
      
      if (flg == 4) {                                               # initialize cumulative sums in common case
        sumb3XtWX <<- nimMatrix(0, 3L, 3L)
        sumb3zbeta <<- nimMatrix(0, 3L, 1L)
      }
    } else {
      if (flg == 2 | flg == 5) {                                    # quadratic
        for (j in 1:n) {
          for (k in 3:4) {
            b2X[j,k-2] <<- X[j,k]
          }
        }
        b2Dbetag <<- nimMatrix(0, 2L, 2L)
        for (k in 3:4) {
          b2mbetag[k-2,1] <<- mbetag[k,1]
          b2Dbetag[k-2,k-2] <<- Dbetag[k,k]  
        }
        b2Xt <<- t(b2X)                                             # transpose bX
        b2prbeta <<- b2Dbetag %*% b2mbetag                          # contribution to posterior mean from prior
        
        if (flg == 5) {                                             # initialize cumulative sums in common case
          sumb2XtWX <<- nimMatrix(0, 2L, 2L)
          sumb2zbeta <<- nimMatrix(0, 2L, 1L)
        }
      } else { 
        if (flg == 3 | flg == 6) {                                  # linear
          for (j in 1:n) {
            b1X[j,1] <<- X[j,3]
          }
          b1mbetag[1,1] <<- mbetag[3,1]
          b1Dbetag[1,1] <<- Dbetag[3,3]  
          b1Xt <<- t(b1X)                                           # transpose bX
          b1prbeta <<- b1Dbetag %*% b1mbetag                        # contribution to posterior mean from prior
          
          if (flg == 6) {                                           # initialize cumulative sums in common case
            sumb1XtWX[1,1] <<- 0
            sumb1zbeta[1,1] <<- 0
          }
        }
      }
    }
    
    if (flg == 1 | flg == 2 | flg == 3) {
      
      #####################################################
      # Generate proposal for group-specific trend models #
      #####################################################
      
      # Update coefficients conditional on intercepts
      for (i in 1:g) {                                              # cycle through each group independently
        abeta[1,1] <<- s1ag[i]                                      # group-specific abeta vector, segment 1
        abeta[2,1] <<- s2ag[i]                                      # group-specific abeta vector, segment 2
        aXbeta <<- aX %*% abeta                                     # vector of intercepts a
        
        VgD <<- nimMatrix(0, n, n)
        for (j in 1:n) {
          Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]               # populate nx1 data vector Zvec = Yvec - a
          VgD[j,j] <<- S2arr[(i-1)*n+j]                             # (Re-)set VgD to diagonal matrix of sampling variances
        }
        irho <- rhoarr[i]
        inu <- nuarr[i]
        Vg <<- ar1_cov_matrix(rts, irho, inu)                       # add AR covariance matrix Vgamma
        Vg <<- VgD + Vg
        Wg <<- inverse(Vg)                                          # Wg = Vg^{-1}
        
        if (flg == 1) {                                             # group-specific cubic trend model
          b3XtW <<- b3Xt %*% Wg                                     # multiply Xt and Wg
          b3XtWX <<- b3XtW %*% b3X                                  # calculate XtWX, the precision matrix from WLS
          b3DXtWX <<- b3Dbetag + b3XtWX                             # posterior precision matrix for beta is Dbetag + XtWX
          b3zbeta <<- b3XtW %*% Zvec                                # contribution to posterior mean from WLS
          b3pbeta <<- b3prbeta + b3zbeta                            # sum of prior and WLS contributions
          for (k in 1:3) {
            b3beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal
          }
          b3CC <<- chol(b3DXtWX)                                    # Cholesky decomposition for precision matrix (R returns upper triangular)
          b3CI <<- inverse(b3CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb3CI <<- t(b3CI)                                         # transpose
          b3pbeta <<- tb3CI %*% b3pbeta                             # rescale
          b3beta <<- b3pbeta + b3beta                               # center
          b3beta <<- b3CI %*% b3beta                                # rescale
          
          b1g[i] <- b3beta[1,1]
          b2g[i] <- b3beta[2,1]
          b3g[i] <- b3beta[3,1]
          
        } else {
          if (flg == 2) {                                           # group-specific quadratic trend model
            b2XtW <<- b2Xt %*% Wg                                   # multiply Xt and Wg
            b2XtWX <<- b2XtW %*% b2X                                # calculate XtWX, the precision matrix from WLS
            b2DXtWX <<- b2Dbetag + b2XtWX                           # posterior precision matrix for beta is Dbetag + XtWX
            b2zbeta <<- b2XtW %*% Zvec                              # contribution to posterior mean from WLS
            b2pbeta <<- b2prbeta + b2zbeta                          # sum of prior and WLS contributions
            for (k in 1:2) {
              b2beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal
            }
            b2CC <<- chol(b2DXtWX)                                  # Cholesky decomposition for precision matrix (R returns upper triangular)
            b2CI <<- inverse(b2CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb2CI <<- t(b2CI)                                       # transpose
            b2pbeta <<- tb2CI %*% b2pbeta                           # rescale
            b2beta <<- b2pbeta + b2beta                             # center
            b2beta <<- b2CI %*% b2beta                              # rescale
            
            b1g[i] <- b2beta[1,1]
            b2g[i] <- b2beta[2,1]
            b3g[i] <- 0
            
          } else { # (flg == 3)                                     # group-specific linear trend model
            b1XtW <<- b1Xt %*% Wg                                   # multiply Xt and Wg
            b1XtWX <<- b1XtW %*% b1X                                # calculate XtWX, the precision matrix from WLS
            b1DXtWX <<- b1Dbetag + b1XtWX                           # posterior precision matrix for beta is Dbetag + XtWX
            b1zbeta <<- b1XtW %*% Zvec                              # contribution to posterior mean from WLS
            b1pbeta <<- b1prbeta + b1zbeta                          # sum of prior and WLS contributions
            b1beta[1,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal
            b1CC <<- chol(b1DXtWX)                                  # Cholesky decomposition for precision matrix (R returns upper triangular)
            b1CI <<- inverse(b1CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb1CI <<- t(b1CI)                                       # transpose
            b1pbeta <<- tb1CI %*% b1pbeta                           # rescale
            b1beta <<- b1pbeta + b1beta                             # center
            b1beta <<- b1CI %*% b1beta                              # rescale
            
            b1g[i] <- b1beta[1,1]
            b2g[i] <- 0
            b3g[i] <- 0
          }
        }
      }                                                             # end cycle through groups
      
      # Common coefficients are averages of group-specific ones
      b1g[gp1] <- 0
      b2g[gp1] <- 0
      b3g[gp1] <- 0
      for (i in 1:g) {
        b1g[gp1] <- b1g[gp1] + b1g[i]
        b2g[gp1] <- b2g[gp1] + b2g[i] 
        b3g[gp1] <- b3g[gp1] + b3g[i]
      }
      b1g[gp1] <- b1g[gp1]/g
      b2g[gp1] <- b2g[gp1]/g
      b3g[gp1] <- b3g[gp1]/g
      
    } else { 
      
      if (flg == 4 | flg == 5 | flg == 6) {
        
        #############################################
        # Generate proposal for common trend models #
        #############################################
        
        # Update common coefficient(s) conditional on intercepts
        for (i in 1:g) {                                            # cycle through each group independently
          abeta[1,1] <<- s1ag[i]                                    # group-specific abeta vector, segment 1
          abeta[2,1] <<- s2ag[i]                                    # group-specific abeta vector, segment 2
          aXbeta <<- aX %*% abeta                                   # vector of intercepts a
          
          VgD <<- nimMatrix(0, n, n)
          for (j in 1:n) {
            Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]             # populate nx1 data vector Zvec = Yvec - a
            VgD[j,j] <<- S2arr[(i-1)*n+j]                           # (Re-)set VgD to diagonal matrix of sampling variances
          }
          irho <- rhoarr[i]
          inu <- nuarr[i]
          Vg <<- ar1_cov_matrix(rts, irho, inu)                     # add AR covariance matrix Vgamma
          Vg <<- VgD + Vg
          Wg <<- inverse(Vg)                                        # Wg = Vg^{-1}
          
          if (flg == 4) {                                           # common cubic trend model
            b3XtW <<- b3Xt %*% Wg                                   # multiply bXt and Wg
            b3XtWX <<- b3XtW %*% b3X                                # calculate bXtWX
            sumb3XtWX <<- sumb3XtWX + b3XtWX                        # cumulative matrix sum
            b3zbeta <<- b3XtW %*% Zvec                              # contributions to posterior mean from WLS
            sumb3zbeta <<- sumb3zbeta + b3zbeta                     # cumulative matrix sum
            
          } else {
            if (flg == 5) {                                         # common quadratic trend model
              b2XtW <<- b2Xt %*% Wg                                 # multiply bXt and Wg
              b2XtWX <<- b2XtW %*% b2X                              # calculate bXtWX
              sumb2XtWX <<- sumb2XtWX + b2XtWX                      # cumulative matrix sum
              b2zbeta <<- b2XtW %*% Zvec                            # contributions to posterior mean from WLS
              sumb2zbeta <<- sumb2zbeta + b2zbeta                   # cumulative matrix sum
              
            } else { # (flg == 6)                                   # common linear trend model
              b1XtW <<- b1Xt %*% Wg                                 # multiply bXt and Wg
              b1XtWX <<- b1XtW %*% b1X                              # calculate bXtWX
              sumb1XtWX <<- sumb1XtWX + b1XtWX                      # cumulative matrix sum
              b1zbeta <<- b1XtW %*% Zvec                            # contributions to posterior mean from WLS
              sumb1zbeta <<- sumb1zbeta + b1zbeta                   # cumulative matrix sum
            }
          }
        }                                                           # end cycle through groups
        
        if (flg == 4) {                                             # common cubic trend model
          b3DXtWX <<- b3Dbetag + sumb3XtWX                          # posterior precision matrix for common beta is bDbetag + sumbXtWX
          b3pbeta <<- b3prbeta + sumb3zbeta                         # sum of prior and the cumulative WLS contributions
          for (k in 1:3) {
            b3beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal(s)
          }
          b3CC <<- chol(b3DXtWX)                                    # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
          b3CI <<- inverse(b3CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb3CI <<- t(b3CI)                                         # transpose
          b3pbeta <<- tb3CI %*% b3pbeta                             # rescale
          b3beta <<- b3pbeta + b3beta                               # center
          b3beta <<- b3CI %*% b3beta                                # rescale
          
          b1g[gp1] <- b3beta[1,1]
          b2g[gp1] <- b3beta[2,1]
          b3g[gp1] <- b3beta[3,1]
          
        } else {
          if (flg == 5) {                                           # common quadratic trend model
            b2DXtWX <<- b2Dbetag + sumb2XtWX                        # posterior precision matrix for common beta is bDbetag + sumbXtWX
            b2pbeta <<- b2prbeta + sumb2zbeta                       # sum of prior and the cumulative WLS contributions
            for (k in 1:2) {
              b2beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal(s)
            }
            b2CC <<- chol(b2DXtWX)                                  # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
            b2CI <<- inverse(b2CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb2CI <<- t(b2CI)                                       # transpose
            b2pbeta <<- tb2CI %*% b2pbeta                           # rescale
            b2beta <<- b2pbeta + b2beta                             # center
            b2beta <<- b2CI %*% b2beta                              # rescale
            
            b1g[gp1] <- b2beta[1,1]
            b2g[gp1] <- b2beta[2,1]
            b3g[gp1] <- 0
            
          } else { # (flg == 6)                                     # common linear trend model
            b1DXtWX <<- b1Dbetag + sumb1XtWX                        # posterior precision matrix for common beta is bDbetag + sumbXtWX
            b1pbeta <<- b1prbeta + sumb1zbeta                       # sum of prior and the cumulative WLS contributions
            b1beta[1,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal(s)
            b1CC <<- chol(b1DXtWX)                                  # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
            b1CI <<- inverse(b1CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb1CI <<- t(b1CI)                                       # transpose
            b1pbeta <<- tb1CI %*% b1pbeta                           # rescale
            b1beta <<- b1pbeta + b1beta                             # center
            b1beta <<- b1CI %*% b1beta                              # rescale
            
            b1g[gp1] <- b1beta[1,1]
            b2g[gp1] <- 0
            b3g[gp1] <- 0
          }
        }
        
        # Group-specific coefficient(s) reflect common value(s)
        for (i in 1:g) {
          b1g[i] <- b1g[gp1]
          b2g[i] <- b2g[gp1]
          b3g[i] <- b3g[gp1]
        }
        
      } else { # (iflg == 7)
        
        ###############################################
        # Generate proposal for intercepts-only model #
        ###############################################
        
        for (i in 1:gp1) {
          b1g[i] <- 0
          b2g[i] <- 0
          b3g[i] <- 0
        }
        
      }
    }
    
    # Update model with new node values    
    model[['b1g']] <<- b1g
    model[['b2g']] <<- b2g
    model[['b3g']] <<- b3g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPb_xptl_bmac

# Custom Gibbs sampler for trend coefficients conditional on the intercepts in the cubic-cubic BMA model with full trend break
sampler_CPb_xptf_bmac_bmac <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 4 here)
    s1pm2 <- 0L
    s1pm1 <- 0L
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    s1pm1 <- s1p - 1
    s1pm2 <- s1p - 2
    
    # Overall dimensionality (p == 8 here)
    pm2 <- 0L
    pm1 <- 0L
    p <- model$getConstants()$p
    pm1 <- p-1
    pm2 <- p-2
    
    # Make sure the 'target' values are correctly specified
    if (p != 8 | lt != p-2) {
      nimStop("All six nodes 's1b1g', 's1b2g', 's1b3g', 's2b1g', 's2b2g', and 's2b3g' must be sampled, conditionally on 's1ag' and 's2ag', using 'sampler_CPb_xptf_bmac_bmac'.")
    } else {
      if (target[1] != 's1b1g') {
        nimStop("1st target node must be the vector (length g+1) of linear coefficients 's1b1g'.")
      } else {
        if (target[2] != 's1b2g') {
          nimStop("2nd target node must be the vector (length g+1) of quadratic coefficients 's1b2g'.")
        } else {
          if (target[3] != 's1b3g') {
            nimStop("3rd target node must be the vector (length g+1) of cubic coefficients 's1b3g'.")
          } else {
            if (target[4] != 's2b1g') {
              nimStop("4th target node must be the vector (length g+1) of linear coefficients 's2b1g'.")
            } else {
              if (target[5] != 's2b2g') {
                nimStop("5th target node must be the vector (length g+1) of quadratic coefficients 's2b2g'.")
              } else {
                if (target[6] != 's2b3g') {
                  nimStop("6th target node must be the vector (length g+1) of cubic coefficients 's2b3g'.")
                }
              }
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    gp1 <- 0L
    g <- model$getConstants()$g
    gp1 <- g+1
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb6XtWX <- nimMatrix(nrow=6L, ncol=6L, init = FALSE)
    sumb5XtWX <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    sumb4XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    sumb3XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    sumb2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    sumb6zbeta <- nimMatrix(nrow=6L, ncol=1L, init = FALSE)
    sumb5zbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    sumb4zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    sumb3zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    sumb2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b6mbetag <- nimMatrix(nrow=6L, ncol=1L, init = FALSE)
    b5mbetag <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4mbetag <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3mbetag <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2mbetag <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b6Dbetag <- nimMatrix(nrow=6L, ncol=6L, init = FALSE)
    b5Dbetag <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4Dbetag <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3Dbetag <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2Dbetag <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b6X <- nimMatrix(nrow=n, ncol=6L, init = FALSE)
    b5X <- nimMatrix(nrow=n, ncol=5L, init = FALSE)
    b4X <- nimMatrix(nrow=n, ncol=4L, init = FALSE)
    b3X <- nimMatrix(nrow=n, ncol=4L, init = FALSE)
    b2X <- nimMatrix(nrow=n, ncol=3L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    
    b6Xt <- nimMatrix(nrow=6L, ncol=n, init = FALSE)
    b5Xt <- nimMatrix(nrow=5L, ncol=n, init = FALSE)
    b4Xt <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b3Xt <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2Xt <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b6XtW <- nimMatrix(nrow=6L, ncol=n, init = FALSE)
    b5XtW <- nimMatrix(nrow=5L, ncol=n, init = FALSE)
    b4XtW <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b3XtW <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2XtW <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b6XtWX <- nimMatrix(nrow=6L, ncol=6L, init = FALSE)
    b5XtWX <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b6DXtWX <- nimMatrix(nrow=6L, ncol=6L, init = FALSE)
    b5DXtWX <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4DXtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3DXtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2DXtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b6prbeta <- nimMatrix(nrow=6L, ncol=1L, init = FALSE)
    b5prbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4prbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3prbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2prbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b6pbeta <- nimMatrix(nrow=6L, ncol=1L, init = FALSE)
    b5pbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4pbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3pbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2pbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b6zbeta <- nimMatrix(nrow=6L, ncol=1L, init = FALSE)
    b5zbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b6beta <- nimMatrix(nrow=6L, ncol=1L, init = FALSE)
    b5beta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4beta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3beta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2beta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1beta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b6CC <- nimMatrix(nrow=6L, ncol=6L, init = FALSE)
    b5CC <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4CC <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3CC <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2CC <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1CC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b6CI <- nimMatrix(nrow=6L, ncol=6L, init = FALSE)
    b5CI <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    tb6CI <- nimMatrix(nrow=6L, ncol=6L, init = FALSE)
    tb5CI <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    tb4CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    tb3CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    tb2CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    tb1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and intercepts (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Get current integer-valued model flags (draws are conditional on the model flags)
    s1flg <- 0L
    s1flg <- as.integer(model[['s1flg']])
    if (s1flg > 7 | s1flg < 1) {
      nimStop("Expecting integer model flag values for segment 1 between 1 and 7 in 'sampler_CPb_xptf_bmac_bmac'.")
    }
    s2flg <- 0L
    s2flg <- as.integer(model[['s2flg']])
    if (s2flg > 7 | s2flg < 1) {
      nimStop("Expecting integer model flag values for segment 2 between 1 and 7 in 'sampler_CPb_xptf_bmac_bmac'.")
    }
    
    # Combination flag for internal use
    iflg <- 0L
    iflg <- 10*s1flg + s2flg
    
    # Node values to be replaced
    s1b1g <- model[['s1b1g']]
    s1b2g <- model[['s1b2g']]
    s1b3g <- model[['s1b3g']]
    s2b1g <- model[['s2b1g']]
    s2b2g <- model[['s2b2g']]
    s2b3g <- model[['s2b3g']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    ###########################################################
    # Populate conformal matrices depending on the model flag #
    ###########################################################
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    
    if (iflg == 11 | iflg == 44) {                                  # cubic-cubic
      for (j in 1:n) {
        for (k in 2:s1p) {
          b6X[j,k-1] <<- X[j,k]
        }
        for (k in s1pp2:p) {
          b6X[j,k-2] <<- X[j,k]
        }
      }
      b6Dbetag <<- nimMatrix(0, 6L, 6L)
      for (k in 2:s1p) {
        b6mbetag[k-1,1] <<- mbetag[k,1]
        b6Dbetag[k-1,k-1] <<- Dbetag[k,k]  
      }
      for (k in s1pp2:p) {
        b6mbetag[k-2,1] <<- mbetag[k,1]
        b6Dbetag[k-2,k-2] <<- Dbetag[k,k]  
      }
      b6Xt <<- t(b6X)                                               # transpose bX
      b6prbeta <<- b6Dbetag %*% b6mbetag                            # contribution to posterior mean from prior
      
      if (iflg == 44) {                                             # initialize cumulative sums in common case
        sumb6XtWX <<- nimMatrix(0, 6L, 6L)
        sumb6zbeta <<- nimMatrix(0, 6L, 1L)
      }
    } else {
      if (iflg == 12 | iflg == 45) {                                # cubic-quadratic
        for (j in 1:n) {
          for (k in 2:s1p) {
            b5X[j,k-1] <<- X[j,k]
          }
          for (k in s1pp2:pm1) {
            b5X[j,k-2] <<- X[j,k]
          }
        }
        b5Dbetag <<- nimMatrix(0, 5L, 5L)
        for (k in 2:s1p) {
          b5mbetag[k-1,1] <<- mbetag[k,1]
          b5Dbetag[k-1,k-1] <<- Dbetag[k,k]  
        }
        for (k in s1pp2:pm1) {
          b5mbetag[k-2,1] <<- mbetag[k,1]
          b5Dbetag[k-2,k-2] <<- Dbetag[k,k]  
        }
        b5Xt <<- t(b5X)                                             # transpose bX
        b5prbeta <<- b5Dbetag %*% b5mbetag                          # contribution to posterior mean from prior
        
        if (iflg == 45) {                                           # initialize cumulative sums in common case
          sumb5XtWX <<- nimMatrix(0, 5L, 5L)
          sumb5zbeta <<- nimMatrix(0, 5L, 1L)
        }
      } else {
        if (iflg == 13 | iflg == 46) {                              # cubic-linear
          for (j in 1:n) {
            for (k in 2:s1p) {
              b4X[j,k-1] <<- X[j,k]
            }
            for (k in s1pp2:pm2) {
              b4X[j,k-2] <<- X[j,k]
            }
          }
          b4Dbetag <<- nimMatrix(0, 4L, 4L)
          for (k in 2:s1p) {
            b4mbetag[k-1,1] <<- mbetag[k,1]
            b4Dbetag[k-1,k-1] <<- Dbetag[k,k]  
          }
          for (k in s1pp2:pm2) {
            b4mbetag[k-2,1] <<- mbetag[k,1]
            b4Dbetag[k-2,k-2] <<- Dbetag[k,k]  
          }
          b4Xt <<- t(b4X)                                           # transpose bX
          b4prbeta <<- b4Dbetag %*% b4mbetag                        # contribution to posterior mean from prior
          
          if (iflg == 46) {                                         # initialize cumulative sums in common case
            sumb4XtWX <<- nimMatrix(0, 4L, 4L)
            sumb4zbeta <<- nimMatrix(0, 4L, 1L)
          }
        } else {
          if (iflg == 22 | iflg == 55) {                            # quadratic-quadratic
            for (j in 1:n) {
              for (k in 2:s1pm1) {
                b3X[j,k-1] <<- X[j,k]
              }
              for (k in s1pp2:pm1) {
                b3X[j,k-3] <<- X[j,k]
              }
            }
            b3Dbetag <<- nimMatrix(0, 4L, 4L)
            for (k in 2:s1pm1) {
              b3mbetag[k-1,1] <<- mbetag[k,1]
              b3Dbetag[k-1,k-1] <<- Dbetag[k,k]  
            }
            for (k in s1pp2:pm1) {
              b3mbetag[k-3,1] <<- mbetag[k,1]
              b3Dbetag[k-3,k-3] <<- Dbetag[k,k]  
            }
            b3Xt <<- t(b3X)                                         # transpose bX
            b3prbeta <<- b3Dbetag %*% b3mbetag                      # contribution to posterior mean from prior
            
            if (iflg == 55) {                                       # initialize cumulative sums in common case
              sumb3XtWX <<- nimMatrix(0, 4L, 4L)
              sumb3zbeta <<- nimMatrix(0, 4L, 1L)
            }
          } else {
            if (iflg == 23 | iflg == 56) {                          # quadratic-linear
              for (j in 1:n) {
                for (k in 2:s1pm1) {
                  b2X[j,k-1] <<- X[j,k]
                }
                for (k in s1pp2:pm2) {
                  b2X[j,k-3] <<- X[j,k]
                }
              }
              b2Dbetag <<- nimMatrix(0, 3L, 3L)
              for (k in 2:s1pm1) {
                b2mbetag[k-1,1] <<- mbetag[k,1]
                b2Dbetag[k-1,k-1] <<- Dbetag[k,k]  
              }
              for (k in s1pp2:pm2) {
                b2mbetag[k-3,1] <<- mbetag[k,1]
                b2Dbetag[k-3,k-3] <<- Dbetag[k,k]  
              }
              b2Xt <<- t(b2X)                                       # transpose bX
              b2prbeta <<- b2Dbetag %*% b2mbetag                    # contribution to posterior mean from prior
              
              if (iflg == 56) {                                     # initialize cumulative sums in common case
                sumb2XtWX <<- nimMatrix(0, 3L, 3L)
                sumb2zbeta <<- nimMatrix(0, 3L, 1L)
              }
            } else {
              if (iflg == 33 | iflg == 66) {                        # linear-linear
                for (j in 1:n) {
                  for (k in 2:s1pm2) {
                    b1X[j,k-1] <<- X[j,k]
                  }
                  for (k in s1pp2:pm2) {
                    b1X[j,k-4] <<- X[j,k]
                  }
                }
                b1Dbetag <<- nimMatrix(0, 2L, 2L)
                for (k in 2:s1pm2) {
                  b1mbetag[k-1,1] <<- mbetag[k,1]
                  b1Dbetag[k-1,k-1] <<- Dbetag[k,k]  
                }
                for (k in s1pp2:pm2) {
                  b1mbetag[k-4,1] <<- mbetag[k,1]
                  b1Dbetag[k-4,k-4] <<- Dbetag[k,k]  
                }
                b1Xt <<- t(b1X)                                     # transpose bX
                b1prbeta <<- b1Dbetag %*% b1mbetag                  # contribution to posterior mean from prior
                
                if (iflg == 66) {                                   # initialize cumulative sums in common case
                  sumb1XtWX <<- nimMatrix(0, 2L, 2L)
                  sumb1zbeta <<- nimMatrix(0, 2L, 1L)
                }
              }
            }
          }
        }
      }
    }
    
    if (iflg == 11 | iflg == 12 | iflg == 13 | iflg == 22  | iflg == 23 | iflg == 33) {
      
      #####################################################
      # Generate proposal for group-specific trend models #
      #####################################################
      
      # Update coefficients conditional on intercepts
      for (i in 1:g) {                                              # cycle through each group independently
        abeta[1,1] <<- s1ag[i]                                      # group-specific abeta vector, segment 1
        abeta[2,1] <<- s2ag[i]                                      # group-specific abeta vector, segment 2
        aXbeta <<- aX %*% abeta                                     # vector of intercepts a
        
        VgD <<- nimMatrix(0, n, n)
        for (j in 1:n) {
          Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]               # populate nx1 data vector Zvec = Yvec - a
          VgD[j,j] <<- S2arr[(i-1)*n+j]                             # (Re-)set VgD to diagonal matrix of sampling variances
        }
        irho <- rhoarr[i]
        inu <- nuarr[i]
        Vg <<- ar1_cov_matrix(rts, irho, inu)                       # add AR covariance matrix Vgamma
        Vg <<- VgD + Vg
        Wg <<- inverse(Vg)                                          # Wg = Vg^{-1}
        
        if (iflg == 11) {                                           # group-specific cubic-cubic trend model
          b6XtW <<- b6Xt %*% Wg                                     # multiply Xt and Wg
          b6XtWX <<- b6XtW %*% b6X                                  # calculate XtWX, the precision matrix from WLS
          b6DXtWX <<- b6Dbetag + b6XtWX                             # posterior precision matrix for beta is Dbetag + XtWX
          b6zbeta <<- b6XtW %*% Zvec                                # contribution to posterior mean from WLS
          b6pbeta <<- b6prbeta + b6zbeta                            # sum of prior and WLS contributions
          for (k in 1:6) {
            b6beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal
          }
          b6CC <<- chol(b6DXtWX)                                    # Cholesky decomposition for precision matrix (R returns upper triangular)
          b6CI <<- inverse(b6CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb6CI <<- t(b6CI)                                         # transpose
          b6pbeta <<- tb6CI %*% b6pbeta                             # rescale
          b6beta <<- b6pbeta + b6beta                               # center
          b6beta <<- b6CI %*% b6beta                                # rescale
          
          s1b1g[i] <- b6beta[1,1]
          s1b2g[i] <- b6beta[2,1]
          s1b3g[i] <- b6beta[3,1]
          s2b1g[i] <- b6beta[4,1]
          s2b2g[i] <- b6beta[5,1]
          s2b3g[i] <- b6beta[6,1]
          
        } else {
          if (iflg == 12) {                                         # group-specific cubic-quadratic trend model
            b5XtW <<- b5Xt %*% Wg                                   # multiply Xt and Wg
            b5XtWX <<- b5XtW %*% b5X                                # calculate XtWX, the precision matrix from WLS
            b5DXtWX <<- b5Dbetag + b5XtWX                           # posterior precision matrix for beta is Dbetag + XtWX
            b5zbeta <<- b5XtW %*% Zvec                              # contribution to posterior mean from WLS
            b5pbeta <<- b5prbeta + b5zbeta                          # sum of prior and WLS contributions
            for (k in 1:5) {
              b5beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal
            }
            b5CC <<- chol(b5DXtWX)                                  # Cholesky decomposition for precision matrix (R returns upper triangular)
            b5CI <<- inverse(b5CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb5CI <<- t(b5CI)                                       # transpose
            b5pbeta <<- tb5CI %*% b5pbeta                           # rescale
            b5beta <<- b5pbeta + b5beta                             # center
            b5beta <<- b5CI %*% b5beta                              # rescale
            
            s1b1g[i] <- b5beta[1,1]
            s1b2g[i] <- b5beta[2,1]
            s1b3g[i] <- b5beta[3,1]
            s2b1g[i] <- b5beta[4,1]
            s2b2g[i] <- b5beta[5,1]
            s2b3g[i] <- 0
            
          } else {
            if (iflg == 13) {                                       # group-specific cubic-linear trend model
              b4XtW <<- b4Xt %*% Wg                                 # multiply Xt and Wg
              b4XtWX <<- b4XtW %*% b4X                              # calculate XtWX, the precision matrix from WLS
              b4DXtWX <<- b4Dbetag + b4XtWX                         # posterior precision matrix for beta is Dbetag + XtWX
              b4zbeta <<- b4XtW %*% Zvec                            # contribution to posterior mean from WLS
              b4pbeta <<- b4prbeta + b4zbeta                        # sum of prior and WLS contributions
              for (k in 1:4) {
                b4beta[k,1] <<- rnorm(1, 0, sd = 1)                 # sample from univariate standard normal
              }
              b4CC <<- chol(b4DXtWX)                                # Cholesky decomposition for precision matrix (R returns upper triangular)
              b4CI <<- inverse(b4CC)                                # inverse of upper triangular matrix from Cholesky decomposition
              tb4CI <<- t(b4CI)                                     # transpose
              b4pbeta <<- tb4CI %*% b4pbeta                         # rescale
              b4beta <<- b4pbeta + b4beta                           # center
              b4beta <<- b4CI %*% b4beta                            # rescale
              
              s1b1g[i] <- b4beta[1,1]
              s1b2g[i] <- b4beta[2,1]
              s1b3g[i] <- b4beta[3,1]
              s2b1g[i] <- b4beta[4,1]
              s2b2g[i] <- 0
              s2b3g[i] <- 0
              
            } else {
              if (iflg == 22) {                                     # group-specific quadratic-quadratic trend model
                b3XtW <<- b3Xt %*% Wg                               # multiply Xt and Wg
                b3XtWX <<- b3XtW %*% b3X                            # calculate XtWX, the precision matrix from WLS
                b3DXtWX <<- b3Dbetag + b3XtWX                       # posterior precision matrix for beta is Dbetag + XtWX
                b3zbeta <<- b3XtW %*% Zvec                          # contribution to posterior mean from WLS
                b3pbeta <<- b3prbeta + b3zbeta                      # sum of prior and WLS contributions
                for (k in 1:4) {
                  b3beta[k,1] <<- rnorm(1, 0, sd = 1)               # sample from univariate standard normal
                }
                b3CC <<- chol(b3DXtWX)                              # Cholesky decomposition for precision matrix (R returns upper triangular)
                b3CI <<- inverse(b3CC)                              # inverse of upper triangular matrix from Cholesky decomposition
                tb3CI <<- t(b3CI)                                   # transpose
                b3pbeta <<- tb3CI %*% b3pbeta                       # rescale
                b3beta <<- b3pbeta + b3beta                         # center
                b3beta <<- b3CI %*% b3beta                          # rescale
                
                s1b1g[i] <- b3beta[1,1]
                s1b2g[i] <- b3beta[2,1]
                s1b3g[i] <- 0
                s2b1g[i] <- b3beta[3,1]
                s2b2g[i] <- b3beta[4,1]
                s2b3g[i] <- 0
                
              } else {
                if (iflg == 23) {                                   # group-specific quadratic-linear trend model
                  b2XtW <<- b2Xt %*% Wg                             # multiply Xt and Wg
                  b2XtWX <<- b2XtW %*% b2X                          # calculate XtWX, the precision matrix from WLS
                  b2DXtWX <<- b2Dbetag + b2XtWX                     # posterior precision matrix for beta is Dbetag + XtWX
                  b2zbeta <<- b2XtW %*% Zvec                        # contribution to posterior mean from WLS
                  b2pbeta <<- b2prbeta + b2zbeta                    # sum of prior and WLS contributions
                  for (k in 1:3) {
                    b2beta[k,1] <<- rnorm(1, 0, sd = 1)             # sample from univariate standard normal
                  }
                  b2CC <<- chol(b2DXtWX)                            # Cholesky decomposition for precision matrix (R returns upper triangular)
                  b2CI <<- inverse(b2CC)                            # inverse of upper triangular matrix from Cholesky decomposition
                  tb2CI <<- t(b2CI)                                 # transpose
                  b2pbeta <<- tb2CI %*% b2pbeta                     # rescale
                  b2beta <<- b2pbeta + b2beta                       # center
                  b2beta <<- b2CI %*% b2beta                        # rescale
                  
                  s1b1g[i] <- b2beta[1,1]
                  s1b2g[i] <- b2beta[2,1]
                  s1b3g[i] <- 0
                  s2b1g[i] <- b2beta[3,1]
                  s2b2g[i] <- 0
                  s2b3g[i] <- 0
                  
                } else { # (iflg == 33)                             # group-specific linear-linear trend model
                  b1XtW <<- b1Xt %*% Wg                             # multiply Xt and Wg
                  b1XtWX <<- b1XtW %*% b1X                          # calculate XtWX, the precision matrix from WLS
                  b1DXtWX <<- b1Dbetag + b1XtWX                     # posterior precision matrix for beta is Dbetag + XtWX
                  b1zbeta <<- b1XtW %*% Zvec                        # contribution to posterior mean from WLS
                  b1pbeta <<- b1prbeta + b1zbeta                    # sum of prior and WLS contributions
                  for (k in 1:2) {
                    b1beta[k,1] <<- rnorm(1, 0, sd = 1)             # sample from univariate standard normal
                  }
                  b1CC <<- chol(b1DXtWX)                            # Cholesky decomposition for precision matrix (R returns upper triangular)
                  b1CI <<- inverse(b1CC)                            # inverse of upper triangular matrix from Cholesky decomposition
                  tb1CI <<- t(b1CI)                                 # transpose
                  b1pbeta <<- tb1CI %*% b1pbeta                     # rescale
                  b1beta <<- b1pbeta + b1beta                       # center
                  b1beta <<- b1CI %*% b1beta                        # rescale
                  
                  s1b1g[i] <- b1beta[1,1]
                  s1b2g[i] <- 0
                  s1b3g[i] <- 0
                  s2b1g[i] <- b1beta[2,1]
                  s2b2g[i] <- 0
                  s2b3g[i] <- 0
                  
                }
              }
            }
          }
        }
      }                                                             # end cycle through groups
      
      # Common coefficients are averages of group-specific ones
      s1b1g[gp1] <- 0
      s1b2g[gp1] <- 0
      s1b3g[gp1] <- 0
      s2b1g[gp1] <- 0
      s2b2g[gp1] <- 0
      s2b3g[gp1] <- 0
      for (i in 1:g) {
        s1b1g[gp1] <- s1b1g[gp1] + s1b1g[i]
        s1b2g[gp1] <- s1b2g[gp1] + s1b2g[i] 
        s1b3g[gp1] <- s1b3g[gp1] + s1b3g[i]
        s2b1g[gp1] <- s2b1g[gp1] + s2b1g[i]
        s2b2g[gp1] <- s2b2g[gp1] + s2b2g[i] 
        s2b3g[gp1] <- s2b3g[gp1] + s2b3g[i]
      }
      s1b1g[gp1] <- s1b1g[gp1]/g
      s1b2g[gp1] <- s1b2g[gp1]/g
      s1b3g[gp1] <- s1b3g[gp1]/g
      s2b1g[gp1] <- s2b1g[gp1]/g
      s2b2g[gp1] <- s2b2g[gp1]/g
      s2b3g[gp1] <- s2b3g[gp1]/g
      
    } else { 
      
      if (iflg == 44 | iflg == 45 | iflg == 46 | iflg == 55 | iflg == 56 | iflg == 66) {
        
        #############################################
        # Generate proposal for common trend models #
        #############################################
        
        # Update common coefficient(s) conditional on intercepts
        for (i in 1:g) {                                            # cycle through each group independently
          abeta[1,1] <<- s1ag[i]                                    # group-specific abeta vector, segment 1
          abeta[2,1] <<- s2ag[i]                                    # group-specific abeta vector, segment 2
          aXbeta <<- aX %*% abeta                                   # vector of intercepts a
          
          VgD <<- nimMatrix(0, n, n)
          for (j in 1:n) {
            Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]             # populate nx1 data vector Zvec = Yvec - a
            VgD[j,j] <<- S2arr[(i-1)*n+j]                           # (Re-)set VgD to diagonal matrix of sampling variances
          }
          irho <- rhoarr[i]
          inu <- nuarr[i]
          Vg <<- ar1_cov_matrix(rts, irho, inu)                     # add AR covariance matrix Vgamma
          Vg <<- VgD + Vg
          Wg <<- inverse(Vg)                                        # Wg = Vg^{-1}
          
          if (iflg == 44) {                                         # common cubic-cubic trend model
            b6XtW <<- b6Xt %*% Wg                                   # multiply bXt and Wg
            b6XtWX <<- b6XtW %*% b6X                                # calculate bXtWX
            sumb6XtWX <<- sumb6XtWX + b6XtWX                        # cumulative matrix sum
            b6zbeta <<- b6XtW %*% Zvec                              # contributions to posterior mean from WLS
            sumb6zbeta <<- sumb6zbeta + b6zbeta                     # cumulative matrix sum
          } else {
            if (iflg == 45) {                                       # common cubic-quadratic trend model
              b5XtW <<- b5Xt %*% Wg                                 # multiply bXt and Wg
              b5XtWX <<- b5XtW %*% b5X                              # calculate bXtWX
              sumb5XtWX <<- sumb5XtWX + b5XtWX                      # cumulative matrix sum
              b5zbeta <<- b5XtW %*% Zvec                            # contributions to posterior mean from WLS
              sumb5zbeta <<- sumb5zbeta + b5zbeta                   # cumulative matrix sum
            } else {
              if (iflg == 46) {                                     # common cubic-linear trend model
                b4XtW <<- b4Xt %*% Wg                               # multiply bXt and Wg
                b4XtWX <<- b4XtW %*% b4X                            # calculate bXtWX
                sumb4XtWX <<- sumb4XtWX + b4XtWX                    # cumulative matrix sum
                b4zbeta <<- b4XtW %*% Zvec                          # contributions to posterior mean from WLS
                sumb4zbeta <<- sumb4zbeta + b4zbeta                 # cumulative matrix sum
              } else {
                if (iflg == 55) {                                   # common quadratic-quadratic trend model
                  b3XtW <<- b3Xt %*% Wg                             # multiply bXt and Wg
                  b3XtWX <<- b3XtW %*% b3X                          # calculate bXtWX
                  sumb3XtWX <<- sumb3XtWX + b3XtWX                  # cumulative matrix sum
                  b3zbeta <<- b3XtW %*% Zvec                        # contributions to posterior mean from WLS
                  sumb3zbeta <<- sumb3zbeta + b3zbeta               # cumulative matrix sum
                } else {
                  if (iflg == 56) {                                 # common quadratic-linear trend model
                    b2XtW <<- b2Xt %*% Wg                           # multiply bXt and Wg
                    b2XtWX <<- b2XtW %*% b2X                        # calculate bXtWX
                    sumb2XtWX <<- sumb2XtWX + b2XtWX                # cumulative matrix sum
                    b2zbeta <<- b2XtW %*% Zvec                      # contributions to posterior mean from WLS
                    sumb2zbeta <<- sumb2zbeta + b2zbeta             # cumulative matrix sum
                    
                  } else { # (iflg == 66)                           # common linear-linear trend model
                    b1XtW <<- b1Xt %*% Wg                           # multiply bXt and Wg
                    b1XtWX <<- b1XtW %*% b1X                        # calculate bXtWX
                    sumb1XtWX <<- sumb1XtWX + b1XtWX                # cumulative matrix sum
                    b1zbeta <<- b1XtW %*% Zvec                      # contributions to posterior mean from WLS
                    sumb1zbeta <<- sumb1zbeta + b1zbeta             # cumulative matrix sum
                  }
                }
              }
            }
          }
        }                                                           # end cycle through groups
        
        if (iflg == 44) {                                           # common cubic-cubic trend model
          b6DXtWX <<- b6Dbetag + sumb6XtWX                          # posterior precision matrix for common beta is bDbetag + sumbXtWX
          b6pbeta <<- b6prbeta + sumb6zbeta                         # sum of prior and the cumulative WLS contributions
          for (k in 1:6) {
            b6beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal(s)
          }
          b6CC <<- chol(b6DXtWX)                                    # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
          b6CI <<- inverse(b6CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb6CI <<- t(b6CI)                                         # transpose
          b6pbeta <<- tb6CI %*% b6pbeta                             # rescale
          b6beta <<- b6pbeta + b6beta                               # center
          b6beta <<- b6CI %*% b6beta                                # rescale
          
          s1b1g[gp1] <- b6beta[1,1]
          s1b2g[gp1] <- b6beta[2,1]
          s1b3g[gp1] <- b6beta[3,1]
          s2b1g[gp1] <- b6beta[4,1]
          s2b2g[gp1] <- b6beta[5,1]
          s2b3g[gp1] <- b6beta[6,1]
          
        } else {
          if (iflg == 45) {                                         # common cubic-quadratic trend model
            b5DXtWX <<- b5Dbetag + sumb5XtWX                        # posterior precision matrix for common beta is bDbetag + sumbXtWX
            b5pbeta <<- b5prbeta + sumb5zbeta                       # sum of prior and the cumulative WLS contributions
            for (k in 1:5) {
              b5beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal(s)
            }
            b5CC <<- chol(b5DXtWX)                                  # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
            b5CI <<- inverse(b5CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb5CI <<- t(b5CI)                                       # transpose
            b5pbeta <<- tb5CI %*% b5pbeta                           # rescale
            b5beta <<- b5pbeta + b5beta                             # center
            b5beta <<- b5CI %*% b5beta                              # rescale
            
            s1b1g[gp1] <- b5beta[1,1]
            s1b2g[gp1] <- b5beta[2,1]
            s1b3g[gp1] <- b5beta[3,1]
            s2b1g[gp1] <- b5beta[4,1]
            s2b2g[gp1] <- b5beta[5,1]
            s2b3g[gp1] <- 0
            
          } else {
            if (iflg == 46) {                                       # common cubic-linear trend model
              b4DXtWX <<- b4Dbetag + sumb4XtWX                      # posterior precision matrix for common beta is bDbetag + sumbXtWX
              b4pbeta <<- b4prbeta + sumb4zbeta                     # sum of prior and the cumulative WLS contributions
              for (k in 1:4) {
                b4beta[k,1] <<- rnorm(1, 0, sd = 1)                 # sample from univariate standard normal(s)
              }
              b4CC <<- chol(b4DXtWX)                                # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
              b4CI <<- inverse(b4CC)                                # inverse of upper triangular matrix from Cholesky decomposition
              tb4CI <<- t(b4CI)                                     # transpose
              b4pbeta <<- tb4CI %*% b4pbeta                         # rescale
              b4beta <<- b4pbeta + b4beta                           # center
              b4beta <<- b4CI %*% b4beta                            # rescale
              
              s1b1g[gp1] <- b4beta[1,1]
              s1b2g[gp1] <- b4beta[2,1]
              s1b3g[gp1] <- b4beta[3,1]
              s2b1g[gp1] <- b4beta[4,1]
              s2b2g[gp1] <- 0
              s2b3g[gp1] <- 0
              
            } else {
              if (iflg == 55) {                                     # common quadratic-quadratic trend model
                b3DXtWX <<- b3Dbetag + sumb3XtWX                    # posterior precision matrix for common beta is bDbetag + sumbXtWX
                b3pbeta <<- b3prbeta + sumb3zbeta                   # sum of prior and the cumulative WLS contributions
                for (k in 1:4) {
                  b3beta[k,1] <<- rnorm(1, 0, sd = 1)               # sample from univariate standard normal(s)
                }
                b3CC <<- chol(b3DXtWX)                              # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
                b3CI <<- inverse(b3CC)                              # inverse of upper triangular matrix from Cholesky decomposition
                tb3CI <<- t(b3CI)                                   # transpose
                b3pbeta <<- tb3CI %*% b3pbeta                       # rescale
                b3beta <<- b3pbeta + b3beta                         # center
                b3beta <<- b3CI %*% b3beta                          # rescale
                
                s1b1g[gp1] <- b3beta[1,1]
                s1b2g[gp1] <- b3beta[2,1]
                s1b3g[gp1] <- 0
                s2b1g[gp1] <- b3beta[3,1]
                s2b2g[gp1] <- b3beta[4,1]
                s2b3g[gp1] <- 0
                
              } else {
                if (iflg == 56) {                                   # common quadratic-linear trend model
                  b2DXtWX <<- b2Dbetag + sumb2XtWX                  # posterior precision matrix for common beta is bDbetag + sumbXtWX
                  b2pbeta <<- b2prbeta + sumb2zbeta                 # sum of prior and the cumulative WLS contributions
                  for (k in 1:3) {
                    b2beta[k,1] <<- rnorm(1, 0, sd = 1)             # sample from univariate standard normal(s)
                  }
                  b2CC <<- chol(b2DXtWX)                            # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
                  b2CI <<- inverse(b2CC)                            # inverse of upper triangular matrix from Cholesky decomposition
                  tb2CI <<- t(b2CI)                                 # transpose
                  b2pbeta <<- tb2CI %*% b2pbeta                     # rescale
                  b2beta <<- b2pbeta + b2beta                       # center
                  b2beta <<- b2CI %*% b2beta                        # rescale
                  
                  s1b1g[gp1] <- b2beta[1,1]
                  s1b2g[gp1] <- b2beta[2,1]
                  s1b3g[gp1] <- 0
                  s2b1g[gp1] <- b2beta[3,1]
                  s2b2g[gp1] <- 0
                  s2b3g[gp1] <- 0
                  
                  } else { # (iflg == 66)                           # common linear-linear trend model
                    b1DXtWX <<- b1Dbetag + sumb1XtWX                # posterior precision matrix for common beta is bDbetag + sumbXtWX
                    b1pbeta <<- b1prbeta + sumb1zbeta               # sum of prior and the cumulative WLS contributions
                    for (k in 1:2) {
                      b1beta[k,1] <<- rnorm(1, 0, sd = 1)           # sample from univariate standard normal(s)
                    }
                    b1CC <<- chol(b1DXtWX)                          # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
                    b1CI <<- inverse(b1CC)                          # inverse of upper triangular matrix from Cholesky decomposition
                    tb1CI <<- t(b1CI)                               # transpose
                    b1pbeta <<- tb1CI %*% b1pbeta                   # rescale
                    b1beta <<- b1pbeta + b1beta                     # center
                    b1beta <<- b1CI %*% b1beta                      # rescale
                    
                    s1b1g[gp1] <- b1beta[1,1]
                    s1b2g[gp1] <- 0
                    s1b3g[gp1] <- 0
                    s2b1g[gp1] <- b1beta[2,1]
                    s2b2g[gp1] <- 0
                    s2b3g[gp1] <- 0
                  } 
                }
              }
            }
          }
        
          # Group-specific coefficient(s) reflect common value(s)
          for (i in 1:g) {
            s1b1g[i] <- s1b1g[gp1]
            s1b2g[i] <- s1b2g[gp1]
            s1b3g[i] <- s1b3g[gp1]
            s2b1g[i] <- s2b1g[gp1]
            s2b2g[i] <- s2b2g[gp1]
            s2b3g[i] <- s2b3g[gp1]
          }
        
      } else { # (iflg == 77)
        
        ###############################################
        # Generate proposal for intercepts-only model #
        ###############################################
        
        for (i in 1:gp1) {
          s1b1g[i] <- 0
          s1b2g[i] <- 0
          s1b3g[i] <- 0
          s2b1g[i] <- 0
          s2b2g[i] <- 0
          s2b3g[i] <- 0
        }
        
      }
    }
    
    # Update model with new node values    
    model[['s1b1g']] <<- s1b1g
    model[['s1b2g']] <<- s1b2g
    model[['s1b3g']] <<- s1b3g
    model[['s2b1g']] <<- s2b1g
    model[['s2b2g']] <<- s2b2g
    model[['s2b3g']] <<- s2b3g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPb_xptf_bmac_bmac

# Custom Gibbs sampler for trend coefficients conditional on the intercepts in the cubic-quadratic BMA model with full trend break
sampler_CPb_xptf_bmac_bmaq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 4 here)
    s1pm2 <- 0L
    s1pm1 <- 0L
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    s1pm1 <- s1p - 1
    s1pm2 <- s1p - 2
    
    # Overall dimensionality (p == 7 here)
    pm1 <- 0L
    p <- model$getConstants()$p
    pm1 <- p-1
    
    # Make sure the 'target' values are correctly specified
    if (p != 7 | lt != p-2) {
      nimStop("All five nodes 's1b1g', 's1b2g', 's1b3g', 's2b1g', and 's2b2g' must be sampled, conditionally on 's1ag' and 's2ag', using 'sampler_CPb_xptf_bmac_bmaq'.")
    } else {
      if (target[1] != 's1b1g') {
        nimStop("1st target node must be the vector (length g+1) of linear coefficients 's1b1g'.")
      } else {
        if (target[2] != 's1b2g') {
          nimStop("2nd target node must be the vector (length g+1) of quadratic coefficients 's1b2g'.")
        } else {
          if (target[3] != 's1b3g') {
            nimStop("3rd target node must be the vector (length g+1) of cubic coefficients 's1b3g'.")
          } else {
            if (target[4] != 's2b1g') {
              nimStop("4th target node must be the vector (length g+1) of linear coefficients 's2b1g'.")
            } else {
              if (target[5] != 's2b2g') {
                nimStop("5th target node must be the vector (length g+1) of quadratic coefficients 's2b2g'.")
              }
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    gp1 <- 0L
    g <- model$getConstants()$g
    gp1 <- g+1
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb5XtWX <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    sumb4XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    sumb3XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    sumb2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    sumb5zbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    sumb4zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    sumb3zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    sumb2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b5mbetag <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4mbetag <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3mbetag <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2mbetag <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b5Dbetag <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4Dbetag <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3Dbetag <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2Dbetag <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b5X <- nimMatrix(nrow=n, ncol=5L, init = FALSE)
    b4X <- nimMatrix(nrow=n, ncol=4L, init = FALSE)
    b3X <- nimMatrix(nrow=n, ncol=4L, init = FALSE)
    b2X <- nimMatrix(nrow=n, ncol=3L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    
    b5Xt <- nimMatrix(nrow=5L, ncol=n, init = FALSE)
    b4Xt <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b3Xt <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2Xt <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b5XtW <- nimMatrix(nrow=5L, ncol=n, init = FALSE)
    b4XtW <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b3XtW <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2XtW <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b5XtWX <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b5DXtWX <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4DXtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3DXtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2DXtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b5prbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4prbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3prbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2prbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b5pbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4pbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3pbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2pbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b5zbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b5beta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4beta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3beta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2beta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1beta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b5CC <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4CC <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3CC <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2CC <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1CC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b5CI <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    tb5CI <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    tb4CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    tb3CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    tb2CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    tb1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and intercepts (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Get current integer-valued model flags (draws are conditional on the model flags)
    s1flg <- 0L
    s1flg <- as.integer(model[['s1flg']])
    if (s1flg > 7 | s1flg < 1) {
      nimStop("Expecting integer model flag values for segment 1 between 1 and 7 in 'sampler_CPb_xptf_bmac_bmaq'.")
    }
    s2flg <- 0L
    s2flg <- as.integer(model[['s2flg']])
    if (s2flg > 5 | s2flg < 1) {
      nimStop("Expecting integer model flag values for segment 2 between 1 and 5 in 'sampler_CPb_xptf_bmac_bmaq'.")
    }
    
    # Combination flag for internal use
    iflg <- 0L
    iflg <- 10*s1flg + s2flg
    
    # Node values to be replaced
    s1b1g <- model[['s1b1g']]
    s1b2g <- model[['s1b2g']]
    s1b3g <- model[['s1b3g']]
    s2b1g <- model[['s2b1g']]
    s2b2g <- model[['s2b2g']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    ###########################################################
    # Populate conformal matrices depending on the model flag #
    ###########################################################
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    
    if (iflg == 11 | iflg == 43) {                                  # cubic-quadratic
      for (j in 1:n) {
        for (k in 2:s1p) {
          b5X[j,k-1] <<- X[j,k]
        }
        for (k in s1pp2:p) {
          b5X[j,k-2] <<- X[j,k]
        }
      }
      b5Dbetag <<- nimMatrix(0, 5L, 5L)
      for (k in 2:s1p) {
        b5mbetag[k-1,1] <<- mbetag[k,1]
        b5Dbetag[k-1,k-1] <<- Dbetag[k,k]  
      }
      for (k in s1pp2:p) {
        b5mbetag[k-2,1] <<- mbetag[k,1]
        b5Dbetag[k-2,k-2] <<- Dbetag[k,k]  
      }
      b5Xt <<- t(b5X)                                               # transpose bX
      b5prbeta <<- b5Dbetag %*% b5mbetag                            # contribution to posterior mean from prior
      
      if (iflg == 43) {                                             # initialize cumulative sums in common case
        sumb5XtWX <<- nimMatrix(0, 5L, 5L)
        sumb5zbeta <<- nimMatrix(0, 5L, 1L)
      }
    } else {
      if (iflg == 12 | iflg == 44) {                                # cubic-linear
        for (j in 1:n) {
          for (k in 2:s1p) {
            b4X[j,k-1] <<- X[j,k]
          }
          for (k in s1pp2:pm1) {
            b4X[j,k-2] <<- X[j,k]
          }
        }
        b4Dbetag <<- nimMatrix(0, 4L, 4L)
        for (k in 2:s1p) {
          b4mbetag[k-1,1] <<- mbetag[k,1]
          b4Dbetag[k-1,k-1] <<- Dbetag[k,k]  
        }
        for (k in s1pp2:pm1) {
          b4mbetag[k-2,1] <<- mbetag[k,1]
          b4Dbetag[k-2,k-2] <<- Dbetag[k,k]  
        }
        b4Xt <<- t(b4X)                                             # transpose bX
        b4prbeta <<- b4Dbetag %*% b4mbetag                          # contribution to posterior mean from prior
        
        if (iflg == 44) {                                           # initialize cumulative sums in common case
          sumb4XtWX <<- nimMatrix(0, 4L, 4L)
          sumb4zbeta <<- nimMatrix(0, 4L, 1L)
        }
      } else {
        if (iflg == 21 | iflg == 53) {                              # quadratic-quadratic
          for (j in 1:n) {
            for (k in 2:s1pm1) {
              b3X[j,k-1] <<- X[j,k]
            }
            for (k in s1pp2:p) {
              b3X[j,k-3] <<- X[j,k]
            }
          }
          b3Dbetag <<- nimMatrix(0, 4L, 4L)
          for (k in 2:s1pm1) {
            b3mbetag[k-1,1] <<- mbetag[k,1]
            b3Dbetag[k-1,k-1] <<- Dbetag[k,k]  
          }
          for (k in s1pp2:p) {
            b3mbetag[k-3,1] <<- mbetag[k,1]
            b3Dbetag[k-3,k-3] <<- Dbetag[k,k]  
          }
          b3Xt <<- t(b3X)                                           # transpose bX
          b3prbeta <<- b3Dbetag %*% b3mbetag                        # contribution to posterior mean from prior
          
          if (iflg == 53) {                                         # initialize cumulative sums in common case
            sumb3XtWX <<- nimMatrix(0, 4L, 4L)
            sumb3zbeta <<- nimMatrix(0, 4L, 1L)
          }
        } else {
          if (iflg == 22 | iflg == 54) {                            # quadratic-linear
            for (j in 1:n) {
              for (k in 2:s1pm1) {
                b2X[j,k-1] <<- X[j,k]
              }
              for (k in s1pp2:pm1) {
                b2X[j,k-3] <<- X[j,k]
              }
            }
            b2Dbetag <<- nimMatrix(0, 3L, 3L)
            for (k in 2:s1pm1) {
              b2mbetag[k-1,1] <<- mbetag[k,1]
              b2Dbetag[k-1,k-1] <<- Dbetag[k,k]  
            }
            for (k in s1pp2:pm1) {
              b2mbetag[k-3,1] <<- mbetag[k,1]
              b2Dbetag[k-3,k-3] <<- Dbetag[k,k]  
            }
            b2Xt <<- t(b2X)                                         # transpose bX
            b2prbeta <<- b2Dbetag %*% b2mbetag                      # contribution to posterior mean from prior
            
            if (iflg == 54) {                                       # initialize cumulative sums in common case
              sumb2XtWX <<- nimMatrix(0, 3L, 3L)
              sumb2zbeta <<- nimMatrix(0, 3L, 1L)
            }
          } else {
            if (iflg == 32 | iflg == 64) {                          # linear-linear
              for (j in 1:n) {
                for (k in 2:s1pm2) {
                  b1X[j,k-1] <<- X[j,k]
                }
                for (k in s1pp2:pm1) {
                  b1X[j,k-4] <<- X[j,k]
                }
              }
              b1Dbetag <<- nimMatrix(0, 2L, 2L)
              for (k in 2:s1pm2) {
                b1mbetag[k-1,1] <<- mbetag[k,1]
                b1Dbetag[k-1,k-1] <<- Dbetag[k,k]  
              }
              for (k in s1pp2:pm1) {
                b1mbetag[k-4,1] <<- mbetag[k,1]
                b1Dbetag[k-4,k-4] <<- Dbetag[k,k]  
              }
              b1Xt <<- t(b1X)                                       # transpose bX
              b1prbeta <<- b1Dbetag %*% b1mbetag                    # contribution to posterior mean from prior
              
              if (iflg == 64) {                                     # initialize cumulative sums in common case
                sumb1XtWX <<- nimMatrix(0, 2L, 2L)
                sumb1zbeta <<- nimMatrix(0, 2L, 1L)
              }
            }
          }
        }
      }
    }
    
    if (iflg == 11 | iflg == 12 | iflg == 21 | iflg == 22  | iflg == 32) {
      
      #####################################################
      # Generate proposal for group-specific trend models #
      #####################################################
      
      # Update coefficients conditional on intercepts
      for (i in 1:g) {                                              # cycle through each group independently
        abeta[1,1] <<- s1ag[i]                                      # group-specific abeta vector, segment 1
        abeta[2,1] <<- s2ag[i]                                      # group-specific abeta vector, segment 2
        aXbeta <<- aX %*% abeta                                     # vector of intercepts a
        
        VgD <<- nimMatrix(0, n, n)
        for (j in 1:n) {
          Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]               # populate nx1 data vector Zvec = Yvec - a
          VgD[j,j] <<- S2arr[(i-1)*n+j]                             # (Re-)set VgD to diagonal matrix of sampling variances
        }
        irho <- rhoarr[i]
        inu <- nuarr[i]
        Vg <<- ar1_cov_matrix(rts, irho, inu)                       # add AR covariance matrix Vgamma
        Vg <<- VgD + Vg
        Wg <<- inverse(Vg)                                          # Wg = Vg^{-1}
        
        if (iflg == 11) {                                           # group-specific cubic-quadratic trend model
          b5XtW <<- b5Xt %*% Wg                                     # multiply Xt and Wg
          b5XtWX <<- b5XtW %*% b5X                                  # calculate XtWX, the precision matrix from WLS
          b5DXtWX <<- b5Dbetag + b5XtWX                             # posterior precision matrix for beta is Dbetag + XtWX
          b5zbeta <<- b5XtW %*% Zvec                                # contribution to posterior mean from WLS
          b5pbeta <<- b5prbeta + b5zbeta                            # sum of prior and WLS contributions
          for (k in 1:5) {
            b5beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal
          }
          b5CC <<- chol(b5DXtWX)                                    # Cholesky decomposition for precision matrix (R returns upper triangular)
          b5CI <<- inverse(b5CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb5CI <<- t(b5CI)                                         # transpose
          b5pbeta <<- tb5CI %*% b5pbeta                             # rescale
          b5beta <<- b5pbeta + b5beta                               # center
          b5beta <<- b5CI %*% b5beta                                # rescale
          
          s1b1g[i] <- b5beta[1,1]
          s1b2g[i] <- b5beta[2,1]
          s1b3g[i] <- b5beta[3,1]
          s2b1g[i] <- b5beta[4,1]
          s2b2g[i] <- b5beta[5,1]
          
        } else {
          if (iflg == 12) {                                         # group-specific cubic-linear trend model
            b4XtW <<- b4Xt %*% Wg                                   # multiply Xt and Wg
            b4XtWX <<- b4XtW %*% b4X                                # calculate XtWX, the precision matrix from WLS
            b4DXtWX <<- b4Dbetag + b4XtWX                           # posterior precision matrix for beta is Dbetag + XtWX
            b4zbeta <<- b4XtW %*% Zvec                              # contribution to posterior mean from WLS
            b4pbeta <<- b4prbeta + b4zbeta                          # sum of prior and WLS contributions
            for (k in 1:4) {
              b4beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal
            }
            b4CC <<- chol(b4DXtWX)                                  # Cholesky decomposition for precision matrix (R returns upper triangular)
            b4CI <<- inverse(b4CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb4CI <<- t(b4CI)                                       # transpose
            b4pbeta <<- tb4CI %*% b4pbeta                           # rescale
            b4beta <<- b4pbeta + b4beta                             # center
            b4beta <<- b4CI %*% b4beta                              # rescale
            
            s1b1g[i] <- b4beta[1,1]
            s1b2g[i] <- b4beta[2,1]
            s1b3g[i] <- b4beta[3,1]
            s2b1g[i] <- b4beta[4,1]
            s2b2g[i] <- 0
            
          } else {
            if (iflg == 21) {                                       # group-specific quadratic-quadratic trend model
              b3XtW <<- b3Xt %*% Wg                                 # multiply Xt and Wg
              b3XtWX <<- b3XtW %*% b3X                              # calculate XtWX, the precision matrix from WLS
              b3DXtWX <<- b3Dbetag + b3XtWX                         # posterior precision matrix for beta is Dbetag + XtWX
              b3zbeta <<- b3XtW %*% Zvec                            # contribution to posterior mean from WLS
              b3pbeta <<- b3prbeta + b3zbeta                        # sum of prior and WLS contributions
              for (k in 1:4) {
                b3beta[k,1] <<- rnorm(1, 0, sd = 1)                 # sample from univariate standard normal
              }
              b3CC <<- chol(b3DXtWX)                                # Cholesky decomposition for precision matrix (R returns upper triangular)
              b3CI <<- inverse(b3CC)                                # inverse of upper triangular matrix from Cholesky decomposition
              tb3CI <<- t(b3CI)                                     # transpose
              b3pbeta <<- tb3CI %*% b3pbeta                         # rescale
              b3beta <<- b3pbeta + b3beta                           # center
              b3beta <<- b3CI %*% b3beta                            # rescale
              
              s1b1g[i] <- b3beta[1,1]
              s1b2g[i] <- b3beta[2,1]
              s1b3g[i] <- 0
              s2b1g[i] <- b3beta[3,1]
              s2b2g[i] <- b3beta[4,1]
              
            } else {
              if (iflg == 22) {                                     # group-specific quadratic-linear trend model
                b2XtW <<- b2Xt %*% Wg                               # multiply Xt and Wg
                b2XtWX <<- b2XtW %*% b2X                            # calculate XtWX, the precision matrix from WLS
                b2DXtWX <<- b2Dbetag + b2XtWX                       # posterior precision matrix for beta is Dbetag + XtWX
                b2zbeta <<- b2XtW %*% Zvec                          # contribution to posterior mean from WLS
                b2pbeta <<- b2prbeta + b2zbeta                      # sum of prior and WLS contributions
                for (k in 1:3) {
                  b2beta[k,1] <<- rnorm(1, 0, sd = 1)               # sample from univariate standard normal
                }
                b2CC <<- chol(b2DXtWX)                              # Cholesky decomposition for precision matrix (R returns upper triangular)
                b2CI <<- inverse(b2CC)                              # inverse of upper triangular matrix from Cholesky decomposition
                tb2CI <<- t(b2CI)                                   # transpose
                b2pbeta <<- tb2CI %*% b2pbeta                       # rescale
                b2beta <<- b2pbeta + b2beta                         # center
                b2beta <<- b2CI %*% b2beta                          # rescale
                
                s1b1g[i] <- b2beta[1,1]
                s1b2g[i] <- b2beta[2,1]
                s1b3g[i] <- 0
                s2b1g[i] <- b2beta[3,1]
                s2b2g[i] <- 0
                
              } else { # (iflg == 32)                               # group-specific linear-linear trend model
                b1XtW <<- b1Xt %*% Wg                               # multiply Xt and Wg
                b1XtWX <<- b1XtW %*% b1X                            # calculate XtWX, the precision matrix from WLS
                b1DXtWX <<- b1Dbetag + b1XtWX                       # posterior precision matrix for beta is Dbetag + XtWX
                b1zbeta <<- b1XtW %*% Zvec                          # contribution to posterior mean from WLS
                b1pbeta <<- b1prbeta + b1zbeta                      # sum of prior and WLS contributions
                for (k in 1:2) {
                  b1beta[k,1] <<- rnorm(1, 0, sd = 1)               # sample from univariate standard normal
                }
                b1CC <<- chol(b1DXtWX)                              # Cholesky decomposition for precision matrix (R returns upper triangular)
                b1CI <<- inverse(b1CC)                              # inverse of upper triangular matrix from Cholesky decomposition
                tb1CI <<- t(b1CI)                                   # transpose
                b1pbeta <<- tb1CI %*% b1pbeta                       # rescale
                b1beta <<- b1pbeta + b1beta                         # center
                b1beta <<- b1CI %*% b1beta                          # rescale
                
                s1b1g[i] <- b1beta[1,1]
                s1b2g[i] <- 0
                s1b3g[i] <- 0
                s2b1g[i] <- b1beta[2,1]
                s2b2g[i] <- 0
                
              }
            }
          }
        }
      }                                                             # end cycle through groups
      
      # Common coefficients are averages of group-specific ones
      s1b1g[gp1] <- 0
      s1b2g[gp1] <- 0
      s1b3g[gp1] <- 0
      s2b1g[gp1] <- 0
      s2b2g[gp1] <- 0
      for (i in 1:g) {
        s1b1g[gp1] <- s1b1g[gp1] + s1b1g[i]
        s1b2g[gp1] <- s1b2g[gp1] + s1b2g[i] 
        s1b3g[gp1] <- s1b3g[gp1] + s1b3g[i]
        s2b1g[gp1] <- s2b1g[gp1] + s2b1g[i]
        s2b2g[gp1] <- s2b2g[gp1] + s2b2g[i] 
      }
      s1b1g[gp1] <- s1b1g[gp1]/g
      s1b2g[gp1] <- s1b2g[gp1]/g
      s1b3g[gp1] <- s1b3g[gp1]/g
      s2b1g[gp1] <- s2b1g[gp1]/g
      s2b2g[gp1] <- s2b2g[gp1]/g
      
    } else { 
      
      if (iflg == 43 | iflg == 44 | iflg == 53 | iflg == 54 | iflg == 64) {
        
        #############################################
        # Generate proposal for common trend models #
        #############################################
        
        # Update common coefficient(s) conditional on intercepts
        for (i in 1:g) {                                            # cycle through each group independently
          abeta[1,1] <<- s1ag[i]                                    # group-specific abeta vector, segment 1
          abeta[2,1] <<- s2ag[i]                                    # group-specific abeta vector, segment 2
          aXbeta <<- aX %*% abeta                                   # vector of intercepts a
          
          VgD <<- nimMatrix(0, n, n)
          for (j in 1:n) {
            Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]             # populate nx1 data vector Zvec = Yvec - a
            VgD[j,j] <<- S2arr[(i-1)*n+j]                           # (Re-)set VgD to diagonal matrix of sampling variances
          }
          irho <- rhoarr[i]
          inu <- nuarr[i]
          Vg <<- ar1_cov_matrix(rts, irho, inu)                     # add AR covariance matrix Vgamma
          Vg <<- VgD + Vg
          Wg <<- inverse(Vg)                                        # Wg = Vg^{-1}
          
          if (iflg == 43) {                                         # common cubic-quadratic trend model
            b5XtW <<- b5Xt %*% Wg                                   # multiply bXt and Wg
            b5XtWX <<- b5XtW %*% b5X                                # calculate bXtWX
            sumb5XtWX <<- sumb5XtWX + b5XtWX                        # cumulative matrix sum
            b5zbeta <<- b5XtW %*% Zvec                              # contributions to posterior mean from WLS
            sumb5zbeta <<- sumb5zbeta + b5zbeta                     # cumulative matrix sum
          } else {
            if (iflg == 44) {                                       # common cubic-linear trend model
              b4XtW <<- b4Xt %*% Wg                                 # multiply bXt and Wg
              b4XtWX <<- b4XtW %*% b4X                              # calculate bXtWX
              sumb4XtWX <<- sumb4XtWX + b4XtWX                      # cumulative matrix sum
              b4zbeta <<- b4XtW %*% Zvec                            # contributions to posterior mean from WLS
              sumb4zbeta <<- sumb4zbeta + b4zbeta                   # cumulative matrix sum
            } else {
              if (iflg == 53) {                                     # common quadratic-quadratic trend model
                b3XtW <<- b3Xt %*% Wg                               # multiply bXt and Wg
                b3XtWX <<- b3XtW %*% b3X                            # calculate bXtWX
                sumb3XtWX <<- sumb3XtWX + b3XtWX                    # cumulative matrix sum
                b3zbeta <<- b3XtW %*% Zvec                          # contributions to posterior mean from WLS
                sumb3zbeta <<- sumb3zbeta + b3zbeta                 # cumulative matrix sum
              } else {
                if (iflg == 54) {                                   # common quadratic-linear trend model
                  b2XtW <<- b2Xt %*% Wg                             # multiply bXt and Wg
                  b2XtWX <<- b2XtW %*% b2X                          # calculate bXtWX
                  sumb2XtWX <<- sumb2XtWX + b2XtWX                  # cumulative matrix sum
                  b2zbeta <<- b2XtW %*% Zvec                        # contributions to posterior mean from WLS
                  sumb2zbeta <<- sumb2zbeta + b2zbeta               # cumulative matrix sum
                  
                } else { # (iflg == 64)                             # common linear-linear trend model
                  b1XtW <<- b1Xt %*% Wg                             # multiply bXt and Wg
                  b1XtWX <<- b1XtW %*% b1X                          # calculate bXtWX
                  sumb1XtWX <<- sumb1XtWX + b1XtWX                  # cumulative matrix sum
                  b1zbeta <<- b1XtW %*% Zvec                        # contributions to posterior mean from WLS
                  sumb1zbeta <<- sumb1zbeta + b1zbeta               # cumulative matrix sum
                }
              }
            }
          }
        }                                                           # end cycle through groups
        
        if (iflg == 43) {                                           # common cubic-quadratic trend model
          b5DXtWX <<- b5Dbetag + sumb5XtWX                          # posterior precision matrix for common beta is bDbetag + sumbXtWX
          b5pbeta <<- b5prbeta + sumb5zbeta                         # sum of prior and the cumulative WLS contributions
          for (k in 1:5) {
            b5beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal(s)
          }
          b5CC <<- chol(b5DXtWX)                                    # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
          b5CI <<- inverse(b5CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb5CI <<- t(b5CI)                                         # transpose
          b5pbeta <<- tb5CI %*% b5pbeta                             # rescale
          b5beta <<- b5pbeta + b5beta                               # center
          b5beta <<- b5CI %*% b5beta                                # rescale
          
          s1b1g[gp1] <- b5beta[1,1]
          s1b2g[gp1] <- b5beta[2,1]
          s1b3g[gp1] <- b5beta[3,1]
          s2b1g[gp1] <- b5beta[4,1]
          s2b2g[gp1] <- b5beta[5,1]
          
        } else {
          if (iflg == 44) {                                         # common cubic-linear trend model
            b4DXtWX <<- b4Dbetag + sumb4XtWX                        # posterior precision matrix for common beta is bDbetag + sumbXtWX
            b4pbeta <<- b4prbeta + sumb4zbeta                       # sum of prior and the cumulative WLS contributions
            for (k in 1:4) {
              b4beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal(s)
            }
            b4CC <<- chol(b4DXtWX)                                  # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
            b4CI <<- inverse(b4CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb4CI <<- t(b4CI)                                       # transpose
            b4pbeta <<- tb4CI %*% b4pbeta                           # rescale
            b4beta <<- b4pbeta + b4beta                             # center
            b4beta <<- b4CI %*% b4beta                              # rescale
            
            s1b1g[gp1] <- b4beta[1,1]
            s1b2g[gp1] <- b4beta[2,1]
            s1b3g[gp1] <- b4beta[3,1]
            s2b1g[gp1] <- b4beta[4,1]
            s2b2g[gp1] <- 0
            
          } else {
            if (iflg == 53) {                                       # common quadratic-quadratic trend model
              b3DXtWX <<- b3Dbetag + sumb3XtWX                      # posterior precision matrix for common beta is bDbetag + sumbXtWX
              b3pbeta <<- b3prbeta + sumb3zbeta                     # sum of prior and the cumulative WLS contributions
              for (k in 1:4) {
                b3beta[k,1] <<- rnorm(1, 0, sd = 1)                 # sample from univariate standard normal(s)
              }
              b3CC <<- chol(b3DXtWX)                                # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
              b3CI <<- inverse(b3CC)                                # inverse of upper triangular matrix from Cholesky decomposition
              tb3CI <<- t(b3CI)                                     # transpose
              b3pbeta <<- tb3CI %*% b3pbeta                         # rescale
              b3beta <<- b3pbeta + b3beta                           # center
              b3beta <<- b3CI %*% b3beta                            # rescale
              
              s1b1g[gp1] <- b3beta[1,1]
              s1b2g[gp1] <- b3beta[2,1]
              s1b3g[gp1] <- 0
              s2b1g[gp1] <- b3beta[3,1]
              s2b2g[gp1] <- b3beta[4,1]
              
            } else {
              if (iflg == 54) {                                     # common quadratic-linear trend model
                b2DXtWX <<- b2Dbetag + sumb2XtWX                    # posterior precision matrix for common beta is bDbetag + sumbXtWX
                b2pbeta <<- b2prbeta + sumb2zbeta                   # sum of prior and the cumulative WLS contributions
                for (k in 1:3) {
                  b2beta[k,1] <<- rnorm(1, 0, sd = 1)               # sample from univariate standard normal(s)
                }
                b2CC <<- chol(b2DXtWX)                              # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
                b2CI <<- inverse(b2CC)                              # inverse of upper triangular matrix from Cholesky decomposition
                tb2CI <<- t(b2CI)                                   # transpose
                b2pbeta <<- tb2CI %*% b2pbeta                       # rescale
                b2beta <<- b2pbeta + b2beta                         # center
                b2beta <<- b2CI %*% b2beta                          # rescale
                
                s1b1g[gp1] <- b2beta[1,1]
                s1b2g[gp1] <- b2beta[2,1]
                s1b3g[gp1] <- 0
                s2b1g[gp1] <- b2beta[3,1]
                s2b2g[gp1] <- 0
                
              } else { # (iflg == 64)                               # common linear-linear trend model
                b1DXtWX <<- b1Dbetag + sumb1XtWX                    # posterior precision matrix for common beta is bDbetag + sumbXtWX
                b1pbeta <<- b1prbeta + sumb1zbeta                   # sum of prior and the cumulative WLS contributions
                for (k in 1:2) {
                  b1beta[k,1] <<- rnorm(1, 0, sd = 1)               # sample from univariate standard normal(s)
                }
                b1CC <<- chol(b1DXtWX)                              # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
                b1CI <<- inverse(b1CC)                              # inverse of upper triangular matrix from Cholesky decomposition
                tb1CI <<- t(b1CI)                                   # transpose
                b1pbeta <<- tb1CI %*% b1pbeta                       # rescale
                b1beta <<- b1pbeta + b1beta                         # center
                b1beta <<- b1CI %*% b1beta                          # rescale
                
                s1b1g[gp1] <- b1beta[1,1]
                s1b2g[gp1] <- 0
                s1b3g[gp1] <- 0
                s2b1g[gp1] <- b1beta[2,1]
                s2b2g[gp1] <- 0
                
              } 
            }
          }
        }
        
        # Group-specific coefficient(s) reflect common value(s)
        for (i in 1:g) {
          s1b1g[i] <- s1b1g[gp1]
          s1b2g[i] <- s1b2g[gp1]
          s1b3g[i] <- s1b3g[gp1]
          s2b1g[i] <- s2b1g[gp1]
          s2b2g[i] <- s2b2g[gp1]
        }
        
      } else { # (iflg == 75)
        
        ###############################################
        # Generate proposal for intercepts-only model #
        ###############################################
        
        for (i in 1:gp1) {
          s1b1g[i] <- 0
          s1b2g[i] <- 0
          s1b3g[i] <- 0
          s2b1g[i] <- 0
          s2b2g[i] <- 0
        }
        
      }
    }
    
    # Update model with new node values    
    model[['s1b1g']] <<- s1b1g
    model[['s1b2g']] <<- s1b2g
    model[['s1b3g']] <<- s1b3g
    model[['s2b1g']] <<- s2b1g
    model[['s2b2g']] <<- s2b2g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPb_xptf_bmac_bmaq

# Custom Gibbs sampler for trend coefficients conditional on the intercepts in the cubic-linear BMA model with full trend break
sampler_CPb_xptf_bmac_bmal <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 4 here)
    s1pm2 <- 0L
    s1pm1 <- 0L
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    s1pm1 <- s1p - 1
    s1pm2 <- s1p - 2
    
    # Overall dimensionality (p == 6 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 6 | lt != p-2) {
      nimStop("All four nodes 's1b1g', 's1b2g', 's1b3g', and 's2b1g' must be sampled, conditionally on 's1ag' and 's2ag', using 'sampler_CPb_xptf_bmac_bmal'.")
    } else {
      if (target[1] != 's1b1g') {
        nimStop("1st target node must be the vector (length g+1) of linear coefficients 's1b1g'.")
      } else {
        if (target[2] != 's1b2g') {
          nimStop("2nd target node must be the vector (length g+1) of quadratic coefficients 's1b2g'.")
        } else {
          if (target[3] != 's1b3g') {
            nimStop("3rd target node must be the vector (length g+1) of cubic coefficients 's1b3g'.")
          } else {
            if (target[4] != 's2b1g') {
              nimStop("4th target node must be the vector (length g+1) of linear coefficients 's2b1g'.")
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    gp1 <- 0L
    g <- model$getConstants()$g
    gp1 <- g+1
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb4XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    sumb2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    sumb4zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    sumb2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b4mbetag <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2mbetag <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b4Dbetag <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2Dbetag <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b4X <- nimMatrix(nrow=n, ncol=4L, init = FALSE)
    b2X <- nimMatrix(nrow=n, ncol=3L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    
    b4Xt <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2Xt <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b4XtW <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2XtW <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b4XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b4DXtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2DXtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b4prbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2prbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b4pbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2pbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b4zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b4beta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2beta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1beta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b4CC <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2CC <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1CC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b4CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    tb4CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    tb2CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    tb1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and intercepts (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Get current integer-valued model flags (draws are conditional on the model flags)
    s1flg <- 0L
    s1flg <- as.integer(model[['s1flg']])
    if (s1flg > 7 | s1flg < 1) {
      nimStop("Expecting integer model flag values for segment 1 between 1 and 7 in 'sampler_CPb_xptf_bmac_bmal'.")
    }
    s2flg <- 0L
    s2flg <- as.integer(model[['s2flg']])
    if (s2flg > 3 | s2flg < 1) {
      nimStop("Expecting integer model flag values for segment 2 between 1 and 3 in 'sampler_CPb_xptf_bmac_bmal'.")
    }
    
    # Combination flag for internal use
    iflg <- 0L
    iflg <- 10*s1flg + s2flg
    
    # Node values to be replaced
    s1b1g <- model[['s1b1g']]
    s1b2g <- model[['s1b2g']]
    s1b3g <- model[['s1b3g']]
    s2b1g <- model[['s2b1g']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    ###########################################################
    # Populate conformal matrices depending on the model flag #
    ###########################################################
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    
    if (iflg == 11 | iflg == 42) {                                  # cubic-linear
      for (j in 1:n) {
        for (k in 2:s1p) {
          b4X[j,k-1] <<- X[j,k]
        }
        for (k in s1pp2:p) {
          b4X[j,k-2] <<- X[j,k]
        }
      }
      b4Dbetag <<- nimMatrix(0, 4L, 4L)
      for (k in 2:s1p) {
        b4mbetag[k-1,1] <<- mbetag[k,1]
        b4Dbetag[k-1,k-1] <<- Dbetag[k,k]  
      }
      for (k in s1pp2:p) {
        b4mbetag[k-2,1] <<- mbetag[k,1]
        b4Dbetag[k-2,k-2] <<- Dbetag[k,k]  
      }
      b4Xt <<- t(b4X)                                               # transpose bX
      b4prbeta <<- b4Dbetag %*% b4mbetag                            # contribution to posterior mean from prior
      
      if (iflg == 42) {                                             # initialize cumulative sums in common case
        sumb4XtWX <<- nimMatrix(0, 4L, 4L)
        sumb4zbeta <<- nimMatrix(0, 4L, 1L)
      }
    } else {
      if (iflg == 21 | iflg == 52) {                                # quadratic-linear
        for (j in 1:n) {
          for (k in 2:s1pm1) {
            b2X[j,k-1] <<- X[j,k]
          }
          for (k in s1pp2:p) {
            b2X[j,k-3] <<- X[j,k]
          }
        }
        b2Dbetag <<- nimMatrix(0, 3L, 3L)
        for (k in 2:s1pm1) {
          b2mbetag[k-1,1] <<- mbetag[k,1]
          b2Dbetag[k-1,k-1] <<- Dbetag[k,k]  
        }
        for (k in s1pp2:p) {
          b2mbetag[k-3,1] <<- mbetag[k,1]
          b2Dbetag[k-3,k-3] <<- Dbetag[k,k]  
        }
        b2Xt <<- t(b2X)                                             # transpose bX
        b2prbeta <<- b2Dbetag %*% b2mbetag                          # contribution to posterior mean from prior
        
        if (iflg == 52) {                                           # initialize cumulative sums in common case
          sumb2XtWX <<- nimMatrix(0, 3L, 3L)
          sumb2zbeta <<- nimMatrix(0, 3L, 1L)
        }
      } else {
        if (iflg == 31 | iflg == 62) {                              # linear-linear
          for (j in 1:n) {
            for (k in 2:s1pm2) {
              b1X[j,k-1] <<- X[j,k]
            }
            for (k in s1pp2:p) {
              b1X[j,k-4] <<- X[j,k]
            }
          }
          b1Dbetag <<- nimMatrix(0, 2L, 2L)
          for (k in 2:s1pm2) {
            b1mbetag[k-1,1] <<- mbetag[k,1]
            b1Dbetag[k-1,k-1] <<- Dbetag[k,k]  
          }
          for (k in s1pp2:p) {
            b1mbetag[k-4,1] <<- mbetag[k,1]
            b1Dbetag[k-4,k-4] <<- Dbetag[k,k]  
          }
          b1Xt <<- t(b1X)                                           # transpose bX
          b1prbeta <<- b1Dbetag %*% b1mbetag                        # contribution to posterior mean from prior
          
          if (iflg == 62) {                                         # initialize cumulative sums in common case
            sumb1XtWX <<- nimMatrix(0, 2L, 2L)
            sumb1zbeta <<- nimMatrix(0, 2L, 1L)
          }
        }
      }
    }
    
    if (iflg == 11 | iflg == 21 | iflg == 31) {
      
      #####################################################
      # Generate proposal for group-specific trend models #
      #####################################################
      
      # Update coefficients conditional on intercepts
      for (i in 1:g) {                                              # cycle through each group independently
        abeta[1,1] <<- s1ag[i]                                      # group-specific abeta vector, segment 1
        abeta[2,1] <<- s2ag[i]                                      # group-specific abeta vector, segment 2
        aXbeta <<- aX %*% abeta                                     # vector of intercepts a
        
        VgD <<- nimMatrix(0, n, n)
        for (j in 1:n) {
          Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]               # populate nx1 data vector Zvec = Yvec - a
          VgD[j,j] <<- S2arr[(i-1)*n+j]                             # (Re-)set VgD to diagonal matrix of sampling variances
        }
        irho <- rhoarr[i]
        inu <- nuarr[i]
        Vg <<- ar1_cov_matrix(rts, irho, inu)                       # add AR covariance matrix Vgamma
        Vg <<- VgD + Vg
        Wg <<- inverse(Vg)                                          # Wg = Vg^{-1}
        
        if (iflg == 11) {                                           # group-specific cubic-linear trend model
          b4XtW <<- b4Xt %*% Wg                                     # multiply Xt and Wg
          b4XtWX <<- b4XtW %*% b4X                                  # calculate XtWX, the precision matrix from WLS
          b4DXtWX <<- b4Dbetag + b4XtWX                             # posterior precision matrix for beta is Dbetag + XtWX
          b4zbeta <<- b4XtW %*% Zvec                                # contribution to posterior mean from WLS
          b4pbeta <<- b4prbeta + b4zbeta                            # sum of prior and WLS contributions
          for (k in 1:4) {
            b4beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal
          }
          b4CC <<- chol(b4DXtWX)                                    # Cholesky decomposition for precision matrix (R returns upper triangular)
          b4CI <<- inverse(b4CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb4CI <<- t(b4CI)                                         # transpose
          b4pbeta <<- tb4CI %*% b4pbeta                             # rescale
          b4beta <<- b4pbeta + b4beta                               # center
          b4beta <<- b4CI %*% b4beta                                # rescale
          
          s1b1g[i] <- b4beta[1,1]
          s1b2g[i] <- b4beta[2,1]
          s1b3g[i] <- b4beta[3,1]
          s2b1g[i] <- b4beta[4,1]
          
        } else {
          if (iflg == 21) {                                         # group-specific quadratic-linear trend model
            b2XtW <<- b2Xt %*% Wg                                   # multiply Xt and Wg
            b2XtWX <<- b2XtW %*% b2X                                # calculate XtWX, the precision matrix from WLS
            b2DXtWX <<- b2Dbetag + b2XtWX                           # posterior precision matrix for beta is Dbetag + XtWX
            b2zbeta <<- b2XtW %*% Zvec                              # contribution to posterior mean from WLS
            b2pbeta <<- b2prbeta + b2zbeta                          # sum of prior and WLS contributions
            for (k in 1:3) {
              b2beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal
            }
            b2CC <<- chol(b2DXtWX)                                  # Cholesky decomposition for precision matrix (R returns upper triangular)
            b2CI <<- inverse(b2CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb2CI <<- t(b2CI)                                       # transpose
            b2pbeta <<- tb2CI %*% b2pbeta                           # rescale
            b2beta <<- b2pbeta + b2beta                             # center
            b2beta <<- b2CI %*% b2beta                              # rescale
            
            s1b1g[i] <- b2beta[1,1]
            s1b2g[i] <- b2beta[2,1]
            s1b3g[i] <- 0
            s2b1g[i] <- b2beta[3,1]
            
          } else { # (iflg == 31)                                   # group-specific linear-linear trend model
            b1XtW <<- b1Xt %*% Wg                                   # multiply Xt and Wg
            b1XtWX <<- b1XtW %*% b1X                                # calculate XtWX, the precision matrix from WLS
            b1DXtWX <<- b1Dbetag + b1XtWX                           # posterior precision matrix for beta is Dbetag + XtWX
            b1zbeta <<- b1XtW %*% Zvec                              # contribution to posterior mean from WLS
            b1pbeta <<- b1prbeta + b1zbeta                          # sum of prior and WLS contributions
            for (k in 1:2) {
              b1beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal
            }
            b1CC <<- chol(b1DXtWX)                                  # Cholesky decomposition for precision matrix (R returns upper triangular)
            b1CI <<- inverse(b1CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb1CI <<- t(b1CI)                                       # transpose
            b1pbeta <<- tb1CI %*% b1pbeta                           # rescale
            b1beta <<- b1pbeta + b1beta                             # center
            b1beta <<- b1CI %*% b1beta                              # rescale
            
            s1b1g[i] <- b1beta[1,1]
            s1b2g[i] <- 0
            s1b3g[i] <- 0
            s2b1g[i] <- b1beta[2,1]
            
          }
        }
      }                                                             # end cycle through groups
      
      # Common coefficients are averages of group-specific ones
      s1b1g[gp1] <- 0
      s1b2g[gp1] <- 0
      s1b3g[gp1] <- 0
      s2b1g[gp1] <- 0
      for (i in 1:g) {
        s1b1g[gp1] <- s1b1g[gp1] + s1b1g[i]
        s1b2g[gp1] <- s1b2g[gp1] + s1b2g[i] 
        s1b3g[gp1] <- s1b3g[gp1] + s1b3g[i]
        s2b1g[gp1] <- s2b1g[gp1] + s2b1g[i]
      }
      s1b1g[gp1] <- s1b1g[gp1]/g
      s1b2g[gp1] <- s1b2g[gp1]/g
      s1b3g[gp1] <- s1b3g[gp1]/g
      s2b1g[gp1] <- s2b1g[gp1]/g
      
    } else { 
      
      if (iflg == 42 | iflg == 52 | iflg == 62) {
        
        #############################################
        # Generate proposal for common trend models #
        #############################################
        
        # Update common coefficient(s) conditional on intercepts
        for (i in 1:g) {                                            # cycle through each group independently
          abeta[1,1] <<- s1ag[i]                                    # group-specific abeta vector, segment 1
          abeta[2,1] <<- s2ag[i]                                    # group-specific abeta vector, segment 2
          aXbeta <<- aX %*% abeta                                   # vector of intercepts a
          
          VgD <<- nimMatrix(0, n, n)
          for (j in 1:n) {
            Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]             # populate nx1 data vector Zvec = Yvec - a
            VgD[j,j] <<- S2arr[(i-1)*n+j]                           # (Re-)set VgD to diagonal matrix of sampling variances
          }
          irho <- rhoarr[i]
          inu <- nuarr[i]
          Vg <<- ar1_cov_matrix(rts, irho, inu)                     # add AR covariance matrix Vgamma
          Vg <<- VgD + Vg
          Wg <<- inverse(Vg)                                        # Wg = Vg^{-1}
          
          if (iflg == 42) {                                         # common cubic-linear trend model
            b4XtW <<- b4Xt %*% Wg                                   # multiply bXt and Wg
            b4XtWX <<- b4XtW %*% b4X                                # calculate bXtWX
            sumb4XtWX <<- sumb4XtWX + b4XtWX                        # cumulative matrix sum
            b4zbeta <<- b4XtW %*% Zvec                              # contributions to posterior mean from WLS
            sumb4zbeta <<- sumb4zbeta + b4zbeta                     # cumulative matrix sum
          } else {
            if (iflg == 52) {                                       # common quadratic-linear trend model
              b2XtW <<- b2Xt %*% Wg                                 # multiply bXt and Wg
              b2XtWX <<- b2XtW %*% b2X                              # calculate bXtWX
              sumb2XtWX <<- sumb2XtWX + b2XtWX                      # cumulative matrix sum
              b2zbeta <<- b2XtW %*% Zvec                            # contributions to posterior mean from WLS
              sumb2zbeta <<- sumb2zbeta + b2zbeta                   # cumulative matrix sum
              
            } else { # (iflg == 62)                                 # common linear-linear trend model
              b1XtW <<- b1Xt %*% Wg                                 # multiply bXt and Wg
              b1XtWX <<- b1XtW %*% b1X                              # calculate bXtWX
              sumb1XtWX <<- sumb1XtWX + b1XtWX                      # cumulative matrix sum
              b1zbeta <<- b1XtW %*% Zvec                            # contributions to posterior mean from WLS
              sumb1zbeta <<- sumb1zbeta + b1zbeta                   # cumulative matrix sum
            }
          }
        }                                                           # end cycle through groups
        
        if (iflg == 42) {                                           # common cubic-linear trend model
          b4DXtWX <<- b4Dbetag + sumb4XtWX                          # posterior precision matrix for common beta is bDbetag + sumbXtWX
          b4pbeta <<- b4prbeta + sumb4zbeta                         # sum of prior and the cumulative WLS contributions
          for (k in 1:4) {
            b4beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal(s)
          }
          b4CC <<- chol(b4DXtWX)                                    # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
          b4CI <<- inverse(b4CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb4CI <<- t(b4CI)                                         # transpose
          b4pbeta <<- tb4CI %*% b4pbeta                             # rescale
          b4beta <<- b4pbeta + b4beta                               # center
          b4beta <<- b4CI %*% b4beta                                # rescale
          
          s1b1g[gp1] <- b4beta[1,1]
          s1b2g[gp1] <- b4beta[2,1]
          s1b3g[gp1] <- b4beta[3,1]
          s2b1g[gp1] <- b4beta[4,1]
          
        } else {
          if (iflg == 52) {                                         # common quadratic-linear trend model
            b2DXtWX <<- b2Dbetag + sumb2XtWX                        # posterior precision matrix for common beta is bDbetag + sumbXtWX
            b2pbeta <<- b2prbeta + sumb2zbeta                       # sum of prior and the cumulative WLS contributions
            for (k in 1:3) {
              b2beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal(s)
            }
            b2CC <<- chol(b2DXtWX)                                  # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
            b2CI <<- inverse(b2CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb2CI <<- t(b2CI)                                       # transpose
            b2pbeta <<- tb2CI %*% b2pbeta                           # rescale
            b2beta <<- b2pbeta + b2beta                             # center
            b2beta <<- b2CI %*% b2beta                              # rescale
            
            s1b1g[gp1] <- b2beta[1,1]
            s1b2g[gp1] <- b2beta[2,1]
            s1b3g[gp1] <- 0
            s2b1g[gp1] <- b2beta[3,1]
            
          } else { # (iflg == 62)                                   # common linear-linear trend model
            b1DXtWX <<- b1Dbetag + sumb1XtWX                        # posterior precision matrix for common beta is bDbetag + sumbXtWX
            b1pbeta <<- b1prbeta + sumb1zbeta                       # sum of prior and the cumulative WLS contributions
            for (k in 1:2) {
              b1beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal(s)
            }
            b1CC <<- chol(b1DXtWX)                                  # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
            b1CI <<- inverse(b1CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb1CI <<- t(b1CI)                                       # transpose
            b1pbeta <<- tb1CI %*% b1pbeta                           # rescale
            b1beta <<- b1pbeta + b1beta                             # center
            b1beta <<- b1CI %*% b1beta                              # rescale
            
            s1b1g[gp1] <- b1beta[1,1]
            s1b2g[gp1] <- 0
            s1b3g[gp1] <- 0
            s2b1g[gp1] <- b1beta[2,1]
            
          } 
        }
        
        # Group-specific coefficient(s) reflect common value(s)
        for (i in 1:g) {
          s1b1g[i] <- s1b1g[gp1]
          s1b2g[i] <- s1b2g[gp1]
          s1b3g[i] <- s1b3g[gp1]
          s2b1g[i] <- s2b1g[gp1]
        }
        
      } else { # (iflg == 73)
        
        ###############################################
        # Generate proposal for intercepts-only model #
        ###############################################
        
        for (i in 1:gp1) {
          s1b1g[i] <- 0
          s1b2g[i] <- 0
          s1b3g[i] <- 0
          s2b1g[i] <- 0
        }
        
      }
    }
    
    # Update model with new node values    
    model[['s1b1g']] <<- s1b1g
    model[['s1b2g']] <<- s1b2g
    model[['s1b3g']] <<- s1b3g
    model[['s2b1g']] <<- s2b1g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPb_xptf_bmac_bmal

# Custom Gibbs sampler for trend coefficients conditional on the intercepts in the quadratic BMA model
sampler_CPb_bmaq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 3 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 3 | lt != p-1) {
      nimStop("Both nodes 'b1g' and 'b2g' must be sampled, conditionally on 'ag', using 'sampler_CPb_bmaq'.")
    } else {
      if (target[1] != 'b1g') {
        nimStop("1st target node must be the vector (length g+1) of linear coefficients 'b1g'.")
      } else {
        if (target[2] != 'b2g') {
          nimStop("2nd target node must be the vector (length g+1) of quadratic coefficients 'b2g'.")
        } 
      }
    }
    
    # Get other constants from model to use in calculations
    gp1 <- 0L
    g <- model$getConstants()$g
    gp1 <- g+1
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    
    b2Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b2XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2beta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1beta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2CC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1CC <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    tb2CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    tb1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and intercepts (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    ag <- model[['ag']]
    
    # Get current integer-valued model flag (draws are conditional on the model flag)
    flg <- 0L
    flg <- as.integer(model[['flg']])
    if (flg > 5 | flg < 1) {
      nimStop("Expecting integer model flag values between 1 and 5 in 'sampler_CPb_bmaq'.")
    }
    
    # Node values to be replaced
    b1g <- model[['b1g']]
    b2g <- model[['b2g']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    ###########################################################
    # Populate conformal matrices depending on the model flag #
    ###########################################################
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]                                            # intercepts column from design matrix
    }
    
    if (flg == 1 | flg == 3) {                                      # quadratic: q = 2
      for (j in 1:n) {
        for (k in 2:3) {
          b2X[j,k-1] <<- X[j,k]
        }
      }
      b2Dbetag <<- nimMatrix(0, 2L, 2L)
      for (k in 2:3) {
        b2mbetag[k-1,1] <<- mbetag[k,1]
        b2Dbetag[k-1,k-1] <<- Dbetag[k,k]  
      }
      b2Xt <<- t(b2X)                                               # transpose bX
      b2prbeta <<- b2Dbetag %*% b2mbetag                            # contribution to posterior mean from prior
      
      if (flg == 3) {                                               # initialize cumulative sums in common case
        sumb2XtWX <<- nimMatrix(0, 2L, 2L)
        sumb2zbeta <<- nimMatrix(0, 2L, 1L)
      }
    } else { 
      if (flg == 2 | flg == 4) {                                    # linear: q = 1
        for (j in 1:n) {
          b1X[j,1] <<- X[j,2]
        }
        b1mbetag[1,1] <<- mbetag[2,1]
        b1Dbetag[1,1] <<- Dbetag[2,2]  
        b1Xt <<- t(b1X)                                             # transpose bX
        b1prbeta <<- b1Dbetag %*% b1mbetag                          # contribution to posterior mean from prior
        
        if (flg == 4) {                                             # initialize cumulative sums in common case
          sumb1XtWX[1,1] <<- 0
          sumb1zbeta[1,1] <<- 0
        }
      }
    }
    
    if (flg == 1 | flg == 2) {
      
      #####################################################
      # Generate proposal for group-specific trend models #
      #####################################################
      
      # Update coefficients conditional on intercepts
      for (i in 1:g) {                                              # cycle through each group independently
        abeta[1,1] <<- ag[i]                                        # group-specific abeta vector
        aXbeta <<- aX %*% abeta                                     # vector of intercepts a
        
        VgD <<- nimMatrix(0, n, n)
        for (j in 1:n) {
          Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]               # populate nx1 data vector Zvec = Yvec - a
          VgD[j,j] <<- S2arr[(i-1)*n+j]                             # (Re-)set VgD to diagonal matrix of sampling variances
        }
        irho <- rhoarr[i]
        inu <- nuarr[i]
        Vg <<- ar1_cov_matrix(rts, irho, inu)                       # add AR covariance matrix Vgamma
        Vg <<- VgD + Vg
        Wg <<- inverse(Vg)                                          # Wg = Vg^{-1}
        
        if (flg == 1) {                                             # group-specific quadratic trend model
          b2XtW <<- b2Xt %*% Wg                                     # multiply Xt and Wg
          b2XtWX <<- b2XtW %*% b2X                                  # calculate XtWX, the precision matrix from WLS
          b2DXtWX <<- b2Dbetag + b2XtWX                             # posterior precision matrix for beta is Dbetag + XtWX
          b2zbeta <<- b2XtW %*% Zvec                                # contribution to posterior mean from WLS
          b2pbeta <<- b2prbeta + b2zbeta                            # sum of prior and WLS contributions
          for (k in 1:2) {
            b2beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal
          }
          b2CC <<- chol(b2DXtWX)                                    # Cholesky decomposition for precision matrix (R returns upper triangular)
          b2CI <<- inverse(b2CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb2CI <<- t(b2CI)                                         # transpose
          b2pbeta <<- tb2CI %*% b2pbeta                             # rescale
          b2beta <<- b2pbeta + b2beta                               # center
          b2beta <<- b2CI %*% b2beta                                # rescale
          
          b1g[i] <- b2beta[1,1]
          b2g[i] <- b2beta[2,1]
          
        } else { # (flg == 2)                                       # group-specific linear trend model
          b1XtW <<- b1Xt %*% Wg                                     # multiply Xt and Wg
          b1XtWX <<- b1XtW %*% b1X                                  # calculate XtWX, the precision matrix from WLS
          b1DXtWX <<- b1Dbetag + b1XtWX                             # posterior precision matrix for beta is Dbetag + XtWX
          b1zbeta <<- b1XtW %*% Zvec                                # contribution to posterior mean from WLS
          b1pbeta <<- b1prbeta + b1zbeta                            # sum of prior and WLS contributions
          b1beta[1,1] <<- rnorm(1, 0, sd = 1)                       # sample from univariate standard normal
          b1CC <<- chol(b1DXtWX)                                    # Cholesky decomposition for precision matrix (R returns upper triangular)
          b1CI <<- inverse(b1CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb1CI <<- t(b1CI)                                         # transpose
          b1pbeta <<- tb1CI %*% b1pbeta                             # rescale
          b1beta <<- b1pbeta + b1beta                               # center
          b1beta <<- b1CI %*% b1beta                                # rescale
          
          b1g[i] <- b1beta[1,1]
          b2g[i] <- 0
        }
      }                                                             # end cycle through groups
      
      # Common coefficients are averages of group-specific ones
      b1g[gp1] <- 0
      b2g[gp1] <- 0
      for (i in 1:g) {
        b1g[gp1] <- b1g[gp1] + b1g[i]
        b2g[gp1] <- b2g[gp1] + b2g[i] 
      }
      b1g[gp1] <- b1g[gp1]/g
      b2g[gp1] <- b2g[gp1]/g
      
    } else { 
      
      if (flg == 3 | flg == 4) {
        
        #############################################
        # Generate proposal for common trend models #
        #############################################
        
        # Update common coefficient(s) conditional on intercepts
        for (i in 1:g) {                                            # cycle through each group independently
          abeta[1,1] <<- ag[i]                                      # group-specific abeta vector
          aXbeta <<- aX %*% abeta                                   # vector of intercepts a
          
          VgD <<- nimMatrix(0, n, n)
          for (j in 1:n) {
            Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]             # populate nx1 data vector Zvec = Yvec - a
            VgD[j,j] <<- S2arr[(i-1)*n+j]                           # (Re-)set VgD to diagonal matrix of sampling variances
          }
          irho <- rhoarr[i]
          inu <- nuarr[i]
          Vg <<- ar1_cov_matrix(rts, irho, inu)                     # add AR covariance matrix Vgamma
          Vg <<- VgD + Vg
          Wg <<- inverse(Vg)                                        # Wg = Vg^{-1}
          
          if (flg == 3) {                                           # common quadratic trend model
            b2XtW <<- b2Xt %*% Wg                                   # multiply bXt and Wg
            b2XtWX <<- b2XtW %*% b2X                                # calculate bXtWX
            sumb2XtWX <<- sumb2XtWX + b2XtWX                        # cumulative matrix sum
            b2zbeta <<- b2XtW %*% Zvec                              # contributions to posterior mean from WLS
            sumb2zbeta <<- sumb2zbeta + b2zbeta                     # cumulative matrix sum
            
          } else { # (flg == 4)                                     # common linear trend model
            b1XtW <<- b1Xt %*% Wg                                   # multiply bXt and Wg
            b1XtWX <<- b1XtW %*% b1X                                # calculate bXtWX
            sumb1XtWX <<- sumb1XtWX + b1XtWX                        # cumulative matrix sum
            b1zbeta <<- b1XtW %*% Zvec                              # contributions to posterior mean from WLS
            sumb1zbeta <<- sumb1zbeta + b1zbeta                     # cumulative matrix sum
          }
        }                                                           # end cycle through groups
        
        if (flg == 3) {                                             # common quadratic trend model
          b2DXtWX <<- b2Dbetag + sumb2XtWX                          # posterior precision matrix for common beta is bDbetag + sumbXtWX
          b2pbeta <<- b2prbeta + sumb2zbeta                         # sum of prior and the cumulative WLS contributions
          for (k in 1:2) {
            b2beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal(s)
          }
          b2CC <<- chol(b2DXtWX)                                    # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
          b2CI <<- inverse(b2CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb2CI <<- t(b2CI)                                         # transpose
          b2pbeta <<- tb2CI %*% b2pbeta                             # rescale
          b2beta <<- b2pbeta + b2beta                               # center
          b2beta <<- b2CI %*% b2beta                                # rescale
          
          b1g[gp1] <- b2beta[1,1]
          b2g[gp1] <- b2beta[2,1]
          
        } else { # (flg == 4)                                       # common linear trend model
          b1DXtWX <<- b1Dbetag + sumb1XtWX                          # posterior precision matrix for common beta is bDbetag + sumbXtWX
          b1pbeta <<- b1prbeta + sumb1zbeta                         # sum of prior and the cumulative WLS contributions
          b1beta[1,1] <<- rnorm(1, 0, sd = 1)                       # sample from univariate standard normal(s)
          b1CC <<- chol(b1DXtWX)                                    # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
          b1CI <<- inverse(b1CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb1CI <<- t(b1CI)                                         # transpose
          b1pbeta <<- tb1CI %*% b1pbeta                             # rescale
          b1beta <<- b1pbeta + b1beta                               # center
          b1beta <<- b1CI %*% b1beta                                # rescale
          
          b1g[gp1] <- b1beta[1,1]
          b2g[gp1] <- 0
        }
        
        # Group-specific coefficient(s) reflect common value(s)
        for (i in 1:g) {
          b1g[i] <- b1g[gp1]
          b2g[i] <- b2g[gp1]
        }
        
      } else { # (flg == 5)
        
        ###############################################
        # Generate proposal for intercepts-only model #
        ###############################################
        
        for (i in 1:gp1) {
          b1g[i] <- 0
          b2g[i] <- 0
        }
        
      }
    }
    
    # Update model with new node values    
    model[['b1g']] <<- b1g
    model[['b2g']] <<- b2g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPb_bmaq 

# Custom Gibbs sampler for trend coefficients conditional on the intercepts in the quadratic BMA model with a level shift
sampler_CPb_xptl_bmaq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 4 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 4 | lt != p-2) {
      nimStop("Both nodes 'b1g' and 'b2g' must be sampled, conditionally on 's1ag' and 's2ag', using 'sampler_CPb_xptl_bmaq'.")
    } else {
      if (target[1] != 'b1g') {
        nimStop("1st target node must be the vector (length g+1) of linear coefficients 'b1g'.")
      } else {
        if (target[2] != 'b2g') {
          nimStop("2nd target node must be the vector (length g+1) of quadratic coefficients 'b2g'.")
        } 
      }
    }
    
    # Get other constants from model to use in calculations
    gp1 <- 0L
    g <- model$getConstants()$g
    gp1 <- g+1
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    
    b2Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b2XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2beta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1beta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2CC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1CC <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    tb2CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    tb1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and intercepts (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Get current integer-valued model flag (draws are conditional on the model flag)
    flg <- 0L
    flg <- as.integer(model[['flg']])
    if (flg > 5 | flg < 1) {
      nimStop("Expecting integer model flag values between 1 and 5 in 'sampler_CPb_xptl_bmaq'.")
    }
    
    # Node values to be replaced
    b1g <- model[['b1g']]
    b2g <- model[['b2g']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    ###########################################################
    # Populate conformal matrices depending on the model flag #
    ###########################################################
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]                                            # intercepts columns
      aX[j,2] <<- X[j,2]
    }

    if (flg == 1 | flg == 3) {                                      # quadratic: q = 2
      for (j in 1:n) {
        for (k in 3:4) {
          b2X[j,k-2] <<- X[j,k]
        }
      }
      b2Dbetag <<- nimMatrix(0, 2L, 2L)
      for (k in 3:4) {
        b2mbetag[k-2,1] <<- mbetag[k,1]
        b2Dbetag[k-2,k-2] <<- Dbetag[k,k]  
      }
      b2Xt <<- t(b2X)                                               # transpose bX
      b2prbeta <<- b2Dbetag %*% b2mbetag                            # contribution to posterior mean from prior
      
      if (flg == 3) {                                               # initialize cumulative sums in common case
        sumb2XtWX <<- nimMatrix(0, 2L, 2L)
        sumb2zbeta <<- nimMatrix(0, 2L, 1L)
      }
    } else { 
      if (flg == 2 | flg == 4) {                                    # linear: q = 1
        for (j in 1:n) {
          b1X[j,1] <<- X[j,3]
        }
        b1mbetag[1,1] <<- mbetag[3,1]
        b1Dbetag[1,1] <<- Dbetag[3,3]  
        b1Xt <<- t(b1X)                                             # transpose bX
        b1prbeta <<- b1Dbetag %*% b1mbetag                          # contribution to posterior mean from prior
        
        if (flg == 4) {                                             # initialize cumulative sums in common case
          sumb1XtWX[1,1] <<- 0
          sumb1zbeta[1,1] <<- 0
        }
      }
    }
    
    if (flg == 1 | flg == 2) {
      
      #####################################################
      # Generate proposal for group-specific trend models #
      #####################################################
      
      # Update coefficients conditional on intercepts
      for (i in 1:g) {                                              # cycle through each group independently
        abeta[1,1] <<- s1ag[i]                                      # group-specific abeta vector, segment 1
        abeta[2,1] <<- s2ag[i]                                      # group-specific abeta vector, segment 2
        aXbeta <<- aX %*% abeta                                     # vector of intercepts a
        
        VgD <<- nimMatrix(0, n, n)
        for (j in 1:n) {
          Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]               # populate nx1 data vector Zvec = Yvec - a
          VgD[j,j] <<- S2arr[(i-1)*n+j]                             # (Re-)set VgD to diagonal matrix of sampling variances
        }
        irho <- rhoarr[i]
        inu <- nuarr[i]
        Vg <<- ar1_cov_matrix(rts, irho, inu)                       # add AR covariance matrix Vgamma
        Vg <<- VgD + Vg
        Wg <<- inverse(Vg)                                          # Wg = Vg^{-1}
        
        if (flg == 1) {                                             # group-specific quadratic trend model
          b2XtW <<- b2Xt %*% Wg                                     # multiply Xt and Wg
          b2XtWX <<- b2XtW %*% b2X                                  # calculate XtWX, the precision matrix from WLS
          b2DXtWX <<- b2Dbetag + b2XtWX                             # posterior precision matrix for beta is Dbetag + XtWX
          b2zbeta <<- b2XtW %*% Zvec                                # contribution to posterior mean from WLS
          b2pbeta <<- b2prbeta + b2zbeta                            # sum of prior and WLS contributions
          for (k in 1:2) {
            b2beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal
          }
          b2CC <<- chol(b2DXtWX)                                    # Cholesky decomposition for precision matrix (R returns upper triangular)
          b2CI <<- inverse(b2CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb2CI <<- t(b2CI)                                         # transpose
          b2pbeta <<- tb2CI %*% b2pbeta                             # rescale
          b2beta <<- b2pbeta + b2beta                               # center
          b2beta <<- b2CI %*% b2beta                                # rescale
          
          b1g[i] <- b2beta[1,1]
          b2g[i] <- b2beta[2,1]
          
        } else { # (flg == 2)                                       # group-specific linear trend model
          b1XtW <<- b1Xt %*% Wg                                     # multiply Xt and Wg
          b1XtWX <<- b1XtW %*% b1X                                  # calculate XtWX, the precision matrix from WLS
          b1DXtWX <<- b1Dbetag + b1XtWX                             # posterior precision matrix for beta is Dbetag + XtWX
          b1zbeta <<- b1XtW %*% Zvec                                # contribution to posterior mean from WLS
          b1pbeta <<- b1prbeta + b1zbeta                            # sum of prior and WLS contributions
          b1beta[1,1] <<- rnorm(1, 0, sd = 1)                       # sample from univariate standard normal
          b1CC <<- chol(b1DXtWX)                                    # Cholesky decomposition for precision matrix (R returns upper triangular)
          b1CI <<- inverse(b1CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb1CI <<- t(b1CI)                                         # transpose
          b1pbeta <<- tb1CI %*% b1pbeta                             # rescale
          b1beta <<- b1pbeta + b1beta                               # center
          b1beta <<- b1CI %*% b1beta                                # rescale
          
          b1g[i] <- b1beta[1,1]
          b2g[i] <- 0
        }
      }                                                             # end cycle through groups
      
      # Common coefficients are averages of group-specific ones
      b1g[gp1] <- 0
      b2g[gp1] <- 0
      for (i in 1:g) {
        b1g[gp1] <- b1g[gp1] + b1g[i]
        b2g[gp1] <- b2g[gp1] + b2g[i] 
      }
      b1g[gp1] <- b1g[gp1]/g
      b2g[gp1] <- b2g[gp1]/g
      
    } else { 
      
      if (flg == 3 | flg == 4) {
        
        #############################################
        # Generate proposal for common trend models #
        #############################################
        
        # Update common coefficient(s) conditional on intercepts
        for (i in 1:g) {                                            # cycle through each group independently
          abeta[1,1] <<- s1ag[i]                                    # group-specific abeta vector, segment 1
          abeta[2,1] <<- s2ag[i]                                    # group-specific abeta vector, segment 2
          aXbeta <<- aX %*% abeta                                   # vector of intercepts a
          
          VgD <<- nimMatrix(0, n, n)
          for (j in 1:n) {
            Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]             # populate nx1 data vector Zvec = Yvec - a
            VgD[j,j] <<- S2arr[(i-1)*n+j]                           # (Re-)set VgD to diagonal matrix of sampling variances
          }
          irho <- rhoarr[i]
          inu <- nuarr[i]
          Vg <<- ar1_cov_matrix(rts, irho, inu)                     # add AR covariance matrix Vgamma
          Vg <<- VgD + Vg
          Wg <<- inverse(Vg)                                        # Wg = Vg^{-1}
          
          if (flg == 3) {                                           # common quadratic trend model
            b2XtW <<- b2Xt %*% Wg                                   # multiply bXt and Wg
            b2XtWX <<- b2XtW %*% b2X                                # calculate bXtWX
            sumb2XtWX <<- sumb2XtWX + b2XtWX                        # cumulative matrix sum
            b2zbeta <<- b2XtW %*% Zvec                              # contributions to posterior mean from WLS
            sumb2zbeta <<- sumb2zbeta + b2zbeta                     # cumulative matrix sum
            
          } else { # (flg == 4)                                     # common linear trend model
            b1XtW <<- b1Xt %*% Wg                                   # multiply bXt and Wg
            b1XtWX <<- b1XtW %*% b1X                                # calculate bXtWX
            sumb1XtWX <<- sumb1XtWX + b1XtWX                        # cumulative matrix sum
            b1zbeta <<- b1XtW %*% Zvec                              # contributions to posterior mean from WLS
            sumb1zbeta <<- sumb1zbeta + b1zbeta                     # cumulative matrix sum
          }
        }                                                           # end cycle through groups
        
        if (flg == 3) {                                             # common quadratic trend model
          b2DXtWX <<- b2Dbetag + sumb2XtWX                          # posterior precision matrix for common beta is bDbetag + sumbXtWX
          b2pbeta <<- b2prbeta + sumb2zbeta                         # sum of prior and the cumulative WLS contributions
          for (k in 1:2) {
            b2beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal(s)
          }
          b2CC <<- chol(b2DXtWX)                                    # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
          b2CI <<- inverse(b2CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb2CI <<- t(b2CI)                                         # transpose
          b2pbeta <<- tb2CI %*% b2pbeta                             # rescale
          b2beta <<- b2pbeta + b2beta                               # center
          b2beta <<- b2CI %*% b2beta                                # rescale
          
          b1g[gp1] <- b2beta[1,1]
          b2g[gp1] <- b2beta[2,1]
          
        } else { # (flg == 4)                                       # common linear trend model
          b1DXtWX <<- b1Dbetag + sumb1XtWX                          # posterior precision matrix for common beta is bDbetag + sumbXtWX
          b1pbeta <<- b1prbeta + sumb1zbeta                         # sum of prior and the cumulative WLS contributions
          b1beta[1,1] <<- rnorm(1, 0, sd = 1)                       # sample from univariate standard normal(s)
          b1CC <<- chol(b1DXtWX)                                    # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
          b1CI <<- inverse(b1CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb1CI <<- t(b1CI)                                         # transpose
          b1pbeta <<- tb1CI %*% b1pbeta                             # rescale
          b1beta <<- b1pbeta + b1beta                               # center
          b1beta <<- b1CI %*% b1beta                                # rescale
          
          b1g[gp1] <- b1beta[1,1]
          b2g[gp1] <- 0
        }
        
        # Group-specific coefficient(s) reflect common value(s)
        for (i in 1:g) {
          b1g[i] <- b1g[gp1]
          b2g[i] <- b2g[gp1]
        }
        
      } else { # (flg == 5)
        
        ###############################################
        # Generate proposal for intercepts-only model #
        ###############################################
        
        for (i in 1:gp1) {
          b1g[i] <- 0
          b2g[i] <- 0
        }
        
      }
    }
    
    # Update model with new node values    
    model[['b1g']] <<- b1g
    model[['b2g']] <<- b2g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPb_xptl_bmaq 

# Custom Gibbs sampler for trend coefficients conditional on the intercepts in the quadratic-quadratic BMA model with full trend break
sampler_CPb_xptf_bmaq_bmaq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 3 here)
    s1pm1 <- 0L
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    s1pm1 <- s1p - 1
    
    # Overall dimensionality (p == 6 here)
    pm1 <- 0L
    p <- model$getConstants()$p
    pm1 <- p-1
    
    # Make sure the 'target' values are correctly specified
    if (p != 6 | lt != p-2) {
      nimStop("All four nodes 's1b1g', 's1b2g', 's2b1g', and 's2b2g' must be sampled, conditionally on 's1ag' and 's2ag', using 'sampler_CPb_xptf_bmaq_bmaq'.")
    } else {
      if (target[1] != 's1b1g') {
        nimStop("1st target node must be the vector (length g+1) of linear coefficients 's1b1g'.")
      } else {
        if (target[2] != 's1b2g') {
          nimStop("2nd target node must be the vector (length g+1) of quadratic coefficients 's1b2g'.")
        } else {
          if (target[3] != 's2b1g') {
            nimStop("3rd target node must be the vector (length g+1) of linear coefficients 's2b1g'.")
          } else {
            if (target[4] != 's2b2g') {
              nimStop("4th target node must be the vector (length g+1) of quadratic coefficients 's2b2g'.")
            }
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    gp1 <- 0L
    g <- model$getConstants()$g
    gp1 <- g+1
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb3XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    sumb2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    sumb3zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    sumb2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b3mbetag <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2mbetag <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b3Dbetag <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2Dbetag <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b3X <- nimMatrix(nrow=n, ncol=4L, init = FALSE)
    b2X <- nimMatrix(nrow=n, ncol=3L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    
    b3Xt <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2Xt <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b3XtW <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2XtW <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b3XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b3DXtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2DXtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b3prbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2prbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b3pbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2pbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b3zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b3beta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2beta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1beta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b3CC <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2CC <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1CC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b3CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    tb3CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    tb2CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    tb1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and intercepts (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Get current integer-valued model flags (draws are conditional on the model flags)
    s1flg <- 0L
    s1flg <- as.integer(model[['s1flg']])
    if (s1flg > 5 | s1flg < 1) {
      nimStop("Expecting integer model flag values for segment 1 between 1 and 5 in 'sampler_CPb_xptf_bmaq_bmaq'.")
    }
    s2flg <- 0L
    s2flg <- as.integer(model[['s2flg']])
    if (s2flg > 5 | s2flg < 1) {
      nimStop("Expecting integer model flag values for segment 2 between 1 and 5 in 'sampler_CPb_xptf_bmaq_bmaq'.")
    }
    
    # Combination flag for internal use
    iflg <- 0L
    iflg <- 10*s1flg + s2flg
    
    # Node values to be replaced
    s1b1g <- model[['s1b1g']]
    s1b2g <- model[['s1b2g']]
    s2b1g <- model[['s2b1g']]
    s2b2g <- model[['s2b2g']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    ###########################################################
    # Populate conformal matrices depending on the model flag #
    ###########################################################
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    
    if (iflg == 11 | iflg == 33) {                                  # quadratic-quadratic
      for (j in 1:n) {
        for (k in 2:s1p) {
          b3X[j,k-1] <<- X[j,k]
        }
        for (k in s1pp2:p) {
          b3X[j,k-2] <<- X[j,k]
        }
      }
      b3Dbetag <<- nimMatrix(0, 4L, 4L)
      for (k in 2:s1p) {
        b3mbetag[k-1,1] <<- mbetag[k,1]
        b3Dbetag[k-1,k-1] <<- Dbetag[k,k]  
      }
      for (k in s1pp2:p) {
        b3mbetag[k-2,1] <<- mbetag[k,1]
        b3Dbetag[k-2,k-2] <<- Dbetag[k,k]  
      }
      b3Xt <<- t(b3X)                                               # transpose bX
      b3prbeta <<- b3Dbetag %*% b3mbetag                            # contribution to posterior mean from prior
      
      if (iflg == 33) {                                             # initialize cumulative sums in common case
        sumb3XtWX <<- nimMatrix(0, 4L, 4L)
        sumb3zbeta <<- nimMatrix(0, 4L, 1L)
      }
    } else {
      if (iflg == 12 | iflg == 34) {                                # quadratic-linear
        for (j in 1:n) {
          for (k in 2:s1p) {
            b2X[j,k-1] <<- X[j,k]
          }
          for (k in s1pp2:pm1) {
            b2X[j,k-2] <<- X[j,k]
          }
        }
        b2Dbetag <<- nimMatrix(0, 3L, 3L)
        for (k in 2:s1p) {
          b2mbetag[k-1,1] <<- mbetag[k,1]
          b2Dbetag[k-1,k-1] <<- Dbetag[k,k]  
        }
        for (k in s1pp2:pm1) {
          b2mbetag[k-2,1] <<- mbetag[k,1]
          b2Dbetag[k-2,k-2] <<- Dbetag[k,k]  
        }
        b2Xt <<- t(b2X)                                             # transpose bX
        b2prbeta <<- b2Dbetag %*% b2mbetag                          # contribution to posterior mean from prior
        
        if (iflg == 34) {                                           # initialize cumulative sums in common case
          sumb2XtWX <<- nimMatrix(0, 3L, 3L)
          sumb2zbeta <<- nimMatrix(0, 3L, 1L)
        }
      } else {
        if (iflg == 22 | iflg == 44) {                              # linear-linear
          for (j in 1:n) {
            for (k in 2:s1pm1) {
              b1X[j,k-1] <<- X[j,k]
            }
            for (k in s1pp2:pm1) {
              b1X[j,k-3] <<- X[j,k]
            }
          }
          b1Dbetag <<- nimMatrix(0, 2L, 2L)
          for (k in 2:s1pm1) {
            b1mbetag[k-1,1] <<- mbetag[k,1]
            b1Dbetag[k-1,k-1] <<- Dbetag[k,k]  
          }
          for (k in s1pp2:pm1) {
            b1mbetag[k-3,1] <<- mbetag[k,1]
            b1Dbetag[k-3,k-3] <<- Dbetag[k,k]  
          }
          b1Xt <<- t(b1X)                                           # transpose bX
          b1prbeta <<- b1Dbetag %*% b1mbetag                        # contribution to posterior mean from prior
          
          if (iflg == 44) {                                         # initialize cumulative sums in common case
            sumb1XtWX <<- nimMatrix(0, 2L, 2L)
            sumb1zbeta <<- nimMatrix(0, 2L, 1L)
          }
        }
      }
    }

    
    if (iflg == 11 | iflg == 12 | iflg == 22) {
      
      #####################################################
      # Generate proposal for group-specific trend models #
      #####################################################
      
      # Update coefficients conditional on intercepts
      for (i in 1:g) {                                              # cycle through each group independently
        abeta[1,1] <<- s1ag[i]                                      # group-specific abeta vector, segment 1
        abeta[2,1] <<- s2ag[i]                                      # group-specific abeta vector, segment 2
        aXbeta <<- aX %*% abeta                                     # vector of intercepts a
        
        VgD <<- nimMatrix(0, n, n)
        for (j in 1:n) {
          Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]               # populate nx1 data vector Zvec = Yvec - a
          VgD[j,j] <<- S2arr[(i-1)*n+j]                             # (Re-)set VgD to diagonal matrix of sampling variances
        }
        irho <- rhoarr[i]
        inu <- nuarr[i]
        Vg <<- ar1_cov_matrix(rts, irho, inu)                       # add AR covariance matrix Vgamma
        Vg <<- VgD + Vg
        Wg <<- inverse(Vg)                                          # Wg = Vg^{-1}
        
        if (iflg == 11) {                                           # group-specific quadratic-quadratic trend model
          b3XtW <<- b3Xt %*% Wg                                     # multiply Xt and Wg
          b3XtWX <<- b3XtW %*% b3X                                  # calculate XtWX, the precision matrix from WLS
          b3DXtWX <<- b3Dbetag + b3XtWX                             # posterior precision matrix for beta is Dbetag + XtWX
          b3zbeta <<- b3XtW %*% Zvec                                # contribution to posterior mean from WLS
          b3pbeta <<- b3prbeta + b3zbeta                            # sum of prior and WLS contributions
          for (k in 1:4) {
            b3beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal
          }
          b3CC <<- chol(b3DXtWX)                                    # Cholesky decomposition for precision matrix (R returns upper triangular)
          b3CI <<- inverse(b3CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb3CI <<- t(b3CI)                                         # transpose
          b3pbeta <<- tb3CI %*% b3pbeta                             # rescale
          b3beta <<- b3pbeta + b3beta                               # center
          b3beta <<- b3CI %*% b3beta                                # rescale
          
          s1b1g[i] <- b3beta[1,1]
          s1b2g[i] <- b3beta[2,1]
          s2b1g[i] <- b3beta[3,1]
          s2b2g[i] <- b3beta[4,1]
          
        } else {
          if (iflg == 12) {                                         # group-specific quadratic-linear trend model
            b2XtW <<- b2Xt %*% Wg                                   # multiply Xt and Wg
            b2XtWX <<- b2XtW %*% b2X                                # calculate XtWX, the precision matrix from WLS
            b2DXtWX <<- b2Dbetag + b2XtWX                           # posterior precision matrix for beta is Dbetag + XtWX
            b2zbeta <<- b2XtW %*% Zvec                              # contribution to posterior mean from WLS
            b2pbeta <<- b2prbeta + b2zbeta                          # sum of prior and WLS contributions
            for (k in 1:3) {
              b2beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal
            }
            b2CC <<- chol(b2DXtWX)                                  # Cholesky decomposition for precision matrix (R returns upper triangular)
            b2CI <<- inverse(b2CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb2CI <<- t(b2CI)                                       # transpose
            b2pbeta <<- tb2CI %*% b2pbeta                           # rescale
            b2beta <<- b2pbeta + b2beta                             # center
            b2beta <<- b2CI %*% b2beta                              # rescale
            
            s1b1g[i] <- b2beta[1,1]
            s1b2g[i] <- b2beta[2,1]
            s2b1g[i] <- b2beta[3,1]
            s2b2g[i] <- 0
            
          } else { # (iflg == 22)                                   # group-specific linear-linear trend model
            b1XtW <<- b1Xt %*% Wg                                   # multiply Xt and Wg
            b1XtWX <<- b1XtW %*% b1X                                # calculate XtWX, the precision matrix from WLS
            b1DXtWX <<- b1Dbetag + b1XtWX                           # posterior precision matrix for beta is Dbetag + XtWX
            b1zbeta <<- b1XtW %*% Zvec                              # contribution to posterior mean from WLS
            b1pbeta <<- b1prbeta + b1zbeta                          # sum of prior and WLS contributions
            for (k in 1:2) {
              b1beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal
            }
            b1CC <<- chol(b1DXtWX)                                  # Cholesky decomposition for precision matrix (R returns upper triangular)
            b1CI <<- inverse(b1CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb1CI <<- t(b1CI)                                       # transpose
            b1pbeta <<- tb1CI %*% b1pbeta                           # rescale
            b1beta <<- b1pbeta + b1beta                             # center
            b1beta <<- b1CI %*% b1beta                              # rescale
            
            s1b1g[i] <- b1beta[1,1]
            s1b2g[i] <- 0
            s2b1g[i] <- b1beta[2,1]
            s2b2g[i] <- 0
            
          }
        }
      }                                                             # end cycle through groups
      
      # Common coefficients are averages of group-specific ones
      s1b1g[gp1] <- 0
      s1b2g[gp1] <- 0
      s2b1g[gp1] <- 0
      s2b2g[gp1] <- 0
      for (i in 1:g) {
        s1b1g[gp1] <- s1b1g[gp1] + s1b1g[i]
        s1b2g[gp1] <- s1b2g[gp1] + s1b2g[i] 
        s2b1g[gp1] <- s2b1g[gp1] + s2b1g[i]
        s2b2g[gp1] <- s2b2g[gp1] + s2b2g[i] 
      }
      s1b1g[gp1] <- s1b1g[gp1]/g
      s1b2g[gp1] <- s1b2g[gp1]/g
      s2b1g[gp1] <- s2b1g[gp1]/g
      s2b2g[gp1] <- s2b2g[gp1]/g
      
    } else { 
      
      if (iflg == 33 | iflg == 34 | iflg == 44) {
        
        #############################################
        # Generate proposal for common trend models #
        #############################################
        
        # Update common coefficient(s) conditional on intercepts
        for (i in 1:g) {                                            # cycle through each group independently
          abeta[1,1] <<- s1ag[i]                                    # group-specific abeta vector, segment 1
          abeta[2,1] <<- s2ag[i]                                    # group-specific abeta vector, segment 2
          aXbeta <<- aX %*% abeta                                   # vector of intercepts a
          
          VgD <<- nimMatrix(0, n, n)
          for (j in 1:n) {
            Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]             # populate nx1 data vector Zvec = Yvec - a
            VgD[j,j] <<- S2arr[(i-1)*n+j]                           # (Re-)set VgD to diagonal matrix of sampling variances
          }
          irho <- rhoarr[i]
          inu <- nuarr[i]
          Vg <<- ar1_cov_matrix(rts, irho, inu)                     # add AR covariance matrix Vgamma
          Vg <<- VgD + Vg
          Wg <<- inverse(Vg)                                        # Wg = Vg^{-1}
          
          if (iflg == 33) {                                         # common quadratic-quadratic trend model
            b3XtW <<- b3Xt %*% Wg                                   # multiply bXt and Wg
            b3XtWX <<- b3XtW %*% b3X                                # calculate bXtWX
            sumb3XtWX <<- sumb3XtWX + b3XtWX                        # cumulative matrix sum
            b3zbeta <<- b3XtW %*% Zvec                              # contributions to posterior mean from WLS
            sumb3zbeta <<- sumb3zbeta + b3zbeta                     # cumulative matrix sum
          } else {
            if (iflg == 34) {                                       # common quadratic-linear trend model
              b2XtW <<- b2Xt %*% Wg                                 # multiply bXt and Wg
              b2XtWX <<- b2XtW %*% b2X                              # calculate bXtWX
              sumb2XtWX <<- sumb2XtWX + b2XtWX                      # cumulative matrix sum
              b2zbeta <<- b2XtW %*% Zvec                            # contributions to posterior mean from WLS
              sumb2zbeta <<- sumb2zbeta + b2zbeta                   # cumulative matrix sum
              
            } else { # (iflg == 44)                                 # common linear-linear trend model
              b1XtW <<- b1Xt %*% Wg                                 # multiply bXt and Wg
              b1XtWX <<- b1XtW %*% b1X                              # calculate bXtWX
              sumb1XtWX <<- sumb1XtWX + b1XtWX                      # cumulative matrix sum
              b1zbeta <<- b1XtW %*% Zvec                            # contributions to posterior mean from WLS
              sumb1zbeta <<- sumb1zbeta + b1zbeta                   # cumulative matrix sum
            }
          }
        }                                                           # end cycle through groups
        
        if (iflg == 33) {                                           # common quadratic-quadratic trend model
          b3DXtWX <<- b3Dbetag + sumb3XtWX                          # posterior precision matrix for common beta is bDbetag + sumbXtWX
          b3pbeta <<- b3prbeta + sumb3zbeta                         # sum of prior and the cumulative WLS contributions
          for (k in 1:4) {
            b3beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal(s)
          }
          b3CC <<- chol(b3DXtWX)                                    # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
          b3CI <<- inverse(b3CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb3CI <<- t(b3CI)                                         # transpose
          b3pbeta <<- tb3CI %*% b3pbeta                             # rescale
          b3beta <<- b3pbeta + b3beta                               # center
          b3beta <<- b3CI %*% b3beta                                # rescale
          
          s1b1g[gp1] <- b3beta[1,1]
          s1b2g[gp1] <- b3beta[2,1]
          s2b1g[gp1] <- b3beta[3,1]
          s2b2g[gp1] <- b3beta[4,1]
          
        } else {
          if (iflg == 34) {                                         # common quadratic-linear trend model
            b2DXtWX <<- b2Dbetag + sumb2XtWX                        # posterior precision matrix for common beta is bDbetag + sumbXtWX
            b2pbeta <<- b2prbeta + sumb2zbeta                       # sum of prior and the cumulative WLS contributions
            for (k in 1:3) {
              b2beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal(s)
            }
            b2CC <<- chol(b2DXtWX)                                  # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
            b2CI <<- inverse(b2CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb2CI <<- t(b2CI)                                       # transpose
            b2pbeta <<- tb2CI %*% b2pbeta                           # rescale
            b2beta <<- b2pbeta + b2beta                             # center
            b2beta <<- b2CI %*% b2beta                              # rescale
            
            s1b1g[gp1] <- b2beta[1,1]
            s1b2g[gp1] <- b2beta[2,1]
            s2b1g[gp1] <- b2beta[3,1]
            s2b2g[gp1] <- 0
            
          } else { # (iflg == 44)                                   # common linear-linear trend model
            b1DXtWX <<- b1Dbetag + sumb1XtWX                        # posterior precision matrix for common beta is bDbetag + sumbXtWX
            b1pbeta <<- b1prbeta + sumb1zbeta                       # sum of prior and the cumulative WLS contributions
            for (k in 1:2) {
              b1beta[k,1] <<- rnorm(1, 0, sd = 1)                   # sample from univariate standard normal(s)
            }
            b1CC <<- chol(b1DXtWX)                                  # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
            b1CI <<- inverse(b1CC)                                  # inverse of upper triangular matrix from Cholesky decomposition
            tb1CI <<- t(b1CI)                                       # transpose
            b1pbeta <<- tb1CI %*% b1pbeta                           # rescale
            b1beta <<- b1pbeta + b1beta                             # center
            b1beta <<- b1CI %*% b1beta                              # rescale
            
            s1b1g[gp1] <- b1beta[1,1]
            s1b2g[gp1] <- 0
            s2b1g[gp1] <- b1beta[2,1]
            s2b2g[gp1] <- 0
            
          } 
        }
        
        # Group-specific coefficient(s) reflect common value(s)
        for (i in 1:g) {
          s1b1g[i] <- s1b1g[gp1]
          s1b2g[i] <- s1b2g[gp1]
          s2b1g[i] <- s2b1g[gp1]
          s2b2g[i] <- s2b2g[gp1]
        }
        
      } else { # (iflg == 55)
        
        ###############################################
        # Generate proposal for intercepts-only model #
        ###############################################
        
        for (i in 1:gp1) {
          s1b1g[i] <- 0
          s1b2g[i] <- 0
          s2b1g[i] <- 0
          s2b2g[i] <- 0
        }
        
      }
    }
    
    # Update model with new node values    
    model[['s1b1g']] <<- s1b1g
    model[['s1b2g']] <<- s1b2g
    model[['s2b1g']] <<- s2b1g
    model[['s2b2g']] <<- s2b2g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPb_xptf_bmaq_bmaq

# Custom Gibbs sampler for trend coefficients conditional on the intercepts in the quadratic-linear BMA model with full trend break
sampler_CPb_xptf_bmaq_bmal <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 3 here)
    s1pm1 <- 0L
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    s1pm1 <- s1p - 1
    
    # Overall dimensionality (p == 5 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 5 | lt != p-2) {
      nimStop("All three nodes 's1b1g', 's1b2g', and 's2b1g' must be sampled, conditionally on 's1ag' and 's2ag', using 'sampler_CPb_xptf_bmaq_bmal'.")
    } else {
      if (target[1] != 's1b1g') {
        nimStop("1st target node must be the vector (length g+1) of linear coefficients 's1b1g'.")
      } else {
        if (target[2] != 's1b2g') {
          nimStop("2nd target node must be the vector (length g+1) of quadratic coefficients 's1b2g'.")
        } else {
          if (target[3] != 's2b1g') {
            nimStop("3rd target node must be the vector (length g+1) of linear coefficients 's2b1g'.")
          }
        }
      }
    }
    
    # Get other constants from model to use in calculations
    gp1 <- 0L
    g <- model$getConstants()$g
    gp1 <- g+1
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    sumb2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b2mbetag <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b2Dbetag <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b2X <- nimMatrix(nrow=n, ncol=3L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    
    b2Xt <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b2XtW <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b2DXtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b2prbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b2pbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b2beta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1beta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b2CC <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1CC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b2CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    tb2CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    tb1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and intercepts (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Get current integer-valued model flags (draws are conditional on the model flags)
    s1flg <- 0L
    s1flg <- as.integer(model[['s1flg']])
    if (s1flg > 5 | s1flg < 1) {
      nimStop("Expecting integer model flag values for segment 1 between 1 and 5 in 'sampler_CPb_xptf_bmaq_bmal'.")
    }
    s2flg <- 0L
    s2flg <- as.integer(model[['s2flg']])
    if (s2flg > 3 | s2flg < 1) {
      nimStop("Expecting integer model flag values for segment 2 between 1 and 3 in 'sampler_CPb_xptf_bmaq_bmal'.")
    }
    
    # Combination flag for internal use
    iflg <- 0L
    iflg <- 10*s1flg + s2flg
    
    # Node values to be replaced
    s1b1g <- model[['s1b1g']]
    s1b2g <- model[['s1b2g']]
    s2b1g <- model[['s2b1g']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    ###########################################################
    # Populate conformal matrices depending on the model flag #
    ###########################################################
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    
    if (iflg == 11 | iflg == 32) {                                  # quadratic-linear
      for (j in 1:n) {
        for (k in 2:s1p) {
          b2X[j,k-1] <<- X[j,k]
        }
        for (k in s1pp2:p) {
          b2X[j,k-2] <<- X[j,k]
        }
      }
      b2Dbetag <<- nimMatrix(0, 3L, 3L)
      for (k in 2:s1p) {
        b2mbetag[k-1,1] <<- mbetag[k,1]
        b2Dbetag[k-1,k-1] <<- Dbetag[k,k]  
      }
      for (k in s1pp2:p) {
        b2mbetag[k-2,1] <<- mbetag[k,1]
        b2Dbetag[k-2,k-2] <<- Dbetag[k,k]  
      }
      b2Xt <<- t(b2X)                                               # transpose bX
      b2prbeta <<- b2Dbetag %*% b2mbetag                            # contribution to posterior mean from prior
      
      if (iflg == 32) {                                             # initialize cumulative sums in common case
        sumb2XtWX <<- nimMatrix(0, 3L, 3L)
        sumb2zbeta <<- nimMatrix(0, 3L, 1L)
      }
    } else {
      if (iflg == 21 | iflg == 42) {                                # linear-linear
        for (j in 1:n) {
          for (k in 2:s1pm1) {
            b1X[j,k-1] <<- X[j,k]
          }
          for (k in s1pp2:p) {
            b1X[j,k-3] <<- X[j,k]
          }
        }
        b1Dbetag <<- nimMatrix(0, 2L, 2L)
        for (k in 2:s1pm1) {
          b1mbetag[k-1,1] <<- mbetag[k,1]
          b1Dbetag[k-1,k-1] <<- Dbetag[k,k]  
        }
        for (k in s1pp2:p) {
          b1mbetag[k-3,1] <<- mbetag[k,1]
          b1Dbetag[k-3,k-3] <<- Dbetag[k,k]  
        }
        b1Xt <<- t(b1X)                                             # transpose bX
        b1prbeta <<- b1Dbetag %*% b1mbetag                          # contribution to posterior mean from prior
        
        if (iflg == 42) {                                           # initialize cumulative sums in common case
          sumb1XtWX <<- nimMatrix(0, 2L, 2L)
          sumb1zbeta <<- nimMatrix(0, 2L, 1L)
        }
      }
    }
    
    if (iflg == 11 | iflg == 21) {
      
      #####################################################
      # Generate proposal for group-specific trend models #
      #####################################################
      
      # Update coefficients conditional on intercepts
      for (i in 1:g) {                                              # cycle through each group independently
        abeta[1,1] <<- s1ag[i]                                      # group-specific abeta vector, segment 1
        abeta[2,1] <<- s2ag[i]                                      # group-specific abeta vector, segment 2
        aXbeta <<- aX %*% abeta                                     # vector of intercepts a
        
        VgD <<- nimMatrix(0, n, n)
        for (j in 1:n) {
          Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]               # populate nx1 data vector Zvec = Yvec - a
          VgD[j,j] <<- S2arr[(i-1)*n+j]                             # (Re-)set VgD to diagonal matrix of sampling variances
        }
        irho <- rhoarr[i]
        inu <- nuarr[i]
        Vg <<- ar1_cov_matrix(rts, irho, inu)                       # add AR covariance matrix Vgamma
        Vg <<- VgD + Vg
        Wg <<- inverse(Vg)                                          # Wg = Vg^{-1}
        
        if (iflg == 11) {                                           # group-specific quadratic-linear trend model
          b2XtW <<- b2Xt %*% Wg                                     # multiply Xt and Wg
          b2XtWX <<- b2XtW %*% b2X                                  # calculate XtWX, the precision matrix from WLS
          b2DXtWX <<- b2Dbetag + b2XtWX                             # posterior precision matrix for beta is Dbetag + XtWX
          b2zbeta <<- b2XtW %*% Zvec                                # contribution to posterior mean from WLS
          b2pbeta <<- b2prbeta + b2zbeta                            # sum of prior and WLS contributions
          for (k in 1:3) {
            b2beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal
          }
          b2CC <<- chol(b2DXtWX)                                    # Cholesky decomposition for precision matrix (R returns upper triangular)
          b2CI <<- inverse(b2CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb2CI <<- t(b2CI)                                         # transpose
          b2pbeta <<- tb2CI %*% b2pbeta                             # rescale
          b2beta <<- b2pbeta + b2beta                               # center
          b2beta <<- b2CI %*% b2beta                                # rescale
          
          s1b1g[i] <- b2beta[1,1]
          s1b2g[i] <- b2beta[2,1]
          s2b1g[i] <- b2beta[3,1]
          
        } else { # (iflg == 21)                                     # group-specific linear-linear trend model
          b1XtW <<- b1Xt %*% Wg                                     # multiply Xt and Wg
          b1XtWX <<- b1XtW %*% b1X                                  # calculate XtWX, the precision matrix from WLS
          b1DXtWX <<- b1Dbetag + b1XtWX                             # posterior precision matrix for beta is Dbetag + XtWX
          b1zbeta <<- b1XtW %*% Zvec                                # contribution to posterior mean from WLS
          b1pbeta <<- b1prbeta + b1zbeta                            # sum of prior and WLS contributions
          for (k in 1:2) {
            b1beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal
          }
          b1CC <<- chol(b1DXtWX)                                    # Cholesky decomposition for precision matrix (R returns upper triangular)
          b1CI <<- inverse(b1CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb1CI <<- t(b1CI)                                         # transpose
          b1pbeta <<- tb1CI %*% b1pbeta                             # rescale
          b1beta <<- b1pbeta + b1beta                               # center
          b1beta <<- b1CI %*% b1beta                                # rescale
          
          s1b1g[i] <- b1beta[1,1]
          s1b2g[i] <- 0
          s2b1g[i] <- b1beta[2,1]
          
        }
      }                                                             # end cycle through groups
      
      # Common coefficients are averages of group-specific ones
      s1b1g[gp1] <- 0
      s1b2g[gp1] <- 0
      s2b1g[gp1] <- 0
      for (i in 1:g) {
        s1b1g[gp1] <- s1b1g[gp1] + s1b1g[i]
        s1b2g[gp1] <- s1b2g[gp1] + s1b2g[i] 
        s2b1g[gp1] <- s2b1g[gp1] + s2b1g[i]
      }
      s1b1g[gp1] <- s1b1g[gp1]/g
      s1b2g[gp1] <- s1b2g[gp1]/g
      s2b1g[gp1] <- s2b1g[gp1]/g
      
    } else { 
      
      if (iflg == 32 | iflg == 42) {
        
        #############################################
        # Generate proposal for common trend models #
        #############################################
        
        # Update common coefficient(s) conditional on intercepts
        for (i in 1:g) {                                            # cycle through each group independently
          abeta[1,1] <<- s1ag[i]                                    # group-specific abeta vector, segment 1
          abeta[2,1] <<- s2ag[i]                                    # group-specific abeta vector, segment 2
          aXbeta <<- aX %*% abeta                                   # vector of intercepts a
          
          VgD <<- nimMatrix(0, n, n)
          for (j in 1:n) {
            Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]             # populate nx1 data vector Zvec = Yvec - a
            VgD[j,j] <<- S2arr[(i-1)*n+j]                           # (Re-)set VgD to diagonal matrix of sampling variances
          }
          irho <- rhoarr[i]
          inu <- nuarr[i]
          Vg <<- ar1_cov_matrix(rts, irho, inu)                     # add AR covariance matrix Vgamma
          Vg <<- VgD + Vg
          Wg <<- inverse(Vg)                                        # Wg = Vg^{-1}
          
          if (iflg == 32) {                                         # common quadratic-linear trend model
            b2XtW <<- b2Xt %*% Wg                                   # multiply bXt and Wg
            b2XtWX <<- b2XtW %*% b2X                                # calculate bXtWX
            sumb2XtWX <<- sumb2XtWX + b2XtWX                        # cumulative matrix sum
            b2zbeta <<- b2XtW %*% Zvec                              # contributions to posterior mean from WLS
            sumb2zbeta <<- sumb2zbeta + b2zbeta                     # cumulative matrix sum
            
          } else { # (iflg == 42)                                   # common linear-linear trend model
            b1XtW <<- b1Xt %*% Wg                                   # multiply bXt and Wg
            b1XtWX <<- b1XtW %*% b1X                                # calculate bXtWX
            sumb1XtWX <<- sumb1XtWX + b1XtWX                        # cumulative matrix sum
            b1zbeta <<- b1XtW %*% Zvec                              # contributions to posterior mean from WLS
            sumb1zbeta <<- sumb1zbeta + b1zbeta                     # cumulative matrix sum
          }
        }                                                           # end cycle through groups
        
        if (iflg == 32) {                                           # common quadratic-linear trend model
          b2DXtWX <<- b2Dbetag + sumb2XtWX                          # posterior precision matrix for common beta is bDbetag + sumbXtWX
          b2pbeta <<- b2prbeta + sumb2zbeta                         # sum of prior and the cumulative WLS contributions
          for (k in 1:3) {
            b2beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal(s)
          }
          b2CC <<- chol(b2DXtWX)                                    # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
          b2CI <<- inverse(b2CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb2CI <<- t(b2CI)                                         # transpose
          b2pbeta <<- tb2CI %*% b2pbeta                             # rescale
          b2beta <<- b2pbeta + b2beta                               # center
          b2beta <<- b2CI %*% b2beta                                # rescale
          
          s1b1g[gp1] <- b2beta[1,1]
          s1b2g[gp1] <- b2beta[2,1]
          s2b1g[gp1] <- b2beta[3,1]
          
        } else { # (iflg == 42)                                     # common linear-linear trend model
          b1DXtWX <<- b1Dbetag + sumb1XtWX                          # posterior precision matrix for common beta is bDbetag + sumbXtWX
          b1pbeta <<- b1prbeta + sumb1zbeta                         # sum of prior and the cumulative WLS contributions
          for (k in 1:2) {
            b1beta[k,1] <<- rnorm(1, 0, sd = 1)                     # sample from univariate standard normal(s)
          }
          b1CC <<- chol(b1DXtWX)                                    # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
          b1CI <<- inverse(b1CC)                                    # inverse of upper triangular matrix from Cholesky decomposition
          tb1CI <<- t(b1CI)                                         # transpose
          b1pbeta <<- tb1CI %*% b1pbeta                             # rescale
          b1beta <<- b1pbeta + b1beta                               # center
          b1beta <<- b1CI %*% b1beta                                # rescale
          
          s1b1g[gp1] <- b1beta[1,1]
          s1b2g[gp1] <- 0
          s2b1g[gp1] <- b1beta[2,1]
            
        }
        
        # Group-specific coefficient(s) reflect common value(s)
        for (i in 1:g) {
          s1b1g[i] <- s1b1g[gp1]
          s1b2g[i] <- s1b2g[gp1]
          s2b1g[i] <- s2b1g[gp1]
        }
        
      } else { # (iflg == 53)
        
        ###############################################
        # Generate proposal for intercepts-only model #
        ###############################################
        
        for (i in 1:gp1) {
          s1b1g[i] <- 0
          s1b2g[i] <- 0
          s2b1g[i] <- 0
        }
        
      }
    }
    
    # Update model with new node values    
    model[['s1b1g']] <<- s1b1g
    model[['s1b2g']] <<- s1b2g
    model[['s2b1g']] <<- s2b1g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPb_xptf_bmaq_bmal

# Custom Gibbs sampler for trend coefficients conditional on the intercepts in the linear BMA model
sampler_CPb_bmal <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 2 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 2 | lt != p-1) {
      nimStop("Only node 'b1g' can be sampled, conditionally on 'ag', using 'sampler_CPb_bmal'.")
    } else {
      if (target[1] != 'b1g') {
        nimStop("1st target node must be the vector (length g+1) of linear coefficients 'b1g'.")
      }
    }
    
    # Get other constants from model to use in calculations
    gp1 <- 0L
    g <- model$getConstants()$g
    gp1 <- g+1
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1mbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1Dbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1X <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    
    b1Xt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b1XtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1DXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1prbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1pbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1beta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1CC <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    tb1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and intercepts (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    ag <- model[['ag']]
    
    # Get current integer-valued model flag (draws are conditional on the model flag)
    flg <- 0L
    flg <- as.integer(model[['flg']])
    if (flg > 3 | flg < 1) {
      nimStop("Expecting integer model flag values between 1 and 3 in 'sampler_CPb_bmal'.")
    }
    
    # Node values to be replaced
    b1g <- model[['b1g']]

    # Working variables
    i <- 0L 
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    ###########################################################
    # Populate conformal matrices depending on the model flag #
    ###########################################################
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]                                            # intercepts column from design matrix
    }
    
    if (flg == 1 | flg == 2) {                                      # linear
      for (j in 1:n) {
        b1X[j,1] <<- X[j,2]
      }
      b1mbetag[1,1] <<- mbetag[2,1]
      b1Dbetag[1,1] <<- Dbetag[2,2]  
      b1Xt <<- t(b1X)                                               # transpose bX
      b1prbeta <<- b1Dbetag %*% b1mbetag                            # contribution to posterior mean from prior
      
      if (flg == 2) {                                               # initialize cumulative sums in common case
        sumb1XtWX[1,1] <<- 0
        sumb1zbeta[1,1] <<- 0
      }
    }
    
    if (flg == 1) {
      
      ####################################################
      # Generate proposal for group-specific trend model #
      ####################################################
      
      # Update coefficients conditional on intercepts
      for (i in 1:g) {                                              # cycle through each group independently
        abeta[1,1] <<- ag[i]                                        # group-specific abeta vector
        aXbeta <<- aX %*% abeta                                     # vector of intercepts a
        
        VgD <<- nimMatrix(0, n, n)
        for (j in 1:n) {
          Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]               # populate nx1 data vector Zvec = Yvec - a
          VgD[j,j] <<- S2arr[(i-1)*n+j]                             # (Re-)set VgD to diagonal matrix of sampling variances
        }
        irho <- rhoarr[i]
        inu <- nuarr[i]
        Vg <<- ar1_cov_matrix(rts, irho, inu)                       # add AR covariance matrix Vgamma
        Vg <<- VgD + Vg
        Wg <<- inverse(Vg)                                          # Wg = Vg^{-1}
        
        b1XtW <<- b1Xt %*% Wg                                       # multiply Xt and Wg
        b1XtWX <<- b1XtW %*% b1X                                    # calculate XtWX, the precision matrix from WLS
        b1DXtWX <<- b1Dbetag + b1XtWX                               # posterior precision matrix for beta is Dbetag + XtWX
        b1zbeta <<- b1XtW %*% Zvec                                  # contribution to posterior mean from WLS
        b1pbeta <<- b1prbeta + b1zbeta                              # sum of prior and WLS contributions
        
        b1beta[1,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal
        b1CC <<- chol(b1DXtWX)                                      # Cholesky decomposition for precision matrix (R returns upper triangular)
        b1CI <<- inverse(b1CC)                                      # inverse of upper triangular matrix from Cholesky decomposition
        tb1CI <<- t(b1CI)                                           # transpose
        b1pbeta <<- tb1CI %*% b1pbeta                               # rescale
        b1beta <<- b1pbeta + b1beta                                 # center
        b1beta <<- b1CI %*% b1beta                                  # rescale
        
        b1g[i] <- b1beta[1,1]
      }                                                             # end cycle through groups
      
      # Common coefficient is average of group-specific ones
      b1g[gp1] <- 0
      for (i in 1:g) {
        b1g[gp1] <- b1g[gp1] + b1g[i]
      }
      b1g[gp1] <- b1g[gp1]/g

    } else { 
      
      if (flg == 2) {
        
        ############################################
        # Generate proposal for common trend model #
        ############################################
        
        # Update common coefficient conditional on intercepts
        for (i in 1:g) {                                            # cycle through each group independently
          abeta[1,1] <<- ag[i]                                      # group-specific abeta vector
          aXbeta <<- aX %*% abeta                                   # vector of intercepts a
          
          VgD <<- nimMatrix(0, n, n)
          for (j in 1:n) {
            Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]             # populate nx1 data vector Zvec = Yvec - a
            VgD[j,j] <<- S2arr[(i-1)*n+j]                           # (Re-)set VgD to diagonal matrix of sampling variances
          }
          irho <- rhoarr[i]
          inu <- nuarr[i]
          Vg <<- ar1_cov_matrix(rts, irho, inu)                     # add AR covariance matrix Vgamma
          Vg <<- VgD + Vg
          Wg <<- inverse(Vg)                                        # Wg = Vg^{-1}
          
          b1XtW <<- b1Xt %*% Wg                                     # multiply bXt and Wg
          b1XtWX <<- b1XtW %*% b1X                                  # calculate bXtWX
          sumb1XtWX <<- sumb1XtWX + b1XtWX                          # cumulative matrix sum
          b1zbeta <<- b1XtW %*% Zvec                                # contributions to posterior mean from WLS
          sumb1zbeta <<- sumb1zbeta + b1zbeta                       # cumulative matrix sum
        }                                                           # end cycle through groups
        b1DXtWX <<- b1Dbetag + sumb1XtWX                            # posterior precision matrix for common beta is bDbetag + sumbXtWX
        b1pbeta <<- b1prbeta + sumb1zbeta                           # sum of prior and the cumulative WLS contributions
        
        b1beta[1,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
        b1CC <<- chol(b1DXtWX)                                      # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
        b1CI <<- inverse(b1CC)                                      # inverse of upper triangular matrix from Cholesky decomposition
        tb1CI <<- t(b1CI)                                           # transpose
        b1pbeta <<- tb1CI %*% b1pbeta                               # rescale
        b1beta <<- b1pbeta + b1beta                                 # center
        b1beta <<- b1CI %*% b1beta                                  # rescale
        
        b1g[gp1] <- b1beta[1,1]
        
        # Group-specific coefficient(s) reflect common value
        for (i in 1:g) {
          b1g[i] <- b1g[gp1]
        }
        
      } else { # (flg == 3)
        
        ###############################################
        # Generate proposal for intercepts-only model #
        ###############################################
        
        for (i in 1:gp1) {
          b1g[i] <- 0
        }
        
      }
    }
    
    # Update model with new node values    
    model[['b1g']] <<- b1g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPb_bmal 

# Custom Gibbs sampler for trend coefficients conditional on the intercepts in the linear BMA model with a level shift
sampler_CPb_xptl_bmal <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Overall dimensionality (p == 3 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 3 | lt != p-2) {
      nimStop("Only node 'b1g' can be sampled, conditionally on 's1ag' and 's2ag', using 'sampler_CPb_xptl_bmal'.")
    } else {
      if (target[1] != 'b1g') {
        nimStop("1st target node must be the vector (length g+1) of linear coefficients 'b1g'.")
      }
    }
    
    # Get other constants from model to use in calculations
    gp1 <- 0L
    g <- model$getConstants()$g
    gp1 <- g+1
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1mbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1Dbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1X <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    
    b1Xt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b1XtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1DXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1prbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1pbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1beta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1CC <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    tb1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and intercepts (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Get current integer-valued model flag (draws are conditional on the model flag)
    flg <- 0L
    flg <- as.integer(model[['flg']])
    if (flg > 3 | flg < 1) {
      nimStop("Expecting integer model flag values between 1 and 3 in 'sampler_CPb_xptl_bmal'.")
    }
    
    # Node values to be replaced
    b1g <- model[['b1g']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    ###########################################################
    # Populate conformal matrices depending on the model flag #
    ###########################################################
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]                                            # intercepts columns
      aX[j,2] <<- X[j,2]
    }

    if (flg == 1 | flg == 2) {                                      # linear
      for (j in 1:n) {
        b1X[j,1] <<- X[j,3]
      }
      b1mbetag[1,1] <<- mbetag[3,1]
      b1Dbetag[1,1] <<- Dbetag[3,3]  
      b1Xt <<- t(b1X)                                               # transpose bX
      b1prbeta <<- b1Dbetag %*% b1mbetag                            # contribution to posterior mean from prior
      
      if (flg == 2) {                                               # initialize cumulative sums in common case
        sumb1XtWX[1,1] <<- 0
        sumb1zbeta[1,1] <<- 0
      }
    }
    
    if (flg == 1) {
      
      ####################################################
      # Generate proposal for group-specific trend model #
      ####################################################
      
      # Update coefficients conditional on intercepts
      for (i in 1:g) {                                              # cycle through each group independently
        abeta[1,1] <<- s1ag[i]                                      # group-specific abeta vector, segment 1
        abeta[2,1] <<- s2ag[i]                                      # group-specific abeta vector, segment 2
        aXbeta <<- aX %*% abeta                                     # vector of intercepts a
        
        VgD <<- nimMatrix(0, n, n)
        for (j in 1:n) {
          Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]               # populate nx1 data vector Zvec = Yvec - a
          VgD[j,j] <<- S2arr[(i-1)*n+j]                             # (Re-)set VgD to diagonal matrix of sampling variances
        }
        irho <- rhoarr[i]
        inu <- nuarr[i]
        Vg <<- ar1_cov_matrix(rts, irho, inu)                       # add AR covariance matrix Vgamma
        Vg <<- VgD + Vg
        Wg <<- inverse(Vg)                                          # Wg = Vg^{-1}
        
        b1XtW <<- b1Xt %*% Wg                                       # multiply Xt and Wg
        b1XtWX <<- b1XtW %*% b1X                                    # calculate XtWX, the precision matrix from WLS
        b1DXtWX <<- b1Dbetag + b1XtWX                               # posterior precision matrix for beta is Dbetag + XtWX
        b1zbeta <<- b1XtW %*% Zvec                                  # contribution to posterior mean from WLS
        b1pbeta <<- b1prbeta + b1zbeta                              # sum of prior and WLS contributions
        
        b1beta[1,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal
        b1CC <<- chol(b1DXtWX)                                      # Cholesky decomposition for precision matrix (R returns upper triangular)
        b1CI <<- inverse(b1CC)                                      # inverse of upper triangular matrix from Cholesky decomposition
        tb1CI <<- t(b1CI)                                           # transpose
        b1pbeta <<- tb1CI %*% b1pbeta                               # rescale
        b1beta <<- b1pbeta + b1beta                                 # center
        b1beta <<- b1CI %*% b1beta                                  # rescale
        
        b1g[i] <- b1beta[1,1]
      }                                                             # end cycle through groups
      
      # Common coefficient is average of group-specific ones
      b1g[gp1] <- 0
      for (i in 1:g) {
        b1g[gp1] <- b1g[gp1] + b1g[i]
      }
      b1g[gp1] <- b1g[gp1]/g
      
    } else { 
      
      if (flg == 2) {
        
        ############################################
        # Generate proposal for common trend model #
        ############################################
        
        # Update common coefficient conditional on intercepts
        for (i in 1:g) {                                            # cycle through each group independently
          abeta[1,1] <<- s1ag[i]                                    # group-specific abeta vector, segment 1
          abeta[2,1] <<- s2ag[i]                                    # group-specific abeta vector, segment 2
          aXbeta <<- aX %*% abeta                                   # vector of intercepts a
          
          VgD <<- nimMatrix(0, n, n)
          for (j in 1:n) {
            Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]             # populate nx1 data vector Zvec = Yvec - a
            VgD[j,j] <<- S2arr[(i-1)*n+j]                           # (Re-)set VgD to diagonal matrix of sampling variances
          }
          irho <- rhoarr[i]
          inu <- nuarr[i]
          Vg <<- ar1_cov_matrix(rts, irho, inu)                     # add AR covariance matrix Vgamma
          Vg <<- VgD + Vg
          Wg <<- inverse(Vg)                                        # Wg = Vg^{-1}
          
          b1XtW <<- b1Xt %*% Wg                                     # multiply bXt and Wg
          b1XtWX <<- b1XtW %*% b1X                                  # calculate bXtWX
          sumb1XtWX <<- sumb1XtWX + b1XtWX                          # cumulative matrix sum
          b1zbeta <<- b1XtW %*% Zvec                                # contributions to posterior mean from WLS
          sumb1zbeta <<- sumb1zbeta + b1zbeta                       # cumulative matrix sum
        }                                                           # end cycle through groups
        b1DXtWX <<- b1Dbetag + sumb1XtWX                            # posterior precision matrix for common beta is bDbetag + sumbXtWX
        b1pbeta <<- b1prbeta + sumb1zbeta                           # sum of prior and the cumulative WLS contributions
        
        b1beta[1,1] <<- rnorm(1, 0, sd = 1)                         # sample from univariate standard normal(s)
        b1CC <<- chol(b1DXtWX)                                      # Cholesky decomposition for (p-1)x(p-1) precision matrix (R returns upper triangular)
        b1CI <<- inverse(b1CC)                                      # inverse of upper triangular matrix from Cholesky decomposition
        tb1CI <<- t(b1CI)                                           # transpose
        b1pbeta <<- tb1CI %*% b1pbeta                               # rescale
        b1beta <<- b1pbeta + b1beta                                 # center
        b1beta <<- b1CI %*% b1beta                                  # rescale
        
        b1g[gp1] <- b1beta[1,1]
        
        # Group-specific coefficient(s) reflect common value
        for (i in 1:g) {
          b1g[i] <- b1g[gp1]
        }
        
      } else { # (flg == 3)
        
        ###############################################
        # Generate proposal for intercepts-only model #
        ###############################################
        
        for (i in 1:gp1) {
          b1g[i] <- 0
        }
        
      }
    }
    
    # Update model with new node values    
    model[['b1g']] <<- b1g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPb_xptl_bmal 

# Custom Gibbs sampler for trend coefficients conditional on the intercepts in the linear-linear BMA model with full trend break
sampler_CPb_xptf_bmal_bmal <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Dimensionality of segment 1 (s1p == 2 here)
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    
    # Overall dimensionality (p == 4 here)
    p <- model$getConstants()$p
    
    # Make sure the 'target' values are correctly specified
    if (p != 4 | lt != p-2) {
      nimStop("Both nodes 's1b1g' and 's2b1g' must be sampled, conditionally on 's1ag' and 's2ag', using 'sampler_CPb_xptf_bmal_bmal'.")
    } else {
      if (target[1] != 's1b1g') {
        nimStop("1st target node must be the vector (length g+1) of linear coefficients 's1b1g'.")
      } else {
        if (target[2] != 's2b1g') {
          nimStop("2nd target node must be the vector (length g+1) of linear coefficients 's2b1g'.")
        }
      }
    }
    
    # Get other constants from model to use in calculations
    gp1 <- 0L
    g <- model$getConstants()$g
    gp1 <- g+1
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    sumb1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b1mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b1Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b1X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    
    b1Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b1XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b1DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b1prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b1pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b1beta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b1CC <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    tb1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    # Get target and dependent nodes
    target <- model$expandNodeNames(target)
    allDependents <- model$getDependencies(target)
    stochasticDependents <- model$getDependencies(target, self = FALSE, stochOnly = TRUE) 
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get current AR(1) parameters and intercepts (draws are conditional on these values)
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    
    # Get current integer-valued model flags (draws are conditional on the model flags)
    s1flg <- 0L
    s1flg <- as.integer(model[['s1flg']])
    if (s1flg > 3 | s1flg < 1) {
      nimStop("Expecting integer model flag values for segment 1 between 1 and 3 in 'sampler_CPb_xptf_bmal_bmal'.")
    }
    s2flg <- 0L
    s2flg <- as.integer(model[['s2flg']])
    if (s2flg > 3 | s2flg < 1) {
      nimStop("Expecting integer model flag values for segment 2 between 1 and 3 in 'sampler_CPb_xptf_bmal_bmal'.")
    }
    
    # Combination flag for internal use
    iflg <- 0L
    iflg <- 10*s1flg + s2flg
    
    # Node values to be replaced
    s1b1g <- model[['s1b1g']]
    s2b1g <- model[['s2b1g']]
    
    # Working variables
    i <- 0L 
    j <- 0L
    k <- 0L
    irho <- 0
    inu <- 0
    
    ###########################################################
    # Populate conformal matrices depending on the model flag #
    ###########################################################
    
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    
    if (iflg == 11 | iflg == 22) {                                # linear-linear
      for (j in 1:n) {
        for (k in 2:s1p) {
          b1X[j,k-1] <<- X[j,k]
        }
        for (k in s1pp2:p) {
          b1X[j,k-2] <<- X[j,k]
        }
      }
      b1Dbetag <<- nimMatrix(0, 2L, 2L)
      for (k in 2:s1p) {
        b1mbetag[k-1,1] <<- mbetag[k,1]
        b1Dbetag[k-1,k-1] <<- Dbetag[k,k]  
      }
      for (k in s1pp2:p) {
        b1mbetag[k-2,1] <<- mbetag[k,1]
        b1Dbetag[k-2,k-2] <<- Dbetag[k,k]  
      }
      b1Xt <<- t(b1X)                                               # transpose bX
      b1prbeta <<- b1Dbetag %*% b1mbetag                            # contribution to posterior mean from prior
      
      if (iflg == 22) {                                             # initialize cumulative sums in common case
        sumb1XtWX <<- nimMatrix(0, 2L, 2L)
        sumb1zbeta <<- nimMatrix(0, 2L, 1L)
      }
    }
    
    if (iflg == 11) {
      
      #####################################################
      # Generate proposal for group-specific trend models #
      #####################################################
      
      # Update coefficients conditional on intercepts
      for (i in 1:g) {                                              # cycle through each group independently
        abeta[1,1] <<- s1ag[i]                                      # group-specific abeta vector, segment 1
        abeta[2,1] <<- s2ag[i]                                      # group-specific abeta vector, segment 2
        aXbeta <<- aX %*% abeta                                     # vector of intercepts a
        
        VgD <<- nimMatrix(0, n, n)
        for (j in 1:n) {
          Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]               # populate nx1 data vector Zvec = Yvec - a
          VgD[j,j] <<- S2arr[(i-1)*n+j]                             # (Re-)set VgD to diagonal matrix of sampling variances
        }
        irho <- rhoarr[i]
        inu <- nuarr[i]
        Vg <<- ar1_cov_matrix(rts, irho, inu)                       # add AR covariance matrix Vgamma
        Vg <<- VgD + Vg
        Wg <<- inverse(Vg)                                          # Wg = Vg^{-1}
        
        b1XtW <<- b1Xt %*% Wg                                       # multiply Xt and Wg
        b1XtWX <<- b1XtW %*% b1X                                    # calculate XtWX, the precision matrix from WLS
        b1DXtWX <<- b1Dbetag + b1XtWX                               # posterior precision matrix for beta is Dbetag + XtWX
        b1zbeta <<- b1XtW %*% Zvec                                  # contribution to posterior mean from WLS
        b1pbeta <<- b1prbeta + b1zbeta                              # sum of prior and WLS contributions
        
        for (k in 1:2) {
          b1beta[k,1] <<- rnorm(1, 0, sd = 1)                       # sample from univariate standard normal
        }
        b1CC <<- chol(b1DXtWX)                                      # Cholesky decomposition for precision matrix (R returns upper triangular)
        b1CI <<- inverse(b1CC)                                      # inverse of upper triangular matrix from Cholesky decomposition
        tb1CI <<- t(b1CI)                                           # transpose
        b1pbeta <<- tb1CI %*% b1pbeta                               # rescale
        b1beta <<- b1pbeta + b1beta                                 # center
        b1beta <<- b1CI %*% b1beta                                  # rescale
        
        s1b1g[i] <- b1beta[1,1]
        s2b1g[i] <- b1beta[2,1]
          
      }                                                             # end cycle through groups
      
      # Common coefficients are averages of group-specific ones
      s1b1g[gp1] <- 0
      s2b1g[gp1] <- 0
      for (i in 1:g) {
        s1b1g[gp1] <- s1b1g[gp1] + s1b1g[i]
        s2b1g[gp1] <- s2b1g[gp1] + s2b1g[i]
      }
      s1b1g[gp1] <- s1b1g[gp1]/g
      s2b1g[gp1] <- s2b1g[gp1]/g
      
    } else { 
      
      if (iflg == 22) {
        
        #############################################
        # Generate proposal for common trend models #
        #############################################
        
        # Update common coefficient(s) conditional on intercepts
        for (i in 1:g) {                                            # cycle through each group independently
          abeta[1,1] <<- s1ag[i]                                    # group-specific abeta vector, segment 1
          abeta[2,1] <<- s2ag[i]                                    # group-specific abeta vector, segment 2
          aXbeta <<- aX %*% abeta                                   # vector of intercepts a
          
          VgD <<- nimMatrix(0, n, n)
          for (j in 1:n) {
            Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]             # populate nx1 data vector Zvec = Yvec - a
            VgD[j,j] <<- S2arr[(i-1)*n+j]                           # (Re-)set VgD to diagonal matrix of sampling variances
          }
          irho <- rhoarr[i]
          inu <- nuarr[i]
          Vg <<- ar1_cov_matrix(rts, irho, inu)                     # add AR covariance matrix Vgamma
          Vg <<- VgD + Vg
          Wg <<- inverse(Vg)                                        # Wg = Vg^{-1}
          
          b1XtW <<- b1Xt %*% Wg                                     # multiply bXt and Wg
          b1XtWX <<- b1XtW %*% b1X                                  # calculate bXtWX
          sumb1XtWX <<- sumb1XtWX + b1XtWX                          # cumulative matrix sum
          b1zbeta <<- b1XtW %*% Zvec                                # contributions to posterior mean from WLS
          sumb1zbeta <<- sumb1zbeta + b1zbeta                       # cumulative matrix sum
        }                                                           # end cycle through groups
        
        b1DXtWX <<- b1Dbetag + sumb1XtWX                            # posterior precision matrix for common beta is bDbetag + sumbXtWX
        b1pbeta <<- b1prbeta + sumb1zbeta                           # sum of prior and the cumulative WLS contributions
        
        for (k in 1:2) {
          b1beta[k,1] <<- rnorm(1, 0, sd = 1)                       # sample from univariate standard normal(s)
        }
        b1CC <<- chol(b1DXtWX)                                      # Cholesky decomposition for (p-2)x(p-2) precision matrix (R returns upper triangular)
        b1CI <<- inverse(b1CC)                                      # inverse of upper triangular matrix from Cholesky decomposition
        tb1CI <<- t(b1CI)                                           # transpose
        b1pbeta <<- tb1CI %*% b1pbeta                               # rescale
        b1beta <<- b1pbeta + b1beta                                 # center
        b1beta <<- b1CI %*% b1beta                                  # rescale
        
        s1b1g[gp1] <- b1beta[1,1]
        s2b1g[gp1] <- b1beta[2,1]
        
        # Group-specific coefficient(s) reflect common value(s)
        for (i in 1:g) {
          s1b1g[i] <- s1b1g[gp1]
          s2b1g[i] <- s2b1g[gp1]
        }
        
      } else { # (iflg == 33)
        
        ###############################################
        # Generate proposal for intercepts-only model #
        ###############################################
        
        for (i in 1:gp1) {
          s1b1g[i] <- 0
          s2b1g[i] <- 0
        }
        
      }
    }
    
    # Update model with new node values    
    model[['s1b1g']] <<- s1b1g
    model[['s2b1g']] <<- s2b1g
    
    # Re-calculate all deterministic dependents as well as log-probabilities for stochastic nodes 
    model$calculate(allDependents)
    
    # Copy from model to mvSaved objects
    nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
    nimCopy(from = model, to = mvSaved, row = 1, nodes = stochasticDependents, logProbOnly = TRUE)
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_CPb_xptf_bmal_bmal

#####################################################################################################
# eMKF: Custom Gibbs sampler for model flag, conditional on intercepts, in the supported BMA models #
#####################################################################################################

# Custom Gibbs sampler for model flag, conditional on intercepts, in the cubic BMA model
sampler_FP_bmac <- nimbleFunction(

  contains = sampler_BASE,

  setup = function(model, mvSaved, target, control) {

    # Length of target
    lt <- 0L
    lt <- length(target)

    # Make sure the 'target' value is correctly specified
    if (lt != 1) {
      nimStop("Only the model flag 'flg' can be sampled using 'sampler_FP_bmac'.")
    } else {
      if (target[1] != 'flg') {
        nimStop("Target node must be the model flag (integer) 'flg'.")
      }
    }

    # Get constants from model to use in calculations
    p <- model$getConstants()$p
    if (p != 4) {
      nimStop("Expecting design matrix to have 4 columns in 'sampler_FP_bmac'.")
    }
    fp <- model$getConstants()$fp
    if (fp != 7) {
      nimStop("Expecting a total of 7 possible trend models in 'sampler_FP_bmac'.")
    }
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts

    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################

    pwts <- nimNumeric(fp, init = FALSE)
    qwts <- nimNumeric(fp, init = FALSE)

    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)

    aX <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)

    sumb3XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    sumb2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)

    sumb3zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    sumb2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)

    b3mbetag <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)

    b3tmbetag <- nimMatrix(nrow=1L, ncol=3L, init = FALSE)
    b2tmbetag <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    b1tmbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)

    b3Dbetag <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)

    b3X <- nimMatrix(nrow=n, ncol=3L, init = FALSE)
    b2X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=1L, init = FALSE)

    b3Xt <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b2Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)

    b3XtW <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b2XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)

    b3XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)

    b3DXtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)

    b3prbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)

    b3pbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)

    b3tpbeta <- nimMatrix(nrow=1L, ncol=3L, init = FALSE)
    b2tpbeta <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    b1tpbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)

    b3zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)

    b3CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)

    b3exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b2exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b1exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)

    b3wts <- 0
    b2wts <- 0
    b1wts <- 0
    
    # Confirm target is a single scalar
    target <- model$expandNodeNames(target, returnScalarComponents = TRUE)
    if (length(target) > 1) {
      nimStop("Expecting a single model flag as target in 'sampler_FP_bmac'.")
    }

  },

  run = function() {

    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]

    # Get prior model weights, intercepts, and current AR(1) parameters (draws are conditional on these values)
    wts <- model[['wts']]
    ag <- model[['ag']]
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]

    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    l <- 0L
    m <- 0L
    irho <- 0
    inu <- 0
    qwtsum <- 0
    flg <- 0L

    ############################################################
    # Required conformal matrices for each possible model flag #
    ############################################################

    # intercepts
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
    }

    # cubic with no intercept
    for (j in 1:n) {
      for (k in 2:4) {
        b3X[j,k-1] <<- X[j,k]
      }
    }
    b3Dbetag <<- nimMatrix(0, 3L, 3L)
    for (k in 2:4) {
      b3mbetag[k-1,1] <<- mbetag[k,1]
      b3Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    b3Xt <<- t(b3X)                                                 # transpose bX to use below
    b3prbeta <<- b3Dbetag %*% b3mbetag                              # contribution to posterior mean from prior
    
    sumb3XtWX <<- nimMatrix(0, 3L, 3L)                              # initialize cumulative sums to use in common case
    sumb3zbeta <<- nimMatrix(0, 3L, 1L)

    # quad with no intercept
    for (j in 1:n) {
      for (k in 2:3) {
        b2X[j,k-1] <<- X[j,k]
      }
    }
    b2Dbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 2:3) {
      b2mbetag[k-1,1] <<- mbetag[k,1]
      b2Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    b2Xt <<- t(b2X)                                                 # transpose bX to use below
    b2prbeta <<- b2Dbetag %*% b2mbetag                              # contribution to posterior mean from prior
    
    sumb2XtWX <<- nimMatrix(0, 2L, 2L)                              # initialize cumulative sums to use in common case
    sumb2zbeta <<- nimMatrix(0, 2L, 1L)

    # linear with no intercept
    for (j in 1:n) {
      b1X[j,1] <<- X[j,2]
    }
    b1mbetag[1,1] <<- mbetag[2,1]
    b1Dbetag[1,1] <<- Dbetag[2,2]
    b1Xt <<- t(b1X)                                                 # transpose bX to use below
    b1prbeta <<- b1Dbetag %*% b1mbetag                              # contribution to posterior mean from prior
    
    sumb1XtWX[1,1] <<- 0                                            # initialize cumulative sums to use in common case
    sumb1zbeta[1,1] <<- 0

    ################################################################################
    # Model weights (log-scale) for group-specific trend models with no intercepts #
    ################################################################################

    # prior weight for intercepts-only model
    pwts[7] <<- 0                                                   # remains zero

    # prior weight for group-specific cubic trend model
    pwts[1] <<- 0
    b3tmbetag <<- t(b3mbetag)
    b3exp <<- b3tmbetag %*% b3prbeta                                # exponent from normal pdf features g times
    pwts[1] <<- pwts[1] - 0.5*g*b3exp[1,1]                          # log scale
    b3wts <<- logdet(b3Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[1] <<- pwts[1] + 0.5*g*b3wts                               # already on log scale

    # prior weight for group-specific quad trend model
    pwts[2] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf features g times
    pwts[2] <<- pwts[2] - 0.5*g*b2exp[1,1]                          # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[2] <<- pwts[2] + 0.5*g*b2wts                               # already on log scale

    # prior weight for group-specific linear trend model
    pwts[3] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf features g times
    pwts[3] <<- pwts[3] - 0.5*g*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[3] <<- pwts[3] + 0.5*g*b1wts                               # already on log scale

    # posterior weights from group-specific contributions
    for (i in 1:g) {                                                # cycle through each group independently

      abeta[1,1] <<- ag[i]                                          # group-specific abeta vector
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}

      # group-specific cubic trend model with no intercept
      b3XtW <<- b3Xt %*% Wg                                         # multiply Xt and Wg
      b3XtWX <<- b3XtW %*% b3X                                      # calculate XtWX, the precision matrix from WLS
      b3DXtWX <<- b3Dbetag + b3XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b3zbeta <<- b3XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b3pbeta <<- b3prbeta + b3zbeta                                # sum of prior and WLS contributions
      b3tpbeta <<- t(b3pbeta)                                       # transpose
      b3CI <<- inverse(b3DXtWX)                                     # inverse of posterior precision matrix
      b3pbeta <<- b3CI %*% b3pbeta                                  # re-scale psbeta
      b3exp <<- b3tpbeta %*% b3pbeta                                # exponent from normal pdf
      pwts[1] <<- pwts[1] + 0.5*b3exp[1,1]                          # log scale
      b3wts <<- logdet(b3DXtWX)                                     # log-determinant for normalizing constant
      pwts[1] <<- pwts[1] - 0.5*b3wts                               # already on log scale

      # group-specific quad trend model with no intercept
      b2XtW <<- b2Xt %*% Wg                                         # multiply Xt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate XtWX, the precision matrix from WLS
      b2DXtWX <<- b2Dbetag + b2XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b2zbeta <<- b2XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b2pbeta <<- b2prbeta + b2zbeta                                # sum of prior and WLS contributions
      b2tpbeta <<- t(b2pbeta)                                       # transpose
      b2CI <<- inverse(b2DXtWX)                                     # inverse of posterior precision matrix
      b2pbeta <<- b2CI %*% b2pbeta                                  # re-scale psbeta
      b2exp <<- b2tpbeta %*% b2pbeta                                # exponent from normal pdf
      pwts[2] <<- pwts[2] + 0.5*b2exp[1,1]                          # log scale
      b2wts <<- logdet(b2DXtWX)                                     # log-determinant for normalizing constant
      pwts[2] <<- pwts[2] - 0.5*b2wts                               # already on log scale

      # group-specific linear trend model with no intercept
      b1XtW <<- b1Xt %*% Wg                                         # multiply Xt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate XtWX, the precision matrix from WLS
      b1DXtWX <<- b1Dbetag + b1XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b1zbeta <<- b1XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b1pbeta <<- b1prbeta + b1zbeta                                # sum of prior and WLS contributions
      b1tpbeta <<- t(b1pbeta)                                       # transpose
      b1CI <<- inverse(b1DXtWX)                                     # inverse of posterior precision matrix
      b1pbeta <<- b1CI %*% b1pbeta                                  # re-scale psbeta
      b1exp <<- b1tpbeta %*% b1pbeta                                # exponent from normal pdf
      pwts[3] <<- pwts[3] + 0.5*b1exp[1,1]                          # log scale
      b1wts <<- logdet(b1DXtWX)                                     # log-determinant for normalizing constant
      pwts[3] <<- pwts[3] - 0.5*b1wts                               # already on log scale

    }                                                               # end cycle through groups

    ############################################################
    # Model weights for common trend models with no intercepts #
    ############################################################

    # prior weight for common cubic trend model
    pwts[4] <<- 0
    b3tmbetag <<- t(b3mbetag)
    b3exp <<- b3tmbetag %*% b3prbeta                                # exponent from normal pdf
    pwts[4] <<- pwts[4] - 0.5*b3exp[1,1]                            # log scale
    b3wts <<- logdet(b3Dbetag)                                      # log-determinant of prior precision matrix
    pwts[4] <<- pwts[4] + 0.5*b3wts                                 # already on log scale

    # prior weight for common quad trend model
    pwts[5] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf
    pwts[5] <<- pwts[5] - 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix
    pwts[5] <<- pwts[5] + 0.5*b2wts                                 # already on log scale

    # prior weight for common linear trend model
    pwts[6] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf
    pwts[6] <<- pwts[6] - 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix
    pwts[6] <<- pwts[6] + 0.5*b1wts                                 # already on log scale

    # cumulative posterior contributions over groups
    for (i in 1:g) {                                                # cycle through each group independently

      abeta[1,1] <<- ag[i]                                          # group-specific abeta vector
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}

      # common cubic trend model with no intercepts
      b3XtW <<- b3Xt %*% Wg                                         # multiply bXt and Wg
      b3XtWX <<- b3XtW %*% b3X                                      # calculate bXtWX
      sumb3XtWX <<- sumb3XtWX + b3XtWX                              # cumulative matrix sum
      b3zbeta <<- b3XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb3zbeta <<- sumb3zbeta + b3zbeta                           # cumulative matrix sum

      # common quad trend model with no intercepts
      b2XtW <<- b2Xt %*% Wg                                         # multiply bXt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate bXtWX
      sumb2XtWX <<- sumb2XtWX + b2XtWX                              # cumulative matrix sum
      b2zbeta <<- b2XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb2zbeta <<- sumb2zbeta + b2zbeta                           # cumulative matrix sum

      # common linear trend model with no intercepts
      b1XtW <<- b1Xt %*% Wg                                         # multiply bXt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate bXtWX
      sumb1XtWX <<- sumb1XtWX + b1XtWX                              # cumulative matrix sum
      b1zbeta <<- b1XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb1zbeta <<- sumb1zbeta + b1zbeta                           # cumulative matrix sum

    }                                                               # end cycle through groups

    # posterior weight for common cubic trend with no intercepts
    b3DXtWX <<- b3Dbetag + sumb3XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b3pbeta <<- b3prbeta + sumb3zbeta                               # sum of prior and the cumulative WLS contributions
    b3tpbeta <<- t(b3pbeta)                                         # transpose
    b3CI <<- inverse(b3DXtWX)                                       # inverse of posterior precision matrix
    b3pbeta <<- b3CI %*% b3pbeta                                    # re-scale psbeta
    b3exp <<- b3tpbeta %*% b3pbeta                                  # exponent from normal pdf
    pwts[4] <<- pwts[4] + 0.5*b3exp[1,1]                            # log scale
    b3wts <<- logdet(b3DXtWX)                                       # log-determinant for normalizing constant
    pwts[4] <<- pwts[4] - 0.5*b3wts                                 # already on log-scale

    # posterior weight for common quad trend with no intercepts
    b2DXtWX <<- b2Dbetag + sumb2XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b2pbeta <<- b2prbeta + sumb2zbeta                               # sum of prior and the cumulative WLS contributions
    b2tpbeta <<- t(b2pbeta)                                         # transpose
    b2CI <<- inverse(b2DXtWX)                                       # inverse of posterior precision matrix
    b2pbeta <<- b2CI %*% b2pbeta                                    # re-scale psbeta
    b2exp <<- b2tpbeta %*% b2pbeta                                  # exponent from normal pdf
    pwts[5] <<- pwts[5] + 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2DXtWX)                                       # log-determinant for normalizing constant
    pwts[5] <<- pwts[5] - 0.5*b2wts                                 # already on log-scale

    # posterior weight for common linear trend with no intercepts
    b1DXtWX <<- b1Dbetag + sumb1XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b1pbeta <<- b1prbeta + sumb1zbeta                               # sum of prior and the cumulative WLS contributions
    b1tpbeta <<- t(b1pbeta)                                         # transpose
    b1CI <<- inverse(b1DXtWX)                                       # inverse of posterior precision matrix
    b1pbeta <<- b1CI %*% b1pbeta                                    # re-scale psbeta
    b1exp <<- b1tpbeta %*% b1pbeta                                  # exponent from normal pdf
    pwts[6] <<- pwts[6] + 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1DXtWX)                                       # log-determinant for normalizing constant
    pwts[6] <<- pwts[6] - 0.5*b1wts                                 # already on log-scale

    #############################################
    # Rescaled posterior model weights and draw #
    #############################################

    # calculated using differences on log-scale (a.k.a., Bayes factors) for numerical stability
    for (m in 1:fp) {
      qwtsum <- 0
      for (l in 1:fp) {
        qwtsum <- qwtsum + (wts[l]/wts[m])*exp(pwts[l]-pwts[m])
      }
      qwts[m] <<- 1/qwtsum
    }

    # check for any missing values
    qwtsum <- NaN
    if (any_na(qwts)) {                                             # set qwtsum to zero if any missings
      qwtsum <- 0
    }
    if (!is.na(qwtsum)) {                                           # if there were missing values
      for (m in 1:fp) {
        if (is.na(qwts[m])) {
          qwts[m] <<- 0                                             # replace them with zeroes
        }
        qwtsum <- qwtsum + qwts[m]
      }
      if (qwtsum > 0) {                                             # rescale to sum to 1
        for (m in 1:fp) {
          qwts[m] <<- qwts[m]/qwtsum
        }
      } else {                                                      # (qwtsum == 0) all weights are zero: use prior weights
        for (m in 1:fp) {
          qwts[m] <<- wts[m]
        }
      }
    }

    # sample from discrete posterior distribution of model flags (conditional on intercepts)
    flg <- as.integer(rcat(1, qwts))
    
    if (flg != model[['flg']]) {
      
      # Update model with new flag value
      model[['flg']] <<- flg
      
      # Re-calculate prior probabilities (in case they were not uniform)
      model$calculate(target)
      
      # Copy from model to mvSaved objects
      nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)

    }

  },

  methods = list(reset = function() {})

) # sampler_FP_bmac (sampler_CPb_bmac will update trend coefficients according to new 'flg' value)

# Custom Gibbs sampler for model flag, conditional on intercepts, in the cubic BMA model with a level shift
sampler_FP_xptl_bmac <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Make sure the 'target' value is correctly specified
    if (lt != 1) {
      nimStop("Only the model flag 'flg' can be sampled using 'sampler_FP_xptl_bmac'.")
    } else {
      if (target[1] != 'flg') {
        nimStop("Target node must be the model flag (integer) 'flg'.")
      }
    }
    
    # Get constants from model to use in calculations
    p <- model$getConstants()$p
    if (p != 5) {
      nimStop("Expecting design matrix to have 5 columns in 'sampler_FP_xptl_bmac'.")
    }
    fp <- model$getConstants()$fp
    if (fp != 7) {
      nimStop("Expecting a total of 7 possible trend models in 'sampler_FP_xptl_bmac'.")
    }
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    pwts <- nimNumeric(fp, init = FALSE)
    qwts <- nimNumeric(fp, init = FALSE)
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb3XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    sumb2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb3zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    sumb2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3mbetag <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3tmbetag <- nimMatrix(nrow=1L, ncol=3L, init = FALSE)
    b2tmbetag <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    b1tmbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3Dbetag <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3X <- nimMatrix(nrow=n, ncol=3L, init = FALSE)
    b2X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    
    b3Xt <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b2Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b3XtW <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b2XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b3XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3DXtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3prbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3pbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3tpbeta <- nimMatrix(nrow=1L, ncol=3L, init = FALSE)
    b2tpbeta <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    b1tpbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b2CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b2exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b1exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3wts <- 0
    b2wts <- 0
    b1wts <- 0
    
    # Confirm target is a single scalar
    target <- model$expandNodeNames(target, returnScalarComponents = TRUE)
    if (length(target) > 1) {
      nimStop("Expecting a single model flag as target in 'sampler_FP_xptl_bmac'.")
    }
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get prior model weights, intercepts, and current AR(1) parameters (draws are conditional on these values)
    wts <- model[['wts']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    l <- 0L
    m <- 0L
    irho <- 0
    inu <- 0
    qwtsum <- 0
    flg <- 0L
    
    ############################################################
    # Required conformal matrices for each possible model flag #
    ############################################################
    
    # intercepts
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,2]
    }
    
    # cubic with no intercept
    for (j in 1:n) {
      for (k in 3:5) {
        b3X[j,k-2] <<- X[j,k]
      }
    }
    b3Dbetag <<- nimMatrix(0, 3L, 3L)
    for (k in 3:5) {
      b3mbetag[k-2,1] <<- mbetag[k,1]
      b3Dbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    b3Xt <<- t(b3X)                                                 # transpose bX to use below
    b3prbeta <<- b3Dbetag %*% b3mbetag                              # contribution to posterior mean from prior
    
    sumb3XtWX <<- nimMatrix(0, 3L, 3L)                              # initialize cumulative sums to use in common case
    sumb3zbeta <<- nimMatrix(0, 3L, 1L)
    
    # quad with no intercept
    for (j in 1:n) {
      for (k in 3:4) {
        b2X[j,k-2] <<- X[j,k]
      }
    }
    b2Dbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 3:4) {
      b2mbetag[k-2,1] <<- mbetag[k,1]
      b2Dbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    b2Xt <<- t(b2X)                                                 # transpose bX to use below
    b2prbeta <<- b2Dbetag %*% b2mbetag                              # contribution to posterior mean from prior
    
    sumb2XtWX <<- nimMatrix(0, 2L, 2L)                              # initialize cumulative sums to use in common case
    sumb2zbeta <<- nimMatrix(0, 2L, 1L)
    
    # linear with no intercept
    for (j in 1:n) {
      b1X[j,1] <<- X[j,3]
    }
    b1mbetag[1,1] <<- mbetag[3,1]
    b1Dbetag[1,1] <<- Dbetag[3,3]
    b1Xt <<- t(b1X)                                                 # transpose bX to use below
    b1prbeta <<- b1Dbetag %*% b1mbetag                              # contribution to posterior mean from prior
    
    sumb1XtWX[1,1] <<- 0                                            # initialize cumulative sums to use in common case
    sumb1zbeta[1,1] <<- 0
    
    ################################################################################
    # Model weights (log-scale) for group-specific trend models with no intercepts #
    ################################################################################
    
    # prior weight for intercepts-only model
    pwts[7] <<- 0                                                   # remains zero
    
    # prior weight for group-specific cubic trend model
    pwts[1] <<- 0
    b3tmbetag <<- t(b3mbetag)
    b3exp <<- b3tmbetag %*% b3prbeta                                # exponent from normal pdf features g times
    pwts[1] <<- pwts[1] - 0.5*g*b3exp[1,1]                          # log scale
    b3wts <<- logdet(b3Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[1] <<- pwts[1] + 0.5*g*b3wts                               # already on log scale
    
    # prior weight for group-specific quad trend model
    pwts[2] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf features g times
    pwts[2] <<- pwts[2] - 0.5*g*b2exp[1,1]                          # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[2] <<- pwts[2] + 0.5*g*b2wts                               # already on log scale
    
    # prior weight for group-specific linear trend model
    pwts[3] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf features g times
    pwts[3] <<- pwts[3] - 0.5*g*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[3] <<- pwts[3] + 0.5*g*b1wts                               # already on log scale
    
    # posterior weights from group-specific contributions
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2      
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # group-specific cubic trend model with no intercept
      b3XtW <<- b3Xt %*% Wg                                         # multiply Xt and Wg
      b3XtWX <<- b3XtW %*% b3X                                      # calculate XtWX, the precision matrix from WLS
      b3DXtWX <<- b3Dbetag + b3XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b3zbeta <<- b3XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b3pbeta <<- b3prbeta + b3zbeta                                # sum of prior and WLS contributions
      b3tpbeta <<- t(b3pbeta)                                       # transpose
      b3CI <<- inverse(b3DXtWX)                                     # inverse of posterior precision matrix
      b3pbeta <<- b3CI %*% b3pbeta                                  # re-scale psbeta
      b3exp <<- b3tpbeta %*% b3pbeta                                # exponent from normal pdf
      pwts[1] <<- pwts[1] + 0.5*b3exp[1,1]                          # log scale
      b3wts <<- logdet(b3DXtWX)                                     # log-determinant for normalizing constant
      pwts[1] <<- pwts[1] - 0.5*b3wts                               # already on log scale
      
      # group-specific quad trend model with no intercept
      b2XtW <<- b2Xt %*% Wg                                         # multiply Xt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate XtWX, the precision matrix from WLS
      b2DXtWX <<- b2Dbetag + b2XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b2zbeta <<- b2XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b2pbeta <<- b2prbeta + b2zbeta                                # sum of prior and WLS contributions
      b2tpbeta <<- t(b2pbeta)                                       # transpose
      b2CI <<- inverse(b2DXtWX)                                     # inverse of posterior precision matrix
      b2pbeta <<- b2CI %*% b2pbeta                                  # re-scale psbeta
      b2exp <<- b2tpbeta %*% b2pbeta                                # exponent from normal pdf
      pwts[2] <<- pwts[2] + 0.5*b2exp[1,1]                          # log scale
      b2wts <<- logdet(b2DXtWX)                                     # log-determinant for normalizing constant
      pwts[2] <<- pwts[2] - 0.5*b2wts                               # already on log scale
      
      # group-specific linear trend model with no intercept
      b1XtW <<- b1Xt %*% Wg                                         # multiply Xt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate XtWX, the precision matrix from WLS
      b1DXtWX <<- b1Dbetag + b1XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b1zbeta <<- b1XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b1pbeta <<- b1prbeta + b1zbeta                                # sum of prior and WLS contributions
      b1tpbeta <<- t(b1pbeta)                                       # transpose
      b1CI <<- inverse(b1DXtWX)                                     # inverse of posterior precision matrix
      b1pbeta <<- b1CI %*% b1pbeta                                  # re-scale psbeta
      b1exp <<- b1tpbeta %*% b1pbeta                                # exponent from normal pdf
      pwts[3] <<- pwts[3] + 0.5*b1exp[1,1]                          # log scale
      b1wts <<- logdet(b1DXtWX)                                     # log-determinant for normalizing constant
      pwts[3] <<- pwts[3] - 0.5*b1wts                               # already on log scale
      
    }                                                               # end cycle through groups
    
    ############################################################
    # Model weights for common trend models with no intercepts #
    ############################################################
    
    # prior weight for common cubic trend model
    pwts[4] <<- 0
    b3tmbetag <<- t(b3mbetag)
    b3exp <<- b3tmbetag %*% b3prbeta                                # exponent from normal pdf
    pwts[4] <<- pwts[4] - 0.5*b3exp[1,1]                            # log scale
    b3wts <<- logdet(b3Dbetag)                                      # log-determinant of prior precision matrix
    pwts[4] <<- pwts[4] + 0.5*b3wts                                 # already on log scale
    
    # prior weight for common quad trend model
    pwts[5] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf
    pwts[5] <<- pwts[5] - 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix
    pwts[5] <<- pwts[5] + 0.5*b2wts                                 # already on log scale
    
    # prior weight for common linear trend model
    pwts[6] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf
    pwts[6] <<- pwts[6] - 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix
    pwts[6] <<- pwts[6] + 0.5*b1wts                                 # already on log scale
    
    # cumulative posterior contributions over groups
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2      
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # common cubic trend model with no intercepts
      b3XtW <<- b3Xt %*% Wg                                         # multiply bXt and Wg
      b3XtWX <<- b3XtW %*% b3X                                      # calculate bXtWX
      sumb3XtWX <<- sumb3XtWX + b3XtWX                              # cumulative matrix sum
      b3zbeta <<- b3XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb3zbeta <<- sumb3zbeta + b3zbeta                           # cumulative matrix sum
      
      # common quad trend model with no intercepts
      b2XtW <<- b2Xt %*% Wg                                         # multiply bXt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate bXtWX
      sumb2XtWX <<- sumb2XtWX + b2XtWX                              # cumulative matrix sum
      b2zbeta <<- b2XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb2zbeta <<- sumb2zbeta + b2zbeta                           # cumulative matrix sum
      
      # common linear trend model with no intercepts
      b1XtW <<- b1Xt %*% Wg                                         # multiply bXt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate bXtWX
      sumb1XtWX <<- sumb1XtWX + b1XtWX                              # cumulative matrix sum
      b1zbeta <<- b1XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb1zbeta <<- sumb1zbeta + b1zbeta                           # cumulative matrix sum
      
    }                                                               # end cycle through groups
    
    # posterior weight for common cubic trend with no intercepts
    b3DXtWX <<- b3Dbetag + sumb3XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b3pbeta <<- b3prbeta + sumb3zbeta                               # sum of prior and the cumulative WLS contributions
    b3tpbeta <<- t(b3pbeta)                                         # transpose
    b3CI <<- inverse(b3DXtWX)                                       # inverse of posterior precision matrix
    b3pbeta <<- b3CI %*% b3pbeta                                    # re-scale psbeta
    b3exp <<- b3tpbeta %*% b3pbeta                                  # exponent from normal pdf
    pwts[4] <<- pwts[4] + 0.5*b3exp[1,1]                            # log scale
    b3wts <<- logdet(b3DXtWX)                                       # log-determinant for normalizing constant
    pwts[4] <<- pwts[4] - 0.5*b3wts                                 # already on log-scale
    
    # posterior weight for common quad trend with no intercepts
    b2DXtWX <<- b2Dbetag + sumb2XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b2pbeta <<- b2prbeta + sumb2zbeta                               # sum of prior and the cumulative WLS contributions
    b2tpbeta <<- t(b2pbeta)                                         # transpose
    b2CI <<- inverse(b2DXtWX)                                       # inverse of posterior precision matrix
    b2pbeta <<- b2CI %*% b2pbeta                                    # re-scale psbeta
    b2exp <<- b2tpbeta %*% b2pbeta                                  # exponent from normal pdf
    pwts[5] <<- pwts[5] + 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2DXtWX)                                       # log-determinant for normalizing constant
    pwts[5] <<- pwts[5] - 0.5*b2wts                                 # already on log-scale
    
    # posterior weight for common linear trend with no intercepts
    b1DXtWX <<- b1Dbetag + sumb1XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b1pbeta <<- b1prbeta + sumb1zbeta                               # sum of prior and the cumulative WLS contributions
    b1tpbeta <<- t(b1pbeta)                                         # transpose
    b1CI <<- inverse(b1DXtWX)                                       # inverse of posterior precision matrix
    b1pbeta <<- b1CI %*% b1pbeta                                    # re-scale psbeta
    b1exp <<- b1tpbeta %*% b1pbeta                                  # exponent from normal pdf
    pwts[6] <<- pwts[6] + 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1DXtWX)                                       # log-determinant for normalizing constant
    pwts[6] <<- pwts[6] - 0.5*b1wts                                 # already on log-scale
    
    #############################################
    # Rescaled posterior model weights and draw #
    #############################################
    
    # calculated using differences on log-scale (a.k.a., Bayes factors) for numerical stability
    for (m in 1:fp) {
      qwtsum <- 0
      for (l in 1:fp) {
        qwtsum <- qwtsum + (wts[l]/wts[m])*exp(pwts[l]-pwts[m])
      }
      qwts[m] <<- 1/qwtsum
    }
    
    # check for any missing values
    qwtsum <- NaN
    if (any_na(qwts)) {                                             # set qwtsum to zero if any missings
      qwtsum <- 0
    }
    if (!is.na(qwtsum)) {                                           # if there were missing values
      for (m in 1:fp) {
        if (is.na(qwts[m])) {
          qwts[m] <<- 0                                             # replace them with zeroes
        }
        qwtsum <- qwtsum + qwts[m]
      }
      if (qwtsum > 0) {                                             # rescale to sum to 1
        for (m in 1:fp) {
          qwts[m] <<- qwts[m]/qwtsum
        }
      } else {                                                      # (qwtsum == 0) all weights are zero: use prior weights
        for (m in 1:fp) {
          qwts[m] <<- wts[m]
        }
      }
    }
    
    # sample from discrete posterior distribution of model flags (conditional on intercepts)
    flg <- as.integer(rcat(1, qwts))
    
    if (flg != model[['flg']]) {
      
      # Update model with new flag value
      model[['flg']] <<- flg
      
      # Re-calculate prior probabilities (in case they were not uniform)
      model$calculate(target)
      
      # Copy from model to mvSaved objects
      nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
      
    }
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_FP_xptl_bmac (sampler_CPb_xptl_bmac will update trend coefficients according to new 'flg' value)

# Custom Gibbs sampler for model flags, conditional on intercepts, in the cubic-cubic BMA model with a full trend break
sampler_FP_xptf_bmac_bmac <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Make sure the 'target' value is correctly specified
    if (lt != 1) {
      nimStop("Only the model flag 'flg' can be sampled using 'sampler_FP_xptf_bmac_bmac'. Segment-specific flags 's1flg' and 's2flg' are derived deterministically from 'flg'.")
    } else {
      if (target[1] != 'flg') {
        nimStop("Target node must be the model flag (integer) 'flg'.")
      }
    }
    
    # Get constants from model to use in calculations
    s1pp2 <- 0L
    s1pm2 <- 0L
    s1pm1 <- 0L
    s1p <- model$getConstants()$s1p
    s1pm1 <- s1p - 1
    s1pm2 <- s1p - 2
    s1pp2 <- s1p + 2
    pm2 <- 0L
    pm1 <- 0L
    p <- model$getConstants()$p
    pm1 <- p-1
    pm2 <- p-2
    if (p != 8) {
      nimStop("Expecting design matrix to have 8 columns in 'sampler_FP_xptf_bmac_bmac'.")
    }
    fp <- model$getConstants()$fp
    if (fp != 13) {
      nimStop("Expecting a total of 13 possible trend models in 'sampler_FP_xptf_bmac_bmac'.")
    }
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    pwts <- nimNumeric(fp, init = FALSE)
    qwts <- nimNumeric(fp, init = FALSE)
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb6XtWX <- nimMatrix(nrow=6L, ncol=6L, init = FALSE)
    sumb5XtWX <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    sumb4XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    sumb3XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    sumb2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    sumb6zbeta <- nimMatrix(nrow=6L, ncol=1L, init = FALSE)
    sumb5zbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    sumb4zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    sumb3zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    sumb2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b6mbetag <- nimMatrix(nrow=6L, ncol=1L, init = FALSE)
    b5mbetag <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4mbetag <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3mbetag <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2mbetag <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b6tmbetag <- nimMatrix(nrow=1L, ncol=6L, init = FALSE)
    b5tmbetag <- nimMatrix(nrow=1L, ncol=5L, init = FALSE)
    b4tmbetag <- nimMatrix(nrow=1L, ncol=4L, init = FALSE)
    b3tmbetag <- nimMatrix(nrow=1L, ncol=4L, init = FALSE)
    b2tmbetag <- nimMatrix(nrow=1L, ncol=3L, init = FALSE)
    b1tmbetag <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    
    b6Dbetag <- nimMatrix(nrow=6L, ncol=6L, init = FALSE)
    b5Dbetag <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4Dbetag <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3Dbetag <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2Dbetag <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b6X <- nimMatrix(nrow=n, ncol=6L, init = FALSE)
    b5X <- nimMatrix(nrow=n, ncol=5L, init = FALSE)
    b4X <- nimMatrix(nrow=n, ncol=4L, init = FALSE)
    b3X <- nimMatrix(nrow=n, ncol=4L, init = FALSE)
    b2X <- nimMatrix(nrow=n, ncol=3L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    
    b6Xt <- nimMatrix(nrow=6L, ncol=n, init = FALSE)
    b5Xt <- nimMatrix(nrow=5L, ncol=n, init = FALSE)
    b4Xt <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b3Xt <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2Xt <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b6XtW <- nimMatrix(nrow=6L, ncol=n, init = FALSE)
    b5XtW <- nimMatrix(nrow=5L, ncol=n, init = FALSE)
    b4XtW <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b3XtW <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2XtW <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b6XtWX <- nimMatrix(nrow=6L, ncol=6L, init = FALSE)
    b5XtWX <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b6DXtWX <- nimMatrix(nrow=6L, ncol=6L, init = FALSE)
    b5DXtWX <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4DXtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3DXtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2DXtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b6prbeta <- nimMatrix(nrow=6L, ncol=1L, init = FALSE)
    b5prbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4prbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3prbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2prbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b6pbeta <- nimMatrix(nrow=6L, ncol=1L, init = FALSE)
    b5pbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4pbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3pbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2pbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b6tpbeta <- nimMatrix(nrow=1L, ncol=6L, init = FALSE)
    b5tpbeta <- nimMatrix(nrow=1L, ncol=5L, init = FALSE)
    b4tpbeta <- nimMatrix(nrow=1L, ncol=4L, init = FALSE)
    b3tpbeta <- nimMatrix(nrow=1L, ncol=4L, init = FALSE)
    b2tpbeta <- nimMatrix(nrow=1L, ncol=3L, init = FALSE)
    b1tpbeta <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    
    b6zbeta <- nimMatrix(nrow=6L, ncol=1L, init = FALSE)
    b5zbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b6CI <- nimMatrix(nrow=6L, ncol=6L, init = FALSE)
    b5CI <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b6exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b5exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b4exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b3exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b2exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b1exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b6wts <- 0
    b5wts <- 0
    b4wts <- 0
    b3wts <- 0
    b2wts <- 0
    b1wts <- 0
    
    # Confirm target is scalar
    target <- model$expandNodeNames(target, returnScalarComponents = TRUE)
    if (length(target) > 1) {
      nimStop("Expecting a single flag as the target in 'sampler_FP_xptf_bmac_bmac'.")
    }
    
    # Get dependent nodes 's1flg' and 's2flg'
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get prior model weights, intercepts, and current AR(1) parameters (draws are conditional on these values)
    wts <- model[['wts']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    l <- 0L
    m <- 0L
    irho <- 0
    inu <- 0
    qwtsum <- 0
    s1flg <- 0L
    s2flg <- 0L
    flg <- 0L
    
    ############################################################
    # Required conformal matrices for each possible model flag #
    ############################################################
    
    # intercepts
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    
    # cubic-cubic with no intercept
    for (j in 1:n) {
      for (k in 2:s1p) {
        b6X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        b6X[j,k-2] <<- X[j,k]
      }
    }
    b6Dbetag <<- nimMatrix(0, 6L, 6L)
    for (k in 2:s1p) {
      b6mbetag[k-1,1] <<- mbetag[k,1]
      b6Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      b6mbetag[k-2,1] <<- mbetag[k,1]
      b6Dbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    b6Xt <<- t(b6X)                                                  # transpose bX to use below
    b6prbeta <<- b6Dbetag %*% b6mbetag                               # contribution to posterior mean from prior
    
    sumb6XtWX <<- nimMatrix(0, 6L, 6L)                               # initialize cumulative sums to use in common case
    sumb6zbeta <<- nimMatrix(0, 6L, 1L)

    # cubic-quadratic with no intercept
    for (j in 1:n) {
      for (k in 2:s1p) {
        b5X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:pm1) {
        b5X[j,k-2] <<- X[j,k]
      }
    }
    b5Dbetag <<- nimMatrix(0, 5L, 5L)
    for (k in 2:s1p) {
      b5mbetag[k-1,1] <<- mbetag[k,1]
      b5Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:pm1) {
      b5mbetag[k-2,1] <<- mbetag[k,1]
      b5Dbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    b5Xt <<- t(b5X)                                                 # transpose bX to use below
    b5prbeta <<- b5Dbetag %*% b5mbetag                              # contribution to posterior mean from prior
    
    sumb5XtWX <<- nimMatrix(0, 5L, 5L)                              # initialize cumulative sums to use in common case
    sumb5zbeta <<- nimMatrix(0, 5L, 1L)
    
    # cubic-linear with no intercept
    for (j in 1:n) {
      for (k in 2:s1p) {
        b4X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:pm2) {
        b4X[j,k-2] <<- X[j,k]
      }
    }
    b4Dbetag <<- nimMatrix(0, 4L, 4L)
    for (k in 2:s1p) {
      b4mbetag[k-1,1] <<- mbetag[k,1]
      b4Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:pm2) {
      b4mbetag[k-2,1] <<- mbetag[k,1]
      b4Dbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    b4Xt <<- t(b4X)                                                 # transpose bX to use below
    b4prbeta <<- b4Dbetag %*% b4mbetag                              # contribution to posterior mean from prior
    
    sumb4XtWX <<- nimMatrix(0, 4L, 4L)                              # initialize cumulative sums to use in common case
    sumb4zbeta <<- nimMatrix(0, 4L, 1L)
    
    # quadratic-quadratic with no intercept
    for (j in 1:n) {
      for (k in 2:s1pm1) {
        b3X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:pm1) {
        b3X[j,k-3] <<- X[j,k]
      }
    }
    b3Dbetag <<- nimMatrix(0, 4L, 4L)
    for (k in 2:s1pm1) {
      b3mbetag[k-1,1] <<- mbetag[k,1]
      b3Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:pm1) {
      b3mbetag[k-3,1] <<- mbetag[k,1]
      b3Dbetag[k-3,k-3] <<- Dbetag[k,k]
    }
    b3Xt <<- t(b3X)                                                 # transpose bX to use below
    b3prbeta <<- b3Dbetag %*% b3mbetag                              # contribution to posterior mean from prior
    
    sumb3XtWX <<- nimMatrix(0, 4L, 4L)                              # initialize cumulative sums to use in common case
    sumb3zbeta <<- nimMatrix(0, 4L, 1L)
    
    # quadratic-linear with no intercept
    for (j in 1:n) {
      for (k in 2:s1pm1) {
        b2X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:pm2) {
        b2X[j,k-3] <<- X[j,k]
      }
    }
    b2Dbetag <<- nimMatrix(0, 3L, 3L)
    for (k in 2:s1pm1) {
      b2mbetag[k-1,1] <<- mbetag[k,1]
      b2Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:pm2) {
      b2mbetag[k-3,1] <<- mbetag[k,1]
      b2Dbetag[k-3,k-3] <<- Dbetag[k,k]
    }
    b2Xt <<- t(b2X)                                                 # transpose bX to use below
    b2prbeta <<- b2Dbetag %*% b2mbetag                              # contribution to posterior mean from prior
    
    sumb2XtWX <<- nimMatrix(0, 3L, 3L)                              # initialize cumulative sums to use in common case
    sumb2zbeta <<- nimMatrix(0, 3L, 1L)

    # linear-linear with no intercept
    for (j in 1:n) {
      for (k in 2:s1pm2) {
        b1X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:pm2) {
        b1X[j,k-4] <<- X[j,k]
      }
    }
    b1Dbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 2:s1pm2) {
      b1mbetag[k-1,1] <<- mbetag[k,1]
      b1Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:pm2) {
      b1mbetag[k-4,1] <<- mbetag[k,1]
      b1Dbetag[k-4,k-4] <<- Dbetag[k,k]
    }
    b1Xt <<- t(b1X)                                                 # transpose bX to use below
    b1prbeta <<- b1Dbetag %*% b1mbetag                              # contribution to posterior mean from prior
    
    sumb1XtWX <<- nimMatrix(0, 2L, 2L)                              # initialize cumulative sums to use in common case
    sumb1zbeta <<- nimMatrix(0, 2L, 1L)
    
    ################################################################################
    # Model weights (log-scale) for group-specific trend models with no intercepts #
    ################################################################################
    
    # prior weight for intercepts-only model
    pwts[13] <<- 0                                                  # remains zero
    
    # prior weight for group-specific cubic-cubic trend model
    pwts[1] <<- 0
    b6tmbetag <<- t(b6mbetag)
    b6exp <<- b6tmbetag %*% b6prbeta                                # exponent from normal pdf features g times
    pwts[1] <<- pwts[1] - 0.5*g*b6exp[1,1]                          # log scale
    b6wts <<- logdet(b6Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[1] <<- pwts[1] + 0.5*g*b6wts                               # already on log scale
    
    # prior weight for group-specific cubic-quad trend model
    pwts[2] <<- 0
    b5tmbetag <<- t(b5mbetag)
    b5exp <<- b5tmbetag %*% b5prbeta                                # exponent from normal pdf features g times
    pwts[2] <<- pwts[2] - 0.5*g*b5exp[1,1]                          # log scale
    b5wts <<- logdet(b5Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[2] <<- pwts[2] + 0.5*g*b5wts                               # already on log scale

    # prior weight for group-specific cubic-linear trend model
    pwts[3] <<- 0
    b4tmbetag <<- t(b4mbetag)
    b4exp <<- b4tmbetag %*% b4prbeta                                # exponent from normal pdf features g times
    pwts[3] <<- pwts[3] - 0.5*g*b4exp[1,1]                          # log scale
    b4wts <<- logdet(b4Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[3] <<- pwts[3] + 0.5*g*b4wts                               # already on log scale

    # prior weight for group-specific quad-quad trend model
    pwts[4] <<- 0
    b3tmbetag <<- t(b3mbetag)
    b3exp <<- b3tmbetag %*% b3prbeta                                # exponent from normal pdf features g times
    pwts[4] <<- pwts[4] - 0.5*g*b3exp[1,1]                          # log scale
    b3wts <<- logdet(b3Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[4] <<- pwts[4] + 0.5*g*b3wts                               # already on log scale

    # prior weight for group-specific quad-linear trend model
    pwts[5] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf features g times
    pwts[5] <<- pwts[5] - 0.5*g*b2exp[1,1]                          # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[5] <<- pwts[5] + 0.5*g*b2wts                               # already on log scale
    
    # prior weight for group-specific linear-linear trend model
    pwts[6] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf features g times
    pwts[6] <<- pwts[6] - 0.5*g*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[6] <<- pwts[6] + 0.5*g*b1wts                               # already on log scale
    
    # posterior weights from group-specific contributions
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # group-specific cubic-cubic trend model with no intercept
      b6XtW <<- b6Xt %*% Wg                                         # multiply Xt and Wg
      b6XtWX <<- b6XtW %*% b6X                                      # calculate XtWX, the precision matrix from WLS
      b6DXtWX <<- b6Dbetag + b6XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b6zbeta <<- b6XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b6pbeta <<- b6prbeta + b6zbeta                                # sum of prior and WLS contributions
      b6tpbeta <<- t(b6pbeta)                                       # transpose
      b6CI <<- inverse(b6DXtWX)                                     # inverse of posterior precision matrix
      b6pbeta <<- b6CI %*% b6pbeta                                  # re-scale psbeta
      b6exp <<- b6tpbeta %*% b6pbeta                                # exponent from normal pdf
      pwts[1] <<- pwts[1] + 0.5*b6exp[1,1]                          # log scale
      b6wts <<- logdet(b6DXtWX)                                     # log-determinant for normalizing constant
      pwts[1] <<- pwts[1] - 0.5*b6wts                               # already on log scale

      # group-specific cubic-quad trend model with no intercept
      b5XtW <<- b5Xt %*% Wg                                         # multiply Xt and Wg
      b5XtWX <<- b5XtW %*% b5X                                      # calculate XtWX, the precision matrix from WLS
      b5DXtWX <<- b5Dbetag + b5XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b5zbeta <<- b5XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b5pbeta <<- b5prbeta + b5zbeta                                # sum of prior and WLS contributions
      b5tpbeta <<- t(b5pbeta)                                       # transpose
      b5CI <<- inverse(b5DXtWX)                                     # inverse of posterior precision matrix
      b5pbeta <<- b5CI %*% b5pbeta                                  # re-scale psbeta
      b5exp <<- b5tpbeta %*% b5pbeta                                # exponent from normal pdf
      pwts[2] <<- pwts[2] + 0.5*b5exp[1,1]                          # log scale
      b5wts <<- logdet(b5DXtWX)                                     # log-determinant for normalizing constant
      pwts[2] <<- pwts[2] - 0.5*b5wts                               # already on log scale
      
      # group-specific cubic-linear trend model with no intercept
      b4XtW <<- b4Xt %*% Wg                                         # multiply Xt and Wg
      b4XtWX <<- b4XtW %*% b4X                                      # calculate XtWX, the precision matrix from WLS
      b4DXtWX <<- b4Dbetag + b4XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b4zbeta <<- b4XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b4pbeta <<- b4prbeta + b4zbeta                                # sum of prior and WLS contributions
      b4tpbeta <<- t(b4pbeta)                                       # transpose
      b4CI <<- inverse(b4DXtWX)                                     # inverse of posterior precision matrix
      b4pbeta <<- b4CI %*% b4pbeta                                  # re-scale psbeta
      b4exp <<- b4tpbeta %*% b4pbeta                                # exponent from normal pdf
      pwts[3] <<- pwts[3] + 0.5*b4exp[1,1]                          # log scale
      b4wts <<- logdet(b4DXtWX)                                     # log-determinant for normalizing constant
      pwts[3] <<- pwts[3] - 0.5*b4wts                               # already on log scale
      
      # group-specific quad-quad trend model with no intercept
      b3XtW <<- b3Xt %*% Wg                                         # multiply Xt and Wg
      b3XtWX <<- b3XtW %*% b3X                                      # calculate XtWX, the precision matrix from WLS
      b3DXtWX <<- b3Dbetag + b3XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b3zbeta <<- b3XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b3pbeta <<- b3prbeta + b3zbeta                                # sum of prior and WLS contributions
      b3tpbeta <<- t(b3pbeta)                                       # transpose
      b3CI <<- inverse(b3DXtWX)                                     # inverse of posterior precision matrix
      b3pbeta <<- b3CI %*% b3pbeta                                  # re-scale psbeta
      b3exp <<- b3tpbeta %*% b3pbeta                                # exponent from normal pdf
      pwts[4] <<- pwts[4] + 0.5*b3exp[1,1]                          # log scale
      b3wts <<- logdet(b3DXtWX)                                     # log-determinant for normalizing constant
      pwts[4] <<- pwts[4] - 0.5*b3wts                               # already on log scale

      # group-specific quad-linear trend model with no intercept
      b2XtW <<- b2Xt %*% Wg                                         # multiply Xt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate XtWX, the precision matrix from WLS
      b2DXtWX <<- b2Dbetag + b2XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b2zbeta <<- b2XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b2pbeta <<- b2prbeta + b2zbeta                                # sum of prior and WLS contributions
      b2tpbeta <<- t(b2pbeta)                                       # transpose
      b2CI <<- inverse(b2DXtWX)                                     # inverse of posterior precision matrix
      b2pbeta <<- b2CI %*% b2pbeta                                  # re-scale psbeta
      b2exp <<- b2tpbeta %*% b2pbeta                                # exponent from normal pdf
      pwts[5] <<- pwts[5] + 0.5*b2exp[1,1]                          # log scale
      b2wts <<- logdet(b2DXtWX)                                     # log-determinant for normalizing constant
      pwts[5] <<- pwts[5] - 0.5*b2wts                               # already on log scale
      
      # group-specific linear-linear trend model with no intercept
      b1XtW <<- b1Xt %*% Wg                                         # multiply Xt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate XtWX, the precision matrix from WLS
      b1DXtWX <<- b1Dbetag + b1XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b1zbeta <<- b1XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b1pbeta <<- b1prbeta + b1zbeta                                # sum of prior and WLS contributions
      b1tpbeta <<- t(b1pbeta)                                       # transpose
      b1CI <<- inverse(b1DXtWX)                                     # inverse of posterior precision matrix
      b1pbeta <<- b1CI %*% b1pbeta                                  # re-scale psbeta
      b1exp <<- b1tpbeta %*% b1pbeta                                # exponent from normal pdf
      pwts[6] <<- pwts[6] + 0.5*b1exp[1,1]                          # log scale
      b1wts <<- logdet(b1DXtWX)                                     # log-determinant for normalizing constant
      pwts[6] <<- pwts[6] - 0.5*b1wts                               # already on log scale
      
    }                                                               # end cycle through groups
    
    ############################################################
    # Model weights for common trend models with no intercepts #
    ############################################################
    
    # prior weight for common cubic-cubic trend model
    pwts[7] <<- 0
    b6tmbetag <<- t(b6mbetag)
    b6exp <<- b6tmbetag %*% b6prbeta                                # exponent from normal pdf
    pwts[7] <<- pwts[7] - 0.5*b6exp[1,1]                            # log scale
    b6wts <<- logdet(b6Dbetag)                                      # log-determinant of prior precision matrix
    pwts[7] <<- pwts[7] + 0.5*b6wts                                 # already on log scale
    
    # prior weight for common cubic-quad trend model
    pwts[8] <<- 0
    b5tmbetag <<- t(b5mbetag)
    b5exp <<- b5tmbetag %*% b5prbeta                                # exponent from normal pdf
    pwts[8] <<- pwts[8] - 0.5*b5exp[1,1]                            # log scale
    b5wts <<- logdet(b5Dbetag)                                      # log-determinant of prior precision matrix
    pwts[8] <<- pwts[8] + 0.5*b5wts                                 # already on log scale

    # prior weight for common cubic-linear trend model
    pwts[9] <<- 0
    b4tmbetag <<- t(b4mbetag)
    b4exp <<- b4tmbetag %*% b4prbeta                                # exponent from normal pdf
    pwts[9] <<- pwts[9] - 0.5*b4exp[1,1]                            # log scale
    b4wts <<- logdet(b4Dbetag)                                      # log-determinant of prior precision matrix
    pwts[9] <<- pwts[9] + 0.5*b4wts                                 # already on log scale

    # prior weight for common quad-quad trend model
    pwts[10] <<- 0
    b3tmbetag <<- t(b3mbetag)
    b3exp <<- b3tmbetag %*% b3prbeta                                # exponent from normal pdf
    pwts[10] <<- pwts[10] - 0.5*b3exp[1,1]                          # log scale
    b3wts <<- logdet(b3Dbetag)                                      # log-determinant of prior precision matrix
    pwts[10] <<- pwts[10] + 0.5*b3wts                               # already on log scale

    # prior weight for common quad-linear trend model
    pwts[11] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf
    pwts[11] <<- pwts[11] - 0.5*b2exp[1,1]                          # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix
    pwts[11] <<- pwts[11] + 0.5*b2wts                               # already on log scale
    
    # prior weight for common linear-linear trend model
    pwts[12] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf
    pwts[12] <<- pwts[12] - 0.5*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix
    pwts[12] <<- pwts[12] + 0.5*b1wts                               # already on log scale
    
    # cumulative posterior contributions over groups
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # common cubic-cubic trend model with no intercepts
      b6XtW <<- b6Xt %*% Wg                                         # multiply bXt and Wg
      b6XtWX <<- b6XtW %*% b6X                                      # calculate bXtWX
      sumb6XtWX <<- sumb6XtWX + b6XtWX                              # cumulative matrix sum
      b6zbeta <<- b6XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb6zbeta <<- sumb6zbeta + b6zbeta                           # cumulative matrix sum

      # common cubic-quad trend model with no intercepts
      b5XtW <<- b5Xt %*% Wg                                         # multiply bXt and Wg
      b5XtWX <<- b5XtW %*% b5X                                      # calculate bXtWX
      sumb5XtWX <<- sumb5XtWX + b5XtWX                              # cumulative matrix sum
      b5zbeta <<- b5XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb5zbeta <<- sumb5zbeta + b5zbeta                           # cumulative matrix sum

      # common cubic-linear trend model with no intercepts
      b4XtW <<- b4Xt %*% Wg                                         # multiply bXt and Wg
      b4XtWX <<- b4XtW %*% b4X                                      # calculate bXtWX
      sumb4XtWX <<- sumb4XtWX + b4XtWX                              # cumulative matrix sum
      b4zbeta <<- b4XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb4zbeta <<- sumb4zbeta + b4zbeta                           # cumulative matrix sum

      # common quad-quad trend model with no intercepts
      b3XtW <<- b3Xt %*% Wg                                         # multiply bXt and Wg
      b3XtWX <<- b3XtW %*% b3X                                      # calculate bXtWX
      sumb3XtWX <<- sumb3XtWX + b3XtWX                              # cumulative matrix sum
      b3zbeta <<- b3XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb3zbeta <<- sumb3zbeta + b3zbeta                           # cumulative matrix sum
      
      # common quad-linear trend model with no intercepts
      b2XtW <<- b2Xt %*% Wg                                         # multiply bXt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate bXtWX
      sumb2XtWX <<- sumb2XtWX + b2XtWX                              # cumulative matrix sum
      b2zbeta <<- b2XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb2zbeta <<- sumb2zbeta + b2zbeta                           # cumulative matrix sum
      
      # common linear-linear trend model with no intercepts
      b1XtW <<- b1Xt %*% Wg                                         # multiply bXt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate bXtWX
      sumb1XtWX <<- sumb1XtWX + b1XtWX                              # cumulative matrix sum
      b1zbeta <<- b1XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb1zbeta <<- sumb1zbeta + b1zbeta                           # cumulative matrix sum
      
    }                                                               # end cycle through groups
    
    # posterior weight for common cubic-cubic trend with no intercepts
    b6DXtWX <<- b6Dbetag + sumb6XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b6pbeta <<- b6prbeta + sumb6zbeta                               # sum of prior and the cumulative WLS contributions
    b6tpbeta <<- t(b6pbeta)                                         # transpose
    b6CI <<- inverse(b6DXtWX)                                       # inverse of posterior precision matrix
    b6pbeta <<- b6CI %*% b6pbeta                                    # re-scale psbeta
    b6exp <<- b6tpbeta %*% b6pbeta                                  # exponent from normal pdf
    pwts[7] <<- pwts[7] + 0.5*b6exp[1,1]                            # log scale
    b6wts <<- logdet(b6DXtWX)                                       # log-determinant for normalizing constant
    pwts[7] <<- pwts[7] - 0.5*b6wts                                 # already on log-scale

    # posterior weight for common cubic-quad trend with no intercepts
    b5DXtWX <<- b5Dbetag + sumb5XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b5pbeta <<- b5prbeta + sumb5zbeta                               # sum of prior and the cumulative WLS contributions
    b5tpbeta <<- t(b5pbeta)                                         # transpose
    b5CI <<- inverse(b5DXtWX)                                       # inverse of posterior precision matrix
    b5pbeta <<- b5CI %*% b5pbeta                                    # re-scale psbeta
    b5exp <<- b5tpbeta %*% b5pbeta                                  # exponent from normal pdf
    pwts[8] <<- pwts[8] + 0.5*b5exp[1,1]                            # log scale
    b5wts <<- logdet(b5DXtWX)                                       # log-determinant for normalizing constant
    pwts[8] <<- pwts[8] - 0.5*b5wts                                 # already on log-scale

    # posterior weight for common cubic-linear trend with no intercepts
    b4DXtWX <<- b4Dbetag + sumb4XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b4pbeta <<- b4prbeta + sumb4zbeta                               # sum of prior and the cumulative WLS contributions
    b4tpbeta <<- t(b4pbeta)                                         # transpose
    b4CI <<- inverse(b4DXtWX)                                       # inverse of posterior precision matrix
    b4pbeta <<- b4CI %*% b4pbeta                                    # re-scale psbeta
    b4exp <<- b4tpbeta %*% b4pbeta                                  # exponent from normal pdf
    pwts[9] <<- pwts[9] + 0.5*b4exp[1,1]                            # log scale
    b4wts <<- logdet(b4DXtWX)                                       # log-determinant for normalizing constant
    pwts[9] <<- pwts[9] - 0.5*b4wts                                 # already on log-scale

    # posterior weight for common quad-quad trend with no intercepts
    b3DXtWX <<- b3Dbetag + sumb3XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b3pbeta <<- b3prbeta + sumb3zbeta                               # sum of prior and the cumulative WLS contributions
    b3tpbeta <<- t(b3pbeta)                                         # transpose
    b3CI <<- inverse(b3DXtWX)                                       # inverse of posterior precision matrix
    b3pbeta <<- b3CI %*% b3pbeta                                    # re-scale psbeta
    b3exp <<- b3tpbeta %*% b3pbeta                                  # exponent from normal pdf
    pwts[10] <<- pwts[10] + 0.5*b3exp[1,1]                          # log scale
    b3wts <<- logdet(b3DXtWX)                                       # log-determinant for normalizing constant
    pwts[10] <<- pwts[10] - 0.5*b3wts                               # already on log-scale
    
    # posterior weight for common quad-linear trend with no intercepts
    b2DXtWX <<- b2Dbetag + sumb2XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b2pbeta <<- b2prbeta + sumb2zbeta                               # sum of prior and the cumulative WLS contributions
    b2tpbeta <<- t(b2pbeta)                                         # transpose
    b2CI <<- inverse(b2DXtWX)                                       # inverse of posterior precision matrix
    b2pbeta <<- b2CI %*% b2pbeta                                    # re-scale psbeta
    b2exp <<- b2tpbeta %*% b2pbeta                                  # exponent from normal pdf
    pwts[11] <<- pwts[11] + 0.5*b2exp[1,1]                          # log scale
    b2wts <<- logdet(b2DXtWX)                                       # log-determinant for normalizing constant
    pwts[11] <<- pwts[11] - 0.5*b2wts                               # already on log-scale
    
    # posterior weight for common linear-linear trend with no intercepts
    b1DXtWX <<- b1Dbetag + sumb1XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b1pbeta <<- b1prbeta + sumb1zbeta                               # sum of prior and the cumulative WLS contributions
    b1tpbeta <<- t(b1pbeta)                                         # transpose
    b1CI <<- inverse(b1DXtWX)                                       # inverse of posterior precision matrix
    b1pbeta <<- b1CI %*% b1pbeta                                    # re-scale psbeta
    b1exp <<- b1tpbeta %*% b1pbeta                                  # exponent from normal pdf
    pwts[12] <<- pwts[12] + 0.5*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1DXtWX)                                       # log-determinant for normalizing constant
    pwts[12] <<- pwts[12] - 0.5*b1wts                               # already on log-scale
    
    #############################################
    # Rescaled posterior model weights and draw #
    #############################################
    
    # calculated using differences on log-scale (a.k.a., Bayes factors) for numerical stability
    for (m in 1:fp) {
      qwtsum <- 0
      for (l in 1:fp) {
        qwtsum <- qwtsum + (wts[l]/wts[m])*exp(pwts[l]-pwts[m])
      }
      qwts[m] <<- 1/qwtsum
    }
    
    # check for any missing values
    qwtsum <- NaN
    if (any_na(qwts)) {                                             # set qwtsum to zero if any missings
      qwtsum <- 0
    }
    if (!is.na(qwtsum)) {                                           # if there were missing values
      for (m in 1:fp) {
        if (is.na(qwts[m])) {
          qwts[m] <<- 0                                             # replace them with zeroes
        }
        qwtsum <- qwtsum + qwts[m]
      }
      if (qwtsum > 0) {                                             # rescale to sum to 1
        for (m in 1:fp) {
          qwts[m] <<- qwts[m]/qwtsum
        }
      } else {                                                      # (qwtsum == 0) all weights are zero: use prior weights
        for (m in 1:fp) {
          qwts[m] <<- wts[m]
        }
      }
    }
    
    # sample from discrete posterior distribution of model flags (conditional on intercepts)
    flg <- as.integer(rcat(1, qwts))
    
    if (flg != model[['flg']]) {
      
      # Update model with new flag value(s)
      model[['flg']] <<- flg
      
      # Re-calculate prior probabilities (in case they were not uniform)
      model$calculate(target)
      
      # Re-calculate segment-specific flags 's1flg' and 's2flg'
      model$calculate(deterministicDependents)
      
      # Copy from model to mvSaved objects
      nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
      nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
      
    }
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_FP_xptf_bmac_bmac (sampler_CPb_xptf_bmac_bmac will update trend coefficients according to new 's1flg' and 's2flg' values)

# Custom Gibbs sampler for model flags, conditional on intercepts, in the cubic-quadratic BMA model with a full trend break
sampler_FP_xptf_bmac_bmaq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Make sure the 'target' value is correctly specified
    if (lt != 1) {
      nimStop("Only the model flag 'flg' can be sampled using 'sampler_FP_xptf_bmac_bmaq'. Segment-specific flags 's1flg' and 's2flg' are derived deterministically from 'flg'.")
    } else {
      if (target[1] != 'flg') {
        nimStop("Target node must be the model flag (integer) 'flg'.")
      }
    }
    
    # Get constants from model to use in calculations
    s1pp2 <- 0L
    s1pm2 <- 0L
    s1pm1 <- 0L
    s1p <- model$getConstants()$s1p
    s1pm1 <- s1p - 1
    s1pm2 <- s1p - 2
    s1pp2 <- s1p + 2
    pm1 <- 0L
    p <- model$getConstants()$p
    pm1 <- p-1
    if (p != 7) {
      nimStop("Expecting design matrix to have 7 columns in 'sampler_FP_xptf_bmac_bmaq'.")
    }
    fp <- model$getConstants()$fp
    if (fp != 11) {
      nimStop("Expecting a total of 11 possible trend models in 'sampler_FP_xptf_bmac_bmaq'.")
    }
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    pwts <- nimNumeric(fp, init = FALSE)
    qwts <- nimNumeric(fp, init = FALSE)
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb5XtWX <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    sumb4XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    sumb3XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    sumb2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    sumb5zbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    sumb4zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    sumb3zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    sumb2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b5mbetag <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4mbetag <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3mbetag <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2mbetag <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b5tmbetag <- nimMatrix(nrow=1L, ncol=5L, init = FALSE)
    b4tmbetag <- nimMatrix(nrow=1L, ncol=4L, init = FALSE)
    b3tmbetag <- nimMatrix(nrow=1L, ncol=4L, init = FALSE)
    b2tmbetag <- nimMatrix(nrow=1L, ncol=3L, init = FALSE)
    b1tmbetag <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    
    b5Dbetag <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4Dbetag <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3Dbetag <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2Dbetag <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b5X <- nimMatrix(nrow=n, ncol=5L, init = FALSE)
    b4X <- nimMatrix(nrow=n, ncol=4L, init = FALSE)
    b3X <- nimMatrix(nrow=n, ncol=4L, init = FALSE)
    b2X <- nimMatrix(nrow=n, ncol=3L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    
    b5Xt <- nimMatrix(nrow=5L, ncol=n, init = FALSE)
    b4Xt <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b3Xt <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2Xt <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b5XtW <- nimMatrix(nrow=5L, ncol=n, init = FALSE)
    b4XtW <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b3XtW <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2XtW <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b5XtWX <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b5DXtWX <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4DXtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3DXtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2DXtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b5prbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4prbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3prbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2prbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b5pbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4pbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3pbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2pbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b5tpbeta <- nimMatrix(nrow=1L, ncol=5L, init = FALSE)
    b4tpbeta <- nimMatrix(nrow=1L, ncol=4L, init = FALSE)
    b3tpbeta <- nimMatrix(nrow=1L, ncol=4L, init = FALSE)
    b2tpbeta <- nimMatrix(nrow=1L, ncol=3L, init = FALSE)
    b1tpbeta <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    
    b5zbeta <- nimMatrix(nrow=5L, ncol=1L, init = FALSE)
    b4zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b3zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b5CI <- nimMatrix(nrow=5L, ncol=5L, init = FALSE)
    b4CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b3CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b5exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b4exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b3exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b2exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b1exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b5wts <- 0
    b4wts <- 0
    b3wts <- 0
    b2wts <- 0
    b1wts <- 0
    
    # Confirm target is scalar
    target <- model$expandNodeNames(target, returnScalarComponents = TRUE)
    if (length(target) > 1) {
      nimStop("Expecting a single flag as the target in 'sampler_FP_xptf_bmac_bmaq'.")
    }
    
    # Get dependent nodes 's1flg' and 's2flg'
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get prior model weights, intercepts, and current AR(1) parameters (draws are conditional on these values)
    wts <- model[['wts']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    l <- 0L
    m <- 0L
    irho <- 0
    inu <- 0
    qwtsum <- 0
    s1flg <- 0L
    s2flg <- 0L
    flg <- 0L
    
    ############################################################
    # Required conformal matrices for each possible model flag #
    ############################################################
    
    # intercepts
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    
    # cubic-quadratic with no intercept
    for (j in 1:n) {
      for (k in 2:s1p) {
        b5X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        b5X[j,k-2] <<- X[j,k]
      }
    }
    b5Dbetag <<- nimMatrix(0, 5L, 5L)
    for (k in 2:s1p) {
      b5mbetag[k-1,1] <<- mbetag[k,1]
      b5Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      b5mbetag[k-2,1] <<- mbetag[k,1]
      b5Dbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    b5Xt <<- t(b5X)                                                 # transpose bX to use below
    b5prbeta <<- b5Dbetag %*% b5mbetag                              # contribution to posterior mean from prior
    
    sumb5XtWX <<- nimMatrix(0, 5L, 5L)                              # initialize cumulative sums to use in common case
    sumb5zbeta <<- nimMatrix(0, 5L, 1L)
    
    # cubic-linear with no intercept
    for (j in 1:n) {
      for (k in 2:s1p) {
        b4X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:pm1) {
        b4X[j,k-2] <<- X[j,k]
      }
    }
    b4Dbetag <<- nimMatrix(0, 4L, 4L)
    for (k in 2:s1p) {
      b4mbetag[k-1,1] <<- mbetag[k,1]
      b4Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:pm1) {
      b4mbetag[k-2,1] <<- mbetag[k,1]
      b4Dbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    b4Xt <<- t(b4X)                                                 # transpose bX to use below
    b4prbeta <<- b4Dbetag %*% b4mbetag                              # contribution to posterior mean from prior
    
    sumb4XtWX <<- nimMatrix(0, 4L, 4L)                              # initialize cumulative sums to use in common case
    sumb4zbeta <<- nimMatrix(0, 4L, 1L)
    
    # quadratic-quadratic with no intercept
    for (j in 1:n) {
      for (k in 2:s1pm1) {
        b3X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        b3X[j,k-3] <<- X[j,k]
      }
    }
    b3Dbetag <<- nimMatrix(0, 4L, 4L)
    for (k in 2:s1pm1) {
      b3mbetag[k-1,1] <<- mbetag[k,1]
      b3Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      b3mbetag[k-3,1] <<- mbetag[k,1]
      b3Dbetag[k-3,k-3] <<- Dbetag[k,k]
    }
    b3Xt <<- t(b3X)                                                 # transpose bX to use below
    b3prbeta <<- b3Dbetag %*% b3mbetag                              # contribution to posterior mean from prior
    
    sumb3XtWX <<- nimMatrix(0, 4L, 4L)                              # initialize cumulative sums to use in common case
    sumb3zbeta <<- nimMatrix(0, 4L, 1L)
    
    # quadratic-linear with no intercept
    for (j in 1:n) {
      for (k in 2:s1pm1) {
        b2X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:pm1) {
        b2X[j,k-3] <<- X[j,k]
      }
    }
    b2Dbetag <<- nimMatrix(0, 3L, 3L)
    for (k in 2:s1pm1) {
      b2mbetag[k-1,1] <<- mbetag[k,1]
      b2Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:pm1) {
      b2mbetag[k-3,1] <<- mbetag[k,1]
      b2Dbetag[k-3,k-3] <<- Dbetag[k,k]
    }
    b2Xt <<- t(b2X)                                                 # transpose bX to use below
    b2prbeta <<- b2Dbetag %*% b2mbetag                              # contribution to posterior mean from prior
    
    sumb2XtWX <<- nimMatrix(0, 3L, 3L)                              # initialize cumulative sums to use in common case
    sumb2zbeta <<- nimMatrix(0, 3L, 1L)
    
    # linear-linear with no intercept
    for (j in 1:n) {
      for (k in 2:s1pm2) {
        b1X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:pm1) {
        b1X[j,k-4] <<- X[j,k]
      }
    }
    b1Dbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 2:s1pm2) {
      b1mbetag[k-1,1] <<- mbetag[k,1]
      b1Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:pm1) {
      b1mbetag[k-4,1] <<- mbetag[k,1]
      b1Dbetag[k-4,k-4] <<- Dbetag[k,k]
    }
    b1Xt <<- t(b1X)                                                 # transpose bX to use below
    b1prbeta <<- b1Dbetag %*% b1mbetag                              # contribution to posterior mean from prior
    
    sumb1XtWX <<- nimMatrix(0, 2L, 2L)                              # initialize cumulative sums to use in common case
    sumb1zbeta <<- nimMatrix(0, 2L, 1L)
    
    ################################################################################
    # Model weights (log-scale) for group-specific trend models with no intercepts #
    ################################################################################
    
    # prior weight for intercepts-only model
    pwts[11] <<- 0                                                  # remains zero
    
    # prior weight for group-specific cubic-quad trend model
    pwts[1] <<- 0
    b5tmbetag <<- t(b5mbetag)
    b5exp <<- b5tmbetag %*% b5prbeta                                # exponent from normal pdf features g times
    pwts[1] <<- pwts[1] - 0.5*g*b5exp[1,1]                          # log scale
    b5wts <<- logdet(b5Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[1] <<- pwts[1] + 0.5*g*b5wts                               # already on log scale
    
    # prior weight for group-specific cubic-linear trend model
    pwts[2] <<- 0
    b4tmbetag <<- t(b4mbetag)
    b4exp <<- b4tmbetag %*% b4prbeta                                # exponent from normal pdf features g times
    pwts[2] <<- pwts[2] - 0.5*g*b4exp[1,1]                          # log scale
    b4wts <<- logdet(b4Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[2] <<- pwts[2] + 0.5*g*b4wts                               # already on log scale
    
    # prior weight for group-specific quad-quad trend model
    pwts[3] <<- 0
    b3tmbetag <<- t(b3mbetag)
    b3exp <<- b3tmbetag %*% b3prbeta                                # exponent from normal pdf features g times
    pwts[3] <<- pwts[3] - 0.5*g*b3exp[1,1]                          # log scale
    b3wts <<- logdet(b3Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[3] <<- pwts[3] + 0.5*g*b3wts                               # already on log scale
    
    # prior weight for group-specific quad-linear trend model
    pwts[4] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf features g times
    pwts[4] <<- pwts[4] - 0.5*g*b2exp[1,1]                          # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[4] <<- pwts[4] + 0.5*g*b2wts                               # already on log scale
    
    # prior weight for group-specific linear-linear trend model
    pwts[5] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf features g times
    pwts[5] <<- pwts[5] - 0.5*g*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[5] <<- pwts[5] + 0.5*g*b1wts                               # already on log scale
    
    # posterior weights from group-specific contributions
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # group-specific cubic-quad trend model with no intercept
      b5XtW <<- b5Xt %*% Wg                                         # multiply Xt and Wg
      b5XtWX <<- b5XtW %*% b5X                                      # calculate XtWX, the precision matrix from WLS
      b5DXtWX <<- b5Dbetag + b5XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b5zbeta <<- b5XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b5pbeta <<- b5prbeta + b5zbeta                                # sum of prior and WLS contributions
      b5tpbeta <<- t(b5pbeta)                                       # transpose
      b5CI <<- inverse(b5DXtWX)                                     # inverse of posterior precision matrix
      b5pbeta <<- b5CI %*% b5pbeta                                  # re-scale psbeta
      b5exp <<- b5tpbeta %*% b5pbeta                                # exponent from normal pdf
      pwts[1] <<- pwts[1] + 0.5*b5exp[1,1]                          # log scale
      b5wts <<- logdet(b5DXtWX)                                     # log-determinant for normalizing constant
      pwts[1] <<- pwts[1] - 0.5*b5wts                               # already on log scale
      
      # group-specific cubic-linear trend model with no intercept
      b4XtW <<- b4Xt %*% Wg                                         # multiply Xt and Wg
      b4XtWX <<- b4XtW %*% b4X                                      # calculate XtWX, the precision matrix from WLS
      b4DXtWX <<- b4Dbetag + b4XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b4zbeta <<- b4XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b4pbeta <<- b4prbeta + b4zbeta                                # sum of prior and WLS contributions
      b4tpbeta <<- t(b4pbeta)                                       # transpose
      b4CI <<- inverse(b4DXtWX)                                     # inverse of posterior precision matrix
      b4pbeta <<- b4CI %*% b4pbeta                                  # re-scale psbeta
      b4exp <<- b4tpbeta %*% b4pbeta                                # exponent from normal pdf
      pwts[2] <<- pwts[2] + 0.5*b4exp[1,1]                          # log scale
      b4wts <<- logdet(b4DXtWX)                                     # log-determinant for normalizing constant
      pwts[2] <<- pwts[2] - 0.5*b4wts                               # already on log scale
      
      # group-specific quad-quad trend model with no intercept
      b3XtW <<- b3Xt %*% Wg                                         # multiply Xt and Wg
      b3XtWX <<- b3XtW %*% b3X                                      # calculate XtWX, the precision matrix from WLS
      b3DXtWX <<- b3Dbetag + b3XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b3zbeta <<- b3XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b3pbeta <<- b3prbeta + b3zbeta                                # sum of prior and WLS contributions
      b3tpbeta <<- t(b3pbeta)                                       # transpose
      b3CI <<- inverse(b3DXtWX)                                     # inverse of posterior precision matrix
      b3pbeta <<- b3CI %*% b3pbeta                                  # re-scale psbeta
      b3exp <<- b3tpbeta %*% b3pbeta                                # exponent from normal pdf
      pwts[3] <<- pwts[3] + 0.5*b3exp[1,1]                          # log scale
      b3wts <<- logdet(b3DXtWX)                                     # log-determinant for normalizing constant
      pwts[3] <<- pwts[3] - 0.5*b3wts                               # already on log scale
      
      # group-specific quad-linear trend model with no intercept
      b2XtW <<- b2Xt %*% Wg                                         # multiply Xt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate XtWX, the precision matrix from WLS
      b2DXtWX <<- b2Dbetag + b2XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b2zbeta <<- b2XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b2pbeta <<- b2prbeta + b2zbeta                                # sum of prior and WLS contributions
      b2tpbeta <<- t(b2pbeta)                                       # transpose
      b2CI <<- inverse(b2DXtWX)                                     # inverse of posterior precision matrix
      b2pbeta <<- b2CI %*% b2pbeta                                  # re-scale psbeta
      b2exp <<- b2tpbeta %*% b2pbeta                                # exponent from normal pdf
      pwts[4] <<- pwts[4] + 0.5*b2exp[1,1]                          # log scale
      b2wts <<- logdet(b2DXtWX)                                     # log-determinant for normalizing constant
      pwts[4] <<- pwts[4] - 0.5*b2wts                               # already on log scale
      
      # group-specific linear-linear trend model with no intercept
      b1XtW <<- b1Xt %*% Wg                                         # multiply Xt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate XtWX, the precision matrix from WLS
      b1DXtWX <<- b1Dbetag + b1XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b1zbeta <<- b1XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b1pbeta <<- b1prbeta + b1zbeta                                # sum of prior and WLS contributions
      b1tpbeta <<- t(b1pbeta)                                       # transpose
      b1CI <<- inverse(b1DXtWX)                                     # inverse of posterior precision matrix
      b1pbeta <<- b1CI %*% b1pbeta                                  # re-scale psbeta
      b1exp <<- b1tpbeta %*% b1pbeta                                # exponent from normal pdf
      pwts[5] <<- pwts[5] + 0.5*b1exp[1,1]                          # log scale
      b1wts <<- logdet(b1DXtWX)                                     # log-determinant for normalizing constant
      pwts[5] <<- pwts[5] - 0.5*b1wts                               # already on log scale
      
    }                                                               # end cycle through groups
    
    ############################################################
    # Model weights for common trend models with no intercepts #
    ############################################################
    
    # prior weight for common cubic-quad trend model
    pwts[6] <<- 0
    b5tmbetag <<- t(b5mbetag)
    b5exp <<- b5tmbetag %*% b5prbeta                                # exponent from normal pdf
    pwts[6] <<- pwts[6] - 0.5*b5exp[1,1]                            # log scale
    b5wts <<- logdet(b5Dbetag)                                      # log-determinant of prior precision matrix
    pwts[6] <<- pwts[6] + 0.5*b5wts                                 # already on log scale
    
    # prior weight for common cubic-linear trend model
    pwts[7] <<- 0
    b4tmbetag <<- t(b4mbetag)
    b4exp <<- b4tmbetag %*% b4prbeta                                # exponent from normal pdf
    pwts[7] <<- pwts[7] - 0.5*b4exp[1,1]                            # log scale
    b4wts <<- logdet(b4Dbetag)                                      # log-determinant of prior precision matrix
    pwts[7] <<- pwts[7] + 0.5*b4wts                                 # already on log scale
    
    # prior weight for common quad-quad trend model
    pwts[8] <<- 0
    b3tmbetag <<- t(b3mbetag)
    b3exp <<- b3tmbetag %*% b3prbeta                                # exponent from normal pdf
    pwts[8] <<- pwts[8] - 0.5*b3exp[1,1]                            # log scale
    b3wts <<- logdet(b3Dbetag)                                      # log-determinant of prior precision matrix
    pwts[8] <<- pwts[8] + 0.5*b3wts                                 # already on log scale
    
    # prior weight for common quad-linear trend model
    pwts[9] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf
    pwts[9] <<- pwts[9] - 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix
    pwts[9] <<- pwts[9] + 0.5*b2wts                                 # already on log scale
    
    # prior weight for common linear-linear trend model
    pwts[10] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf
    pwts[10] <<- pwts[10] - 0.5*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix
    pwts[10] <<- pwts[10] + 0.5*b1wts                               # already on log scale
    
    # cumulative posterior contributions over groups
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # common cubic-quad trend model with no intercepts
      b5XtW <<- b5Xt %*% Wg                                         # multiply bXt and Wg
      b5XtWX <<- b5XtW %*% b5X                                      # calculate bXtWX
      sumb5XtWX <<- sumb5XtWX + b5XtWX                              # cumulative matrix sum
      b5zbeta <<- b5XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb5zbeta <<- sumb5zbeta + b5zbeta                           # cumulative matrix sum
      
      # common cubic-linear trend model with no intercepts
      b4XtW <<- b4Xt %*% Wg                                         # multiply bXt and Wg
      b4XtWX <<- b4XtW %*% b4X                                      # calculate bXtWX
      sumb4XtWX <<- sumb4XtWX + b4XtWX                              # cumulative matrix sum
      b4zbeta <<- b4XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb4zbeta <<- sumb4zbeta + b4zbeta                           # cumulative matrix sum
      
      # common quad-quad trend model with no intercepts
      b3XtW <<- b3Xt %*% Wg                                         # multiply bXt and Wg
      b3XtWX <<- b3XtW %*% b3X                                      # calculate bXtWX
      sumb3XtWX <<- sumb3XtWX + b3XtWX                              # cumulative matrix sum
      b3zbeta <<- b3XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb3zbeta <<- sumb3zbeta + b3zbeta                           # cumulative matrix sum
      
      # common quad-linear trend model with no intercepts
      b2XtW <<- b2Xt %*% Wg                                         # multiply bXt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate bXtWX
      sumb2XtWX <<- sumb2XtWX + b2XtWX                              # cumulative matrix sum
      b2zbeta <<- b2XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb2zbeta <<- sumb2zbeta + b2zbeta                           # cumulative matrix sum
      
      # common linear-linear trend model with no intercepts
      b1XtW <<- b1Xt %*% Wg                                         # multiply bXt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate bXtWX
      sumb1XtWX <<- sumb1XtWX + b1XtWX                              # cumulative matrix sum
      b1zbeta <<- b1XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb1zbeta <<- sumb1zbeta + b1zbeta                           # cumulative matrix sum
      
    }                                                               # end cycle through groups
    
    # posterior weight for common cubic-quad trend with no intercepts
    b5DXtWX <<- b5Dbetag + sumb5XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b5pbeta <<- b5prbeta + sumb5zbeta                               # sum of prior and the cumulative WLS contributions
    b5tpbeta <<- t(b5pbeta)                                         # transpose
    b5CI <<- inverse(b5DXtWX)                                       # inverse of posterior precision matrix
    b5pbeta <<- b5CI %*% b5pbeta                                    # re-scale psbeta
    b5exp <<- b5tpbeta %*% b5pbeta                                  # exponent from normal pdf
    pwts[6] <<- pwts[6] + 0.5*b5exp[1,1]                            # log scale
    b5wts <<- logdet(b5DXtWX)                                       # log-determinant for normalizing constant
    pwts[6] <<- pwts[6] - 0.5*b5wts                                 # already on log-scale
    
    # posterior weight for common cubic-linear trend with no intercepts
    b4DXtWX <<- b4Dbetag + sumb4XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b4pbeta <<- b4prbeta + sumb4zbeta                               # sum of prior and the cumulative WLS contributions
    b4tpbeta <<- t(b4pbeta)                                         # transpose
    b4CI <<- inverse(b4DXtWX)                                       # inverse of posterior precision matrix
    b4pbeta <<- b4CI %*% b4pbeta                                    # re-scale psbeta
    b4exp <<- b4tpbeta %*% b4pbeta                                  # exponent from normal pdf
    pwts[7] <<- pwts[7] + 0.5*b4exp[1,1]                            # log scale
    b4wts <<- logdet(b4DXtWX)                                       # log-determinant for normalizing constant
    pwts[7] <<- pwts[7] - 0.5*b4wts                                 # already on log-scale
    
    # posterior weight for common quad-quad trend with no intercepts
    b3DXtWX <<- b3Dbetag + sumb3XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b3pbeta <<- b3prbeta + sumb3zbeta                               # sum of prior and the cumulative WLS contributions
    b3tpbeta <<- t(b3pbeta)                                         # transpose
    b3CI <<- inverse(b3DXtWX)                                       # inverse of posterior precision matrix
    b3pbeta <<- b3CI %*% b3pbeta                                    # re-scale psbeta
    b3exp <<- b3tpbeta %*% b3pbeta                                  # exponent from normal pdf
    pwts[8] <<- pwts[8] + 0.5*b3exp[1,1]                            # log scale
    b3wts <<- logdet(b3DXtWX)                                       # log-determinant for normalizing constant
    pwts[8] <<- pwts[8] - 0.5*b3wts                                 # already on log-scale
    
    # posterior weight for common quad-linear trend with no intercepts
    b2DXtWX <<- b2Dbetag + sumb2XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b2pbeta <<- b2prbeta + sumb2zbeta                               # sum of prior and the cumulative WLS contributions
    b2tpbeta <<- t(b2pbeta)                                         # transpose
    b2CI <<- inverse(b2DXtWX)                                       # inverse of posterior precision matrix
    b2pbeta <<- b2CI %*% b2pbeta                                    # re-scale psbeta
    b2exp <<- b2tpbeta %*% b2pbeta                                  # exponent from normal pdf
    pwts[9] <<- pwts[9] + 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2DXtWX)                                       # log-determinant for normalizing constant
    pwts[9] <<- pwts[9] - 0.5*b2wts                                 # already on log-scale
    
    # posterior weight for common linear-linear trend with no intercepts
    b1DXtWX <<- b1Dbetag + sumb1XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b1pbeta <<- b1prbeta + sumb1zbeta                               # sum of prior and the cumulative WLS contributions
    b1tpbeta <<- t(b1pbeta)                                         # transpose
    b1CI <<- inverse(b1DXtWX)                                       # inverse of posterior precision matrix
    b1pbeta <<- b1CI %*% b1pbeta                                    # re-scale psbeta
    b1exp <<- b1tpbeta %*% b1pbeta                                  # exponent from normal pdf
    pwts[10] <<- pwts[10] + 0.5*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1DXtWX)                                       # log-determinant for normalizing constant
    pwts[10] <<- pwts[10] - 0.5*b1wts                               # already on log-scale
    
    #############################################
    # Rescaled posterior model weights and draw #
    #############################################
    
    # calculated using differences on log-scale (a.k.a., Bayes factors) for numerical stability
    for (m in 1:fp) {
      qwtsum <- 0
      for (l in 1:fp) {
        qwtsum <- qwtsum + (wts[l]/wts[m])*exp(pwts[l]-pwts[m])
      }
      qwts[m] <<- 1/qwtsum
    }
    
    # check for any missing values
    qwtsum <- NaN
    if (any_na(qwts)) {                                             # set qwtsum to zero if any missings
      qwtsum <- 0
    }
    if (!is.na(qwtsum)) {                                           # if there were missing values
      for (m in 1:fp) {
        if (is.na(qwts[m])) {
          qwts[m] <<- 0                                             # replace them with zeroes
        }
        qwtsum <- qwtsum + qwts[m]
      }
      if (qwtsum > 0) {                                             # rescale to sum to 1
        for (m in 1:fp) {
          qwts[m] <<- qwts[m]/qwtsum
        }
      } else {                                                      # (qwtsum == 0) all weights are zero: use prior weights
        for (m in 1:fp) {
          qwts[m] <<- wts[m]
        }
      }
    }
    
    # sample from discrete posterior distribution of model flags (conditional on intercepts)
    flg <- as.integer(rcat(1, qwts))
    
    if (flg != model[['flg']]) {
      
      # Update model with new flag value(s)
      model[['flg']] <<- flg
      
      # Re-calculate prior probabilities (in case they were not uniform)
      model$calculate(target)
      
      # Re-calculate segment-specific flags 's1flg' and 's2flg'
      model$calculate(deterministicDependents)
      
      # Copy from model to mvSaved objects
      nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
      nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
      
    }
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_FP_xptf_bmac_bmaq (sampler_CPb_xptf_bmac_bmaq will update trend coefficients according to new 's1flg' and 's2flg' values)

# Custom Gibbs sampler for model flags, conditional on intercepts, in the cubic-linear BMA model with a full trend break
sampler_FP_xptf_bmac_bmal <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Make sure the 'target' value is correctly specified
    if (lt != 1) {
      nimStop("Only the model flag 'flg' can be sampled using 'sampler_FP_xptf_bmac_bmal'. Segment-specific flags 's1flg' and 's2flg' are derived deterministically from 'flg'.")
    } else {
      if (target[1] != 'flg') {
        nimStop("Target node must be the model flag (integer) 'flg'.")
      }
    }
    
    # Get constants from model to use in calculations
    s1pp2 <- 0L
    s1pm2 <- 0L
    s1pm1 <- 0L
    s1p <- model$getConstants()$s1p
    s1pm1 <- s1p - 1
    s1pm2 <- s1p - 2
    s1pp2 <- s1p + 2
    p <- model$getConstants()$p
    if (p != 6) {
      nimStop("Expecting design matrix to have 6 columns in 'sampler_FP_xptf_bmac_bmal'.")
    }
    fp <- model$getConstants()$fp
    if (fp != 7) {
      nimStop("Expecting a total of 7 possible trend models in 'sampler_FP_xptf_bmac_bmal'.")
    }
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    pwts <- nimNumeric(fp, init = FALSE)
    qwts <- nimNumeric(fp, init = FALSE)
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb4XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    sumb2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    sumb4zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    sumb2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b4mbetag <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2mbetag <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b4tmbetag <- nimMatrix(nrow=1L, ncol=4L, init = FALSE)
    b2tmbetag <- nimMatrix(nrow=1L, ncol=3L, init = FALSE)
    b1tmbetag <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    
    b4Dbetag <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2Dbetag <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b4X <- nimMatrix(nrow=n, ncol=4L, init = FALSE)
    b2X <- nimMatrix(nrow=n, ncol=3L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    
    b4Xt <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2Xt <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b4XtW <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2XtW <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b4XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b4DXtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2DXtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b4prbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2prbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b4pbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2pbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b4tpbeta <- nimMatrix(nrow=1L, ncol=4L, init = FALSE)
    b2tpbeta <- nimMatrix(nrow=1L, ncol=3L, init = FALSE)
    b1tpbeta <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    
    b4zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b4CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b4exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b2exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b1exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b4wts <- 0
    b2wts <- 0
    b1wts <- 0
    
    # Confirm target is scalar
    target <- model$expandNodeNames(target, returnScalarComponents = TRUE)
    if (length(target) > 1) {
      nimStop("Expecting a single flag as the target in 'sampler_FP_xptf_bmac_bmal'.")
    }
    
    # Get dependent nodes 's1flg' and 's2flg'
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get prior model weights, intercepts, and current AR(1) parameters (draws are conditional on these values)
    wts <- model[['wts']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    l <- 0L
    m <- 0L
    irho <- 0
    inu <- 0
    qwtsum <- 0
    s1flg <- 0L
    s2flg <- 0L
    flg <- 0L
    
    ############################################################
    # Required conformal matrices for each possible model flag #
    ############################################################
    
    # intercepts
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    
    # cubic-linear with no intercept
    for (j in 1:n) {
      for (k in 2:s1p) {
        b4X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        b4X[j,k-2] <<- X[j,k]
      }
    }
    b4Dbetag <<- nimMatrix(0, 4L, 4L)
    for (k in 2:s1p) {
      b4mbetag[k-1,1] <<- mbetag[k,1]
      b4Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      b4mbetag[k-2,1] <<- mbetag[k,1]
      b4Dbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    b4Xt <<- t(b4X)                                                 # transpose bX to use below
    b4prbeta <<- b4Dbetag %*% b4mbetag                              # contribution to posterior mean from prior
    
    sumb4XtWX <<- nimMatrix(0, 4L, 4L)                              # initialize cumulative sums to use in common case
    sumb4zbeta <<- nimMatrix(0, 4L, 1L)
    
    # quadratic-linear with no intercept
    for (j in 1:n) {
      for (k in 2:s1pm1) {
        b2X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        b2X[j,k-3] <<- X[j,k]
      }
    }
    b2Dbetag <<- nimMatrix(0, 3L, 3L)
    for (k in 2:s1pm1) {
      b2mbetag[k-1,1] <<- mbetag[k,1]
      b2Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      b2mbetag[k-3,1] <<- mbetag[k,1]
      b2Dbetag[k-3,k-3] <<- Dbetag[k,k]
    }
    b2Xt <<- t(b2X)                                                 # transpose bX to use below
    b2prbeta <<- b2Dbetag %*% b2mbetag                              # contribution to posterior mean from prior
    
    sumb2XtWX <<- nimMatrix(0, 3L, 3L)                              # initialize cumulative sums to use in common case
    sumb2zbeta <<- nimMatrix(0, 3L, 1L)
    
    # linear-linear with no intercept
    for (j in 1:n) {
      for (k in 2:s1pm2) {
        b1X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        b1X[j,k-4] <<- X[j,k]
      }
    }
    b1Dbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 2:s1pm2) {
      b1mbetag[k-1,1] <<- mbetag[k,1]
      b1Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      b1mbetag[k-4,1] <<- mbetag[k,1]
      b1Dbetag[k-4,k-4] <<- Dbetag[k,k]
    }
    b1Xt <<- t(b1X)                                                 # transpose bX to use below
    b1prbeta <<- b1Dbetag %*% b1mbetag                              # contribution to posterior mean from prior
    
    sumb1XtWX <<- nimMatrix(0, 2L, 2L)                              # initialize cumulative sums to use in common case
    sumb1zbeta <<- nimMatrix(0, 2L, 1L)
    
    ################################################################################
    # Model weights (log-scale) for group-specific trend models with no intercepts #
    ################################################################################
    
    # prior weight for intercepts-only model
    pwts[7] <<- 0                                                   # remains zero
    
    # prior weight for group-specific cubic-linear trend model
    pwts[1] <<- 0
    b4tmbetag <<- t(b4mbetag)
    b4exp <<- b4tmbetag %*% b4prbeta                                # exponent from normal pdf features g times
    pwts[1] <<- pwts[1] - 0.5*g*b4exp[1,1]                          # log scale
    b4wts <<- logdet(b4Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[1] <<- pwts[1] + 0.5*g*b4wts                               # already on log scale
    
    # prior weight for group-specific quad-linear trend model
    pwts[2] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf features g times
    pwts[2] <<- pwts[2] - 0.5*g*b2exp[1,1]                          # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[2] <<- pwts[2] + 0.5*g*b2wts                               # already on log scale
    
    # prior weight for group-specific linear-linear trend model
    pwts[3] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf features g times
    pwts[3] <<- pwts[3] - 0.5*g*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[3] <<- pwts[3] + 0.5*g*b1wts                               # already on log scale
    
    # posterior weights from group-specific contributions
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # group-specific cubic-linear trend model with no intercept
      b4XtW <<- b4Xt %*% Wg                                         # multiply Xt and Wg
      b4XtWX <<- b4XtW %*% b4X                                      # calculate XtWX, the precision matrix from WLS
      b4DXtWX <<- b4Dbetag + b4XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b4zbeta <<- b4XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b4pbeta <<- b4prbeta + b4zbeta                                # sum of prior and WLS contributions
      b4tpbeta <<- t(b4pbeta)                                       # transpose
      b4CI <<- inverse(b4DXtWX)                                     # inverse of posterior precision matrix
      b4pbeta <<- b4CI %*% b4pbeta                                  # re-scale psbeta
      b4exp <<- b4tpbeta %*% b4pbeta                                # exponent from normal pdf
      pwts[1] <<- pwts[1] + 0.5*b4exp[1,1]                          # log scale
      b4wts <<- logdet(b4DXtWX)                                     # log-determinant for normalizing constant
      pwts[1] <<- pwts[1] - 0.5*b4wts                               # already on log scale
      
      # group-specific quad-linear trend model with no intercept
      b2XtW <<- b2Xt %*% Wg                                         # multiply Xt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate XtWX, the precision matrix from WLS
      b2DXtWX <<- b2Dbetag + b2XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b2zbeta <<- b2XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b2pbeta <<- b2prbeta + b2zbeta                                # sum of prior and WLS contributions
      b2tpbeta <<- t(b2pbeta)                                       # transpose
      b2CI <<- inverse(b2DXtWX)                                     # inverse of posterior precision matrix
      b2pbeta <<- b2CI %*% b2pbeta                                  # re-scale psbeta
      b2exp <<- b2tpbeta %*% b2pbeta                                # exponent from normal pdf
      pwts[2] <<- pwts[2] + 0.5*b2exp[1,1]                          # log scale
      b2wts <<- logdet(b2DXtWX)                                     # log-determinant for normalizing constant
      pwts[2] <<- pwts[2] - 0.5*b2wts                               # already on log scale
      
      # group-specific linear-linear trend model with no intercept
      b1XtW <<- b1Xt %*% Wg                                         # multiply Xt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate XtWX, the precision matrix from WLS
      b1DXtWX <<- b1Dbetag + b1XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b1zbeta <<- b1XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b1pbeta <<- b1prbeta + b1zbeta                                # sum of prior and WLS contributions
      b1tpbeta <<- t(b1pbeta)                                       # transpose
      b1CI <<- inverse(b1DXtWX)                                     # inverse of posterior precision matrix
      b1pbeta <<- b1CI %*% b1pbeta                                  # re-scale psbeta
      b1exp <<- b1tpbeta %*% b1pbeta                                # exponent from normal pdf
      pwts[3] <<- pwts[3] + 0.5*b1exp[1,1]                          # log scale
      b1wts <<- logdet(b1DXtWX)                                     # log-determinant for normalizing constant
      pwts[3] <<- pwts[3] - 0.5*b1wts                               # already on log scale
      
    }                                                               # end cycle through groups
    
    ############################################################
    # Model weights for common trend models with no intercepts #
    ############################################################
    
    # prior weight for common cubic-linear trend model
    pwts[4] <<- 0
    b4tmbetag <<- t(b4mbetag)
    b4exp <<- b4tmbetag %*% b4prbeta                                # exponent from normal pdf
    pwts[4] <<- pwts[4] - 0.5*b4exp[1,1]                            # log scale
    b4wts <<- logdet(b4Dbetag)                                      # log-determinant of prior precision matrix
    pwts[4] <<- pwts[4] + 0.5*b4wts                                 # already on log scale
    
    # prior weight for common quad-linear trend model
    pwts[5] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf
    pwts[5] <<- pwts[5] - 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix
    pwts[5] <<- pwts[5] + 0.5*b2wts                                 # already on log scale
    
    # prior weight for common linear-linear trend model
    pwts[6] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf
    pwts[6] <<- pwts[6] - 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix
    pwts[6] <<- pwts[6] + 0.5*b1wts                                 # already on log scale
    
    # cumulative posterior contributions over groups
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # common cubic-linear trend model with no intercepts
      b4XtW <<- b4Xt %*% Wg                                         # multiply bXt and Wg
      b4XtWX <<- b4XtW %*% b4X                                      # calculate bXtWX
      sumb4XtWX <<- sumb4XtWX + b4XtWX                              # cumulative matrix sum
      b4zbeta <<- b4XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb4zbeta <<- sumb4zbeta + b4zbeta                           # cumulative matrix sum
      
      # common quad-linear trend model with no intercepts
      b2XtW <<- b2Xt %*% Wg                                         # multiply bXt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate bXtWX
      sumb2XtWX <<- sumb2XtWX + b2XtWX                              # cumulative matrix sum
      b2zbeta <<- b2XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb2zbeta <<- sumb2zbeta + b2zbeta                           # cumulative matrix sum
      
      # common linear-linear trend model with no intercepts
      b1XtW <<- b1Xt %*% Wg                                         # multiply bXt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate bXtWX
      sumb1XtWX <<- sumb1XtWX + b1XtWX                              # cumulative matrix sum
      b1zbeta <<- b1XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb1zbeta <<- sumb1zbeta + b1zbeta                           # cumulative matrix sum
      
    }                                                               # end cycle through groups
    
    # posterior weight for common cubic-linear trend with no intercepts
    b4DXtWX <<- b4Dbetag + sumb4XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b4pbeta <<- b4prbeta + sumb4zbeta                               # sum of prior and the cumulative WLS contributions
    b4tpbeta <<- t(b4pbeta)                                         # transpose
    b4CI <<- inverse(b4DXtWX)                                       # inverse of posterior precision matrix
    b4pbeta <<- b4CI %*% b4pbeta                                    # re-scale psbeta
    b4exp <<- b4tpbeta %*% b4pbeta                                  # exponent from normal pdf
    pwts[4] <<- pwts[4] + 0.5*b4exp[1,1]                            # log scale
    b4wts <<- logdet(b4DXtWX)                                       # log-determinant for normalizing constant
    pwts[4] <<- pwts[4] - 0.5*b4wts                                 # already on log-scale
    
    # posterior weight for common quad-linear trend with no intercepts
    b2DXtWX <<- b2Dbetag + sumb2XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b2pbeta <<- b2prbeta + sumb2zbeta                               # sum of prior and the cumulative WLS contributions
    b2tpbeta <<- t(b2pbeta)                                         # transpose
    b2CI <<- inverse(b2DXtWX)                                       # inverse of posterior precision matrix
    b2pbeta <<- b2CI %*% b2pbeta                                    # re-scale psbeta
    b2exp <<- b2tpbeta %*% b2pbeta                                  # exponent from normal pdf
    pwts[5] <<- pwts[5] + 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2DXtWX)                                       # log-determinant for normalizing constant
    pwts[5] <<- pwts[5] - 0.5*b2wts                                 # already on log-scale
    
    # posterior weight for common linear-linear trend with no intercepts
    b1DXtWX <<- b1Dbetag + sumb1XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b1pbeta <<- b1prbeta + sumb1zbeta                               # sum of prior and the cumulative WLS contributions
    b1tpbeta <<- t(b1pbeta)                                         # transpose
    b1CI <<- inverse(b1DXtWX)                                       # inverse of posterior precision matrix
    b1pbeta <<- b1CI %*% b1pbeta                                    # re-scale psbeta
    b1exp <<- b1tpbeta %*% b1pbeta                                  # exponent from normal pdf
    pwts[6] <<- pwts[6] + 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1DXtWX)                                       # log-determinant for normalizing constant
    pwts[6] <<- pwts[6] - 0.5*b1wts                                 # already on log-scale
    
    #############################################
    # Rescaled posterior model weights and draw #
    #############################################
    
    # calculated using differences on log-scale (a.k.a., Bayes factors) for numerical stability
    for (m in 1:fp) {
      qwtsum <- 0
      for (l in 1:fp) {
        qwtsum <- qwtsum + (wts[l]/wts[m])*exp(pwts[l]-pwts[m])
      }
      qwts[m] <<- 1/qwtsum
    }
    
    # check for any missing values
    qwtsum <- NaN
    if (any_na(qwts)) {                                             # set qwtsum to zero if any missings
      qwtsum <- 0
    }
    if (!is.na(qwtsum)) {                                           # if there were missing values
      for (m in 1:fp) {
        if (is.na(qwts[m])) {
          qwts[m] <<- 0                                             # replace them with zeroes
        }
        qwtsum <- qwtsum + qwts[m]
      }
      if (qwtsum > 0) {                                             # rescale to sum to 1
        for (m in 1:fp) {
          qwts[m] <<- qwts[m]/qwtsum
        }
      } else {                                                      # (qwtsum == 0) all weights are zero: use prior weights
        for (m in 1:fp) {
          qwts[m] <<- wts[m]
        }
      }
    }
    
    # sample from discrete posterior distribution of model flags (conditional on intercepts)
    flg <- as.integer(rcat(1, qwts))
    
    if (flg != model[['flg']]) {
      
      # Update model with new flag value(s)
      model[['flg']] <<- flg
      
      # Re-calculate prior probabilities (in case they were not uniform)
      model$calculate(target)
      
      # Re-calculate segment-specific flags 's1flg' and 's2flg'
      model$calculate(deterministicDependents)
      
      # Copy from model to mvSaved objects
      nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
      nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
      
    }
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_FP_xptf_bmac_bmal (sampler_CPb_xptf_bmac_bmal will update trend coefficients according to new 's1flg' and 's2flg' values)

# Custom Gibbs sampler for model flag, conditional on intercepts, in the quadratic BMA model
sampler_FP_bmaq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Make sure the 'target' value is correctly specified
    if (lt != 1) {
      nimStop("Only the model flag 'flg' can be sampled using 'sampler_FP_bmaq'.")
    } else {
      if (target[1] != 'flg') {
        nimStop("Target node must be the model flag (integer) 'flg'.")
      }
    }
    
    # Get constants from model to use in calculations
    p <- model$getConstants()$p
    if (p != 3) {
      nimStop("Expecting design matrix to have 3 columns in 'sampler_FP_bmaq'.")
    }
    fp <- model$getConstants()$fp
    if (fp != 5) {
      nimStop("Expecting a total of 5 possible trend models in 'sampler_FP_bmaq'.")
    }
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    pwts <- nimNumeric(fp, init = FALSE)
    qwts <- nimNumeric(fp, init = FALSE)
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2tmbetag <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    b1tmbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    
    b2Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b2XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2tpbeta <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    b1tpbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b1exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2wts <- 0
    b1wts <- 0
    
    # Confirm target is a single scalar
    target <- model$expandNodeNames(target, returnScalarComponents = TRUE)
    if (length(target) > 1) {
      nimStop("Expecting a single model flag as target in 'sampler_FP_bmaq'.")
    }
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get prior model weights, intercepts, and current AR(1) parameters (draws are conditional on these values)
    wts <- model[['wts']]
    ag <- model[['ag']]
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    l <- 0L
    m <- 0L
    irho <- 0
    inu <- 0
    qwtsum <- 0
    flg <- 0L
    
    ############################################################
    # Required conformal matrices for each possible model flag #
    ############################################################
    
    # intercepts
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
    }
    
    # quad with no intercept
    for (j in 1:n) {
      for (k in 2:3) {
        b2X[j,k-1] <<- X[j,k]
      }
    }
    b2Dbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 2:3) {
      b2mbetag[k-1,1] <<- mbetag[k,1]
      b2Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    b2Xt <<- t(b2X)                                                 # transpose bX to use below
    b2prbeta <<- b2Dbetag %*% b2mbetag                              # contribution to posterior mean from prior
    
    sumb2XtWX <<- nimMatrix(0, 2L, 2L)                              # initialize cumulative sums to use in common case
    sumb2zbeta <<- nimMatrix(0, 2L, 1L)
    
    # linear with no intercept
    for (j in 1:n) {
      b1X[j,1] <<- X[j,2]
    }
    b1mbetag[1,1] <<- mbetag[2,1]
    b1Dbetag[1,1] <<- Dbetag[2,2]
    b1Xt <<- t(b1X)                                                 # transpose bX to use below
    b1prbeta <<- b1Dbetag %*% b1mbetag                              # contribution to posterior mean from prior
    
    sumb1XtWX[1,1] <<- 0                                            # initialize cumulative sums to use in common case
    sumb1zbeta[1,1] <<- 0
    
    ################################################################################
    # Model weights (log-scale) for group-specific trend models with no intercepts #
    ################################################################################
    
    # prior weight for intercepts-only model
    pwts[5] <<- 0                                                   # remains zero
    
    # prior weight for group-specific quad trend model
    pwts[1] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf features g times
    pwts[1] <<- pwts[1] - 0.5*g*b2exp[1,1]                          # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[1] <<- pwts[1] + 0.5*g*b2wts                               # already on log scale
    
    # prior weight for group-specific linear trend model
    pwts[2] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf features g times
    pwts[2] <<- pwts[2] - 0.5*g*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[2] <<- pwts[2] + 0.5*g*b1wts                               # already on log scale
    
    # posterior weights from group-specific contributions
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- ag[i]                                          # group-specific abeta vector
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # group-specific quad trend model with no intercept
      b2XtW <<- b2Xt %*% Wg                                         # multiply Xt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate XtWX, the precision matrix from WLS
      b2DXtWX <<- b2Dbetag + b2XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b2zbeta <<- b2XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b2pbeta <<- b2prbeta + b2zbeta                                # sum of prior and WLS contributions
      b2tpbeta <<- t(b2pbeta)                                       # transpose
      b2CI <<- inverse(b2DXtWX)                                     # inverse of posterior precision matrix
      b2pbeta <<- b2CI %*% b2pbeta                                  # re-scale psbeta
      b2exp <<- b2tpbeta %*% b2pbeta                                # exponent from normal pdf
      pwts[1] <<- pwts[1] + 0.5*b2exp[1,1]                          # log scale
      b2wts <<- logdet(b2DXtWX)                                     # log-determinant for normalizing constant
      pwts[1] <<- pwts[1] - 0.5*b2wts                               # already on log scale
      
      # group-specific linear trend model with no intercept
      b1XtW <<- b1Xt %*% Wg                                         # multiply Xt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate XtWX, the precision matrix from WLS
      b1DXtWX <<- b1Dbetag + b1XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b1zbeta <<- b1XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b1pbeta <<- b1prbeta + b1zbeta                                # sum of prior and WLS contributions
      b1tpbeta <<- t(b1pbeta)                                       # transpose
      b1CI <<- inverse(b1DXtWX)                                     # inverse of posterior precision matrix
      b1pbeta <<- b1CI %*% b1pbeta                                  # re-scale psbeta
      b1exp <<- b1tpbeta %*% b1pbeta                                # exponent from normal pdf
      pwts[2] <<- pwts[2] + 0.5*b1exp[1,1]                          # log scale
      b1wts <<- logdet(b1DXtWX)                                     # log-determinant for normalizing constant
      pwts[2] <<- pwts[2] - 0.5*b1wts                               # already on log scale
      
    }                                                               # end cycle through groups
    
    ############################################################
    # Model weights for common trend models with no intercepts #
    ############################################################
    
    # prior weight for common quad trend model
    pwts[3] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf
    pwts[3] <<- pwts[3] - 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix
    pwts[3] <<- pwts[3] + 0.5*b2wts                                 # already on log scale
    
    # prior weight for common linear trend model
    pwts[4] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf
    pwts[4] <<- pwts[4] - 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix
    pwts[4] <<- pwts[4] + 0.5*b1wts                                 # already on log scale
    
    # cumulative posterior contributions over groups
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- ag[i]                                          # group-specific abeta vector
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # common quad trend model with no intercepts
      b2XtW <<- b2Xt %*% Wg                                         # multiply bXt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate bXtWX
      sumb2XtWX <<- sumb2XtWX + b2XtWX                              # cumulative matrix sum
      b2zbeta <<- b2XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb2zbeta <<- sumb2zbeta + b2zbeta                           # cumulative matrix sum
      
      # common linear trend model with no intercepts
      b1XtW <<- b1Xt %*% Wg                                         # multiply bXt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate bXtWX
      sumb1XtWX <<- sumb1XtWX + b1XtWX                              # cumulative matrix sum
      b1zbeta <<- b1XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb1zbeta <<- sumb1zbeta + b1zbeta                           # cumulative matrix sum
      
    }                                                               # end cycle through groups
    
    # posterior weight for common quad trend with no intercepts
    b2DXtWX <<- b2Dbetag + sumb2XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b2pbeta <<- b2prbeta + sumb2zbeta                               # sum of prior and the cumulative WLS contributions
    b2tpbeta <<- t(b2pbeta)                                         # transpose
    b2CI <<- inverse(b2DXtWX)                                       # inverse of posterior precision matrix
    b2pbeta <<- b2CI %*% b2pbeta                                    # re-scale psbeta
    b2exp <<- b2tpbeta %*% b2pbeta                                  # exponent from normal pdf
    pwts[3] <<- pwts[3] + 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2DXtWX)                                       # log-determinant for normalizing constant
    pwts[3] <<- pwts[3] - 0.5*b2wts                                 # already on log-scale
    
    # posterior weight for common linear trend with no intercepts
    b1DXtWX <<- b1Dbetag + sumb1XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b1pbeta <<- b1prbeta + sumb1zbeta                               # sum of prior and the cumulative WLS contributions
    b1tpbeta <<- t(b1pbeta)                                         # transpose
    b1CI <<- inverse(b1DXtWX)                                       # inverse of posterior precision matrix
    b1pbeta <<- b1CI %*% b1pbeta                                    # re-scale psbeta
    b1exp <<- b1tpbeta %*% b1pbeta                                  # exponent from normal pdf
    pwts[4] <<- pwts[4] + 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1DXtWX)                                       # log-determinant for normalizing constant
    pwts[4] <<- pwts[4] - 0.5*b1wts                                 # already on log-scale
    
    #############################################
    # Rescaled posterior model weights and draw #
    #############################################
    
    # calculated using differences on log-scale (Bayes factors) for numerical stability
    for (m in 1:fp) {
      qwtsum <- 0
      for (l in 1:fp) {
        qwtsum <- qwtsum + (wts[l]/wts[m])*exp(pwts[l]-pwts[m])
      }
      qwts[m] <<- 1/qwtsum
    }
    
    # check for any missing values
    qwtsum <- NaN
    if (any_na(qwts)) {                                             # set qwtsum to zero if any missings
      qwtsum <- 0
    }
    if (!is.na(qwtsum)) {                                           # if there were missing values
      for (m in 1:fp) {
        if (is.na(qwts[m])) {
          qwts[m] <<- 0                                             # replace them with zeroes
        }
        qwtsum <- qwtsum + qwts[m]
      }
      if (qwtsum > 0) {                                             # rescale to sum to 1
        for (m in 1:fp) {
          qwts[m] <<- qwts[m]/qwtsum
        }
      } else {                                                      # (qwtsum == 0) all weights are zero: use prior weights
        for (m in 1:fp) {
          qwts[m] <<- wts[m]
        }
      }
    }
    
    # sample from discrete posterior distribution of model flags (conditional on intercepts)
    flg <- as.integer(rcat(1, qwts))
    
    if (flg != model[['flg']]) {

      # Update model with new flag value
      model[['flg']] <<- flg
      
      # Re-calculate prior probabilities (in case they were not uniform)
      model$calculate(target)
      
      # Copy from model to mvSaved objects
      nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
      
    }
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_FP_bmaq (sampler_CPb_bmaq will update trend coefficients according to new 'flg' value)

# Custom Gibbs sampler for model flag, conditional on intercepts, in the quadratic BMA model with a level shift
sampler_FP_xptl_bmaq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Make sure the 'target' value is correctly specified
    if (lt != 1) {
      nimStop("Only the model flag 'flg' can be sampled using 'sampler_FP_xptl_bmaq'.")
    } else {
      if (target[1] != 'flg') {
        nimStop("Target node must be the model flag (integer) 'flg'.")
      }
    }
    
    # Get constants from model to use in calculations
    p <- model$getConstants()$p
    if (p != 4) {
      nimStop("Expecting design matrix to have 4 columns in 'sampler_FP_xptl_bmaq'.")
    }
    fp <- model$getConstants()$fp
    if (fp != 5) {
      nimStop("Expecting a total of 5 possible trend models in 'sampler_FP_xptl_bmaq'.")
    }
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    pwts <- nimNumeric(fp, init = FALSE)
    qwts <- nimNumeric(fp, init = FALSE)
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2tmbetag <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    b1tmbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    
    b2Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b2XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b2XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2tpbeta <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    b1tpbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    b1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b1exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2wts <- 0
    b1wts <- 0
    
    # Confirm target is a single scalar
    target <- model$expandNodeNames(target, returnScalarComponents = TRUE)
    if (length(target) > 1) {
      nimStop("Expecting a single model flag as target in 'sampler_FP_xptl_bmaq'.")
    }
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get prior model weights, intercepts, and current AR(1) parameters (draws are conditional on these values)
    wts <- model[['wts']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    l <- 0L
    m <- 0L
    irho <- 0
    inu <- 0
    qwtsum <- 0
    flg <- 0L
    
    ############################################################
    # Required conformal matrices for each possible model flag #
    ############################################################
    
    # intercepts
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,2]
    }
    
    # quad with no intercept
    for (j in 1:n) {
      for (k in 3:4) {
        b2X[j,k-2] <<- X[j,k]
      }
    }
    b2Dbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 3:4) {
      b2mbetag[k-2,1] <<- mbetag[k,1]
      b2Dbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    b2Xt <<- t(b2X)                                                 # transpose bX to use below
    b2prbeta <<- b2Dbetag %*% b2mbetag                              # contribution to posterior mean from prior
    
    sumb2XtWX <<- nimMatrix(0, 2L, 2L)                              # initialize cumulative sums to use in common case
    sumb2zbeta <<- nimMatrix(0, 2L, 1L)
    
    # linear with no intercept
    for (j in 1:n) {
      b1X[j,1] <<- X[j,3]
    }
    b1mbetag[1,1] <<- mbetag[3,1]
    b1Dbetag[1,1] <<- Dbetag[3,3]
    b1Xt <<- t(b1X)                                                 # transpose bX to use below
    b1prbeta <<- b1Dbetag %*% b1mbetag                              # contribution to posterior mean from prior
    
    sumb1XtWX[1,1] <<- 0                                            # initialize cumulative sums to use in common case
    sumb1zbeta[1,1] <<- 0
    
    ################################################################################
    # Model weights (log-scale) for group-specific trend models with no intercepts #
    ################################################################################
    
    # prior weight for intercepts-only model
    pwts[5] <<- 0                                                   # remains zero
    
    # prior weight for group-specific quad trend model
    pwts[1] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf features g times
    pwts[1] <<- pwts[1] - 0.5*g*b2exp[1,1]                          # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[1] <<- pwts[1] + 0.5*g*b2wts                               # already on log scale
    
    # prior weight for group-specific linear trend model
    pwts[2] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf features g times
    pwts[2] <<- pwts[2] - 0.5*g*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[2] <<- pwts[2] + 0.5*g*b1wts                               # already on log scale
    
    # posterior weights from group-specific contributions
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # group-specific quad trend model with no intercept
      b2XtW <<- b2Xt %*% Wg                                         # multiply Xt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate XtWX, the precision matrix from WLS
      b2DXtWX <<- b2Dbetag + b2XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b2zbeta <<- b2XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b2pbeta <<- b2prbeta + b2zbeta                                # sum of prior and WLS contributions
      b2tpbeta <<- t(b2pbeta)                                       # transpose
      b2CI <<- inverse(b2DXtWX)                                     # inverse of posterior precision matrix
      b2pbeta <<- b2CI %*% b2pbeta                                  # re-scale psbeta
      b2exp <<- b2tpbeta %*% b2pbeta                                # exponent from normal pdf
      pwts[1] <<- pwts[1] + 0.5*b2exp[1,1]                          # log scale
      b2wts <<- logdet(b2DXtWX)                                     # log-determinant for normalizing constant
      pwts[1] <<- pwts[1] - 0.5*b2wts                               # already on log scale
      
      # group-specific linear trend model with no intercept
      b1XtW <<- b1Xt %*% Wg                                         # multiply Xt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate XtWX, the precision matrix from WLS
      b1DXtWX <<- b1Dbetag + b1XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b1zbeta <<- b1XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b1pbeta <<- b1prbeta + b1zbeta                                # sum of prior and WLS contributions
      b1tpbeta <<- t(b1pbeta)                                       # transpose
      b1CI <<- inverse(b1DXtWX)                                     # inverse of posterior precision matrix
      b1pbeta <<- b1CI %*% b1pbeta                                  # re-scale psbeta
      b1exp <<- b1tpbeta %*% b1pbeta                                # exponent from normal pdf
      pwts[2] <<- pwts[2] + 0.5*b1exp[1,1]                          # log scale
      b1wts <<- logdet(b1DXtWX)                                     # log-determinant for normalizing constant
      pwts[2] <<- pwts[2] - 0.5*b1wts                               # already on log scale
      
    }                                                               # end cycle through groups
    
    ############################################################
    # Model weights for common trend models with no intercepts #
    ############################################################
    
    # prior weight for common quad trend model
    pwts[3] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf
    pwts[3] <<- pwts[3] - 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix
    pwts[3] <<- pwts[3] + 0.5*b2wts                                 # already on log scale
    
    # prior weight for common linear trend model
    pwts[4] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf
    pwts[4] <<- pwts[4] - 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix
    pwts[4] <<- pwts[4] + 0.5*b1wts                                 # already on log scale
    
    # cumulative posterior contributions over groups
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # common quad trend model with no intercepts
      b2XtW <<- b2Xt %*% Wg                                         # multiply bXt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate bXtWX
      sumb2XtWX <<- sumb2XtWX + b2XtWX                              # cumulative matrix sum
      b2zbeta <<- b2XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb2zbeta <<- sumb2zbeta + b2zbeta                           # cumulative matrix sum
      
      # common linear trend model with no intercepts
      b1XtW <<- b1Xt %*% Wg                                         # multiply bXt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate bXtWX
      sumb1XtWX <<- sumb1XtWX + b1XtWX                              # cumulative matrix sum
      b1zbeta <<- b1XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb1zbeta <<- sumb1zbeta + b1zbeta                           # cumulative matrix sum
      
    }                                                               # end cycle through groups
    
    # posterior weight for common quad trend with no intercepts
    b2DXtWX <<- b2Dbetag + sumb2XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b2pbeta <<- b2prbeta + sumb2zbeta                               # sum of prior and the cumulative WLS contributions
    b2tpbeta <<- t(b2pbeta)                                         # transpose
    b2CI <<- inverse(b2DXtWX)                                       # inverse of posterior precision matrix
    b2pbeta <<- b2CI %*% b2pbeta                                    # re-scale psbeta
    b2exp <<- b2tpbeta %*% b2pbeta                                  # exponent from normal pdf
    pwts[3] <<- pwts[3] + 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2DXtWX)                                       # log-determinant for normalizing constant
    pwts[3] <<- pwts[3] - 0.5*b2wts                                 # already on log-scale
    
    # posterior weight for common linear trend with no intercepts
    b1DXtWX <<- b1Dbetag + sumb1XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b1pbeta <<- b1prbeta + sumb1zbeta                               # sum of prior and the cumulative WLS contributions
    b1tpbeta <<- t(b1pbeta)                                         # transpose
    b1CI <<- inverse(b1DXtWX)                                       # inverse of posterior precision matrix
    b1pbeta <<- b1CI %*% b1pbeta                                    # re-scale psbeta
    b1exp <<- b1tpbeta %*% b1pbeta                                  # exponent from normal pdf
    pwts[4] <<- pwts[4] + 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1DXtWX)                                       # log-determinant for normalizing constant
    pwts[4] <<- pwts[4] - 0.5*b1wts                                 # already on log-scale
    
    #############################################
    # Rescaled posterior model weights and draw #
    #############################################
    
    # calculated using differences on log-scale (Bayes factors) for numerical stability
    for (m in 1:fp) {
      qwtsum <- 0
      for (l in 1:fp) {
        qwtsum <- qwtsum + (wts[l]/wts[m])*exp(pwts[l]-pwts[m])
      }
      qwts[m] <<- 1/qwtsum
    }
    
    # check for any missing values
    qwtsum <- NaN
    if (any_na(qwts)) {                                             # set qwtsum to zero if any missings
      qwtsum <- 0
    }
    if (!is.na(qwtsum)) {                                           # if there were missing values
      for (m in 1:fp) {
        if (is.na(qwts[m])) {
          qwts[m] <<- 0                                             # replace them with zeroes
        }
        qwtsum <- qwtsum + qwts[m]
      }
      if (qwtsum > 0) {                                             # rescale to sum to 1
        for (m in 1:fp) {
          qwts[m] <<- qwts[m]/qwtsum
        }
      } else {                                                      # (qwtsum == 0) all weights are zero: use prior weights
        for (m in 1:fp) {
          qwts[m] <<- wts[m]
        }
      }
    }
    
    # sample from discrete posterior distribution of model flags (conditional on intercepts)
    flg <- as.integer(rcat(1, qwts))
    
    if (flg != model[['flg']]) {
      
      # Update model with new flag value
      model[['flg']] <<- flg
      
      # Re-calculate prior probabilities (in case they were not uniform)
      model$calculate(target)
      
      # Copy from model to mvSaved objects
      nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
      
    }
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_FP_xptl_bmaq (sampler_CPb_xptl_bmaq will update trend coefficients according to new 'flg' value)

# Custom Gibbs sampler for model flags, conditional on intercepts, in the quadratic-quadratic BMA model with a full trend break
sampler_FP_xptf_bmaq_bmaq <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Make sure the 'target' value is correctly specified
    if (lt != 1) {
      nimStop("Only the model flag 'flg' can be sampled using 'sampler_FP_xptf_bmaq_bmaq'. Segment-specific flags 's1flg' and 's2flg' are derived deterministically from 'flg'.")
    } else {
      if (target[1] != 'flg') {
        nimStop("Target node must be the model flag (integer) 'flg'.")
      }
    }
    
    # Get constants from model to use in calculations
    s1pp2 <- 0L
    s1pm1 <- 0L
    s1p <- model$getConstants()$s1p
    s1pm1 <- s1p - 1
    s1pp2 <- s1p + 2
    pm1 <- 0L
    p <- model$getConstants()$p
    pm1 <- p-1
    if (p != 6) {
      nimStop("Expecting design matrix to have 6 columns in 'sampler_FP_xptf_bmaq_bmaq'.")
    }
    fp <- model$getConstants()$fp
    if (fp != 7) {
      nimStop("Expecting a total of 7 possible trend models in 'sampler_FP_xptf_bmaq_bmaq'.")
    }
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    pwts <- nimNumeric(fp, init = FALSE)
    qwts <- nimNumeric(fp, init = FALSE)
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb3XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    sumb2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    sumb3zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    sumb2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b3mbetag <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2mbetag <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b3tmbetag <- nimMatrix(nrow=1L, ncol=4L, init = FALSE)
    b2tmbetag <- nimMatrix(nrow=1L, ncol=3L, init = FALSE)
    b1tmbetag <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    
    b3Dbetag <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2Dbetag <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b3X <- nimMatrix(nrow=n, ncol=4L, init = FALSE)
    b2X <- nimMatrix(nrow=n, ncol=3L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    
    b3Xt <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2Xt <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b3XtW <- nimMatrix(nrow=4L, ncol=n, init = FALSE)
    b2XtW <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b3XtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b3DXtWX <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2DXtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b3prbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2prbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b3pbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2pbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b3tpbeta <- nimMatrix(nrow=1L, ncol=4L, init = FALSE)
    b2tpbeta <- nimMatrix(nrow=1L, ncol=3L, init = FALSE)
    b1tpbeta <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    
    b3zbeta <- nimMatrix(nrow=4L, ncol=1L, init = FALSE)
    b2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b3CI <- nimMatrix(nrow=4L, ncol=4L, init = FALSE)
    b2CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b3exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b2exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b1exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b3wts <- 0
    b2wts <- 0
    b1wts <- 0
    
    # Confirm target is scalar
    target <- model$expandNodeNames(target, returnScalarComponents = TRUE)
    if (length(target) > 1) {
      nimStop("Expecting a single flag as the target in 'sampler_FP_xptf_bmaq_bmaq'.")
    }
    
    # Get dependent nodes 's1flg' and 's2flg'
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get prior model weights, intercepts, and current AR(1) parameters (draws are conditional on these values)
    wts <- model[['wts']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    l <- 0L
    m <- 0L
    irho <- 0
    inu <- 0
    qwtsum <- 0
    s1flg <- 0L
    s2flg <- 0L
    flg <- 0L
    
    ############################################################
    # Required conformal matrices for each possible model flag #
    ############################################################
    
    # intercepts
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    
    # quadratic-quadratic with no intercept
    for (j in 1:n) {
      for (k in 2:s1p) {
        b3X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        b3X[j,k-2] <<- X[j,k]
      }
    }
    b3Dbetag <<- nimMatrix(0, 4L, 4L)
    for (k in 2:s1p) {
      b3mbetag[k-1,1] <<- mbetag[k,1]
      b3Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      b3mbetag[k-2,1] <<- mbetag[k,1]
      b3Dbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    b3Xt <<- t(b3X)                                                 # transpose bX to use below
    b3prbeta <<- b3Dbetag %*% b3mbetag                              # contribution to posterior mean from prior
    
    sumb3XtWX <<- nimMatrix(0, 4L, 4L)                              # initialize cumulative sums to use in common case
    sumb3zbeta <<- nimMatrix(0, 4L, 1L)
    
    # quadratic-linear with no intercept
    for (j in 1:n) {
      for (k in 2:s1p) {
        b2X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:pm1) {
        b2X[j,k-2] <<- X[j,k]
      }
    }
    b2Dbetag <<- nimMatrix(0, 3L, 3L)
    for (k in 2:s1p) {
      b2mbetag[k-1,1] <<- mbetag[k,1]
      b2Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:pm1) {
      b2mbetag[k-2,1] <<- mbetag[k,1]
      b2Dbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    b2Xt <<- t(b2X)                                                 # transpose bX to use below
    b2prbeta <<- b2Dbetag %*% b2mbetag                              # contribution to posterior mean from prior
    
    sumb2XtWX <<- nimMatrix(0, 3L, 3L)                              # initialize cumulative sums to use in common case
    sumb2zbeta <<- nimMatrix(0, 3L, 1L)
    
    # linear-linear with no intercept
    for (j in 1:n) {
      for (k in 2:s1pm1) {
        b1X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:pm1) {
        b1X[j,k-3] <<- X[j,k]
      }
    }
    b1Dbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 2:s1pm1) {
      b1mbetag[k-1,1] <<- mbetag[k,1]
      b1Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:pm1) {
      b1mbetag[k-3,1] <<- mbetag[k,1]
      b1Dbetag[k-3,k-3] <<- Dbetag[k,k]
    }
    b1Xt <<- t(b1X)                                                 # transpose bX to use below
    b1prbeta <<- b1Dbetag %*% b1mbetag                              # contribution to posterior mean from prior
    
    sumb1XtWX <<- nimMatrix(0, 2L, 2L)                              # initialize cumulative sums to use in common case
    sumb1zbeta <<- nimMatrix(0, 2L, 1L)
    
    ################################################################################
    # Model weights (log-scale) for group-specific trend models with no intercepts #
    ################################################################################
    
    # prior weight for intercepts-only model
    pwts[7] <<- 0                                                   # remains zero
    
    # prior weight for group-specific quad-quad trend model
    pwts[1] <<- 0
    b3tmbetag <<- t(b3mbetag)
    b3exp <<- b3tmbetag %*% b3prbeta                                # exponent from normal pdf features g times
    pwts[1] <<- pwts[1] - 0.5*g*b3exp[1,1]                          # log scale
    b3wts <<- logdet(b3Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[1] <<- pwts[1] + 0.5*g*b3wts                               # already on log scale
    
    # prior weight for group-specific quad-linear trend model
    pwts[2] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf features g times
    pwts[2] <<- pwts[2] - 0.5*g*b2exp[1,1]                          # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[2] <<- pwts[2] + 0.5*g*b2wts                               # already on log scale
    
    # prior weight for group-specific linear-linear trend model
    pwts[3] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf features g times
    pwts[3] <<- pwts[3] - 0.5*g*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[3] <<- pwts[3] + 0.5*g*b1wts                               # already on log scale
    
    # posterior weights from group-specific contributions
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # group-specific quad-quad trend model with no intercept
      b3XtW <<- b3Xt %*% Wg                                         # multiply Xt and Wg
      b3XtWX <<- b3XtW %*% b3X                                      # calculate XtWX, the precision matrix from WLS
      b3DXtWX <<- b3Dbetag + b3XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b3zbeta <<- b3XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b3pbeta <<- b3prbeta + b3zbeta                                # sum of prior and WLS contributions
      b3tpbeta <<- t(b3pbeta)                                       # transpose
      b3CI <<- inverse(b3DXtWX)                                     # inverse of posterior precision matrix
      b3pbeta <<- b3CI %*% b3pbeta                                  # re-scale psbeta
      b3exp <<- b3tpbeta %*% b3pbeta                                # exponent from normal pdf
      pwts[1] <<- pwts[1] + 0.5*b3exp[1,1]                          # log scale
      b3wts <<- logdet(b3DXtWX)                                     # log-determinant for normalizing constant
      pwts[1] <<- pwts[1] - 0.5*b3wts                               # already on log scale
      
      # group-specific quad-linear trend model with no intercept
      b2XtW <<- b2Xt %*% Wg                                         # multiply Xt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate XtWX, the precision matrix from WLS
      b2DXtWX <<- b2Dbetag + b2XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b2zbeta <<- b2XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b2pbeta <<- b2prbeta + b2zbeta                                # sum of prior and WLS contributions
      b2tpbeta <<- t(b2pbeta)                                       # transpose
      b2CI <<- inverse(b2DXtWX)                                     # inverse of posterior precision matrix
      b2pbeta <<- b2CI %*% b2pbeta                                  # re-scale psbeta
      b2exp <<- b2tpbeta %*% b2pbeta                                # exponent from normal pdf
      pwts[2] <<- pwts[2] + 0.5*b2exp[1,1]                          # log scale
      b2wts <<- logdet(b2DXtWX)                                     # log-determinant for normalizing constant
      pwts[2] <<- pwts[2] - 0.5*b2wts                               # already on log scale
      
      # group-specific linear-linear trend model with no intercept
      b1XtW <<- b1Xt %*% Wg                                         # multiply Xt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate XtWX, the precision matrix from WLS
      b1DXtWX <<- b1Dbetag + b1XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b1zbeta <<- b1XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b1pbeta <<- b1prbeta + b1zbeta                                # sum of prior and WLS contributions
      b1tpbeta <<- t(b1pbeta)                                       # transpose
      b1CI <<- inverse(b1DXtWX)                                     # inverse of posterior precision matrix
      b1pbeta <<- b1CI %*% b1pbeta                                  # re-scale psbeta
      b1exp <<- b1tpbeta %*% b1pbeta                                # exponent from normal pdf
      pwts[3] <<- pwts[3] + 0.5*b1exp[1,1]                          # log scale
      b1wts <<- logdet(b1DXtWX)                                     # log-determinant for normalizing constant
      pwts[3] <<- pwts[3] - 0.5*b1wts                               # already on log scale
      
    }                                                               # end cycle through groups
    
    ############################################################
    # Model weights for common trend models with no intercepts #
    ############################################################
    
    # prior weight for common quad-quad trend model
    pwts[4] <<- 0
    b3tmbetag <<- t(b3mbetag)
    b3exp <<- b3tmbetag %*% b3prbeta                                # exponent from normal pdf
    pwts[4] <<- pwts[4] - 0.5*b3exp[1,1]                            # log scale
    b3wts <<- logdet(b3Dbetag)                                      # log-determinant of prior precision matrix
    pwts[4] <<- pwts[4] + 0.5*b3wts                                 # already on log scale
    
    # prior weight for common quad-linear trend model
    pwts[5] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf
    pwts[5] <<- pwts[5] - 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix
    pwts[5] <<- pwts[5] + 0.5*b2wts                                 # already on log scale
    
    # prior weight for common linear-linear trend model
    pwts[6] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf
    pwts[6] <<- pwts[6] - 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix
    pwts[6] <<- pwts[6] + 0.5*b1wts                                 # already on log scale
    
    # cumulative posterior contributions over groups
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # common quad-quad trend model with no intercepts
      b3XtW <<- b3Xt %*% Wg                                         # multiply bXt and Wg
      b3XtWX <<- b3XtW %*% b3X                                      # calculate bXtWX
      sumb3XtWX <<- sumb3XtWX + b3XtWX                              # cumulative matrix sum
      b3zbeta <<- b3XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb3zbeta <<- sumb3zbeta + b3zbeta                           # cumulative matrix sum
      
      # common quad-linear trend model with no intercepts
      b2XtW <<- b2Xt %*% Wg                                         # multiply bXt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate bXtWX
      sumb2XtWX <<- sumb2XtWX + b2XtWX                              # cumulative matrix sum
      b2zbeta <<- b2XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb2zbeta <<- sumb2zbeta + b2zbeta                           # cumulative matrix sum
      
      # common linear-linear trend model with no intercepts
      b1XtW <<- b1Xt %*% Wg                                         # multiply bXt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate bXtWX
      sumb1XtWX <<- sumb1XtWX + b1XtWX                              # cumulative matrix sum
      b1zbeta <<- b1XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb1zbeta <<- sumb1zbeta + b1zbeta                           # cumulative matrix sum
      
    }                                                               # end cycle through groups
    
    # posterior weight for common quad-quad trend with no intercepts
    b3DXtWX <<- b3Dbetag + sumb3XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b3pbeta <<- b3prbeta + sumb3zbeta                               # sum of prior and the cumulative WLS contributions
    b3tpbeta <<- t(b3pbeta)                                         # transpose
    b3CI <<- inverse(b3DXtWX)                                       # inverse of posterior precision matrix
    b3pbeta <<- b3CI %*% b3pbeta                                    # re-scale psbeta
    b3exp <<- b3tpbeta %*% b3pbeta                                  # exponent from normal pdf
    pwts[4] <<- pwts[4] + 0.5*b3exp[1,1]                            # log scale
    b3wts <<- logdet(b3DXtWX)                                       # log-determinant for normalizing constant
    pwts[4] <<- pwts[4] - 0.5*b3wts                                 # already on log-scale
    
    # posterior weight for common quad-linear trend with no intercepts
    b2DXtWX <<- b2Dbetag + sumb2XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b2pbeta <<- b2prbeta + sumb2zbeta                               # sum of prior and the cumulative WLS contributions
    b2tpbeta <<- t(b2pbeta)                                         # transpose
    b2CI <<- inverse(b2DXtWX)                                       # inverse of posterior precision matrix
    b2pbeta <<- b2CI %*% b2pbeta                                    # re-scale psbeta
    b2exp <<- b2tpbeta %*% b2pbeta                                  # exponent from normal pdf
    pwts[5] <<- pwts[5] + 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2DXtWX)                                       # log-determinant for normalizing constant
    pwts[5] <<- pwts[5] - 0.5*b2wts                                 # already on log-scale
    
    # posterior weight for common linear-linear trend with no intercepts
    b1DXtWX <<- b1Dbetag + sumb1XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b1pbeta <<- b1prbeta + sumb1zbeta                               # sum of prior and the cumulative WLS contributions
    b1tpbeta <<- t(b1pbeta)                                         # transpose
    b1CI <<- inverse(b1DXtWX)                                       # inverse of posterior precision matrix
    b1pbeta <<- b1CI %*% b1pbeta                                    # re-scale psbeta
    b1exp <<- b1tpbeta %*% b1pbeta                                  # exponent from normal pdf
    pwts[6] <<- pwts[6] + 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1DXtWX)                                       # log-determinant for normalizing constant
    pwts[6] <<- pwts[6] - 0.5*b1wts                                 # already on log-scale
    
    #############################################
    # Rescaled posterior model weights and draw #
    #############################################
    
    # calculated using differences on log-scale (a.k.a., Bayes factors) for numerical stability
    for (m in 1:fp) {
      qwtsum <- 0
      for (l in 1:fp) {
        qwtsum <- qwtsum + (wts[l]/wts[m])*exp(pwts[l]-pwts[m])
      }
      qwts[m] <<- 1/qwtsum
    }
    
    # check for any missing values
    qwtsum <- NaN
    if (any_na(qwts)) {                                             # set qwtsum to zero if any missings
      qwtsum <- 0
    }
    if (!is.na(qwtsum)) {                                           # if there were missing values
      for (m in 1:fp) {
        if (is.na(qwts[m])) {
          qwts[m] <<- 0                                             # replace them with zeroes
        }
        qwtsum <- qwtsum + qwts[m]
      }
      if (qwtsum > 0) {                                             # rescale to sum to 1
        for (m in 1:fp) {
          qwts[m] <<- qwts[m]/qwtsum
        }
      } else {                                                      # (qwtsum == 0) all weights are zero: use prior weights
        for (m in 1:fp) {
          qwts[m] <<- wts[m]
        }
      }
    }
    
    # sample from discrete posterior distribution of model flags (conditional on intercepts)
    flg <- as.integer(rcat(1, qwts))
    
    if (flg != model[['flg']]) {
      
      # Update model with new flag value(s)
      model[['flg']] <<- flg
      
      # Re-calculate prior probabilities (in case they were not uniform)
      model$calculate(target)
      
      # Re-calculate segment-specific flags 's1flg' and 's2flg'
      model$calculate(deterministicDependents)
      
      # Copy from model to mvSaved objects
      nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
      nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
      
    }
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_FP_xptf_bmaq_bmaq (sampler_CPb_xptf_bmaq_bmaq will update trend coefficients according to new 's1flg' and 's2flg' values)

# Custom Gibbs sampler for model flags, conditional on intercepts, in the quadratic-linear BMA model with a full trend break
sampler_FP_xptf_bmaq_bmal <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Make sure the 'target' value is correctly specified
    if (lt != 1) {
      nimStop("Only the model flag 'flg' can be sampled using 'sampler_FP_xptf_bmaq_bmal'. Segment-specific flags 's1flg' and 's2flg' are derived deterministically from 'flg'.")
    } else {
      if (target[1] != 'flg') {
        nimStop("Target node must be the model flag (integer) 'flg'.")
      }
    }
    
    # Get constants from model to use in calculations
    s1pp2 <- 0L
    s1pm1 <- 0L
    s1p <- model$getConstants()$s1p
    s1pm1 <- s1p - 1
    s1pp2 <- s1p + 2
    p <- model$getConstants()$p
    if (p != 5) {
      nimStop("Expecting design matrix to have 5 columns in 'sampler_FP_xptf_bmaq_bmal'.")
    }
    fp <- model$getConstants()$fp
    if (fp != 5) {
      nimStop("Expecting a total of 5 possible trend models in 'sampler_FP_xptf_bmaq_bmal'.")
    }
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    pwts <- nimNumeric(fp, init = FALSE)
    qwts <- nimNumeric(fp, init = FALSE)
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    sumb1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    sumb2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    sumb1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b2mbetag <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b2tmbetag <- nimMatrix(nrow=1L, ncol=3L, init = FALSE)
    b1tmbetag <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    
    b2Dbetag <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b2X <- nimMatrix(nrow=n, ncol=3L, init = FALSE)
    b1X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    
    b2Xt <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b2XtW <- nimMatrix(nrow=3L, ncol=n, init = FALSE)
    b1XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b2XtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b2DXtWX <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b2prbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b2pbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b2tpbeta <- nimMatrix(nrow=1L, ncol=3L, init = FALSE)
    b1tpbeta <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    
    b2zbeta <- nimMatrix(nrow=3L, ncol=1L, init = FALSE)
    b1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b2CI <- nimMatrix(nrow=3L, ncol=3L, init = FALSE)
    b1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b2exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    b1exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b2wts <- 0
    b1wts <- 0
    
    # Confirm target is scalar
    target <- model$expandNodeNames(target, returnScalarComponents = TRUE)
    if (length(target) > 1) {
      nimStop("Expecting a single flag as the target in 'sampler_FP_xptf_bmaq_bmal'.")
    }
    
    # Get dependent nodes 's1flg' and 's2flg'
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get prior model weights, intercepts, and current AR(1) parameters (draws are conditional on these values)
    wts <- model[['wts']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    l <- 0L
    m <- 0L
    irho <- 0
    inu <- 0
    qwtsum <- 0
    s1flg <- 0L
    s2flg <- 0L
    flg <- 0L
    
    ############################################################
    # Required conformal matrices for each possible model flag #
    ############################################################
    
    # intercepts
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    
    # quadratic-linear with no intercept
    for (j in 1:n) {
      for (k in 2:s1p) {
        b2X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        b2X[j,k-2] <<- X[j,k]
      }
    }
    b2Dbetag <<- nimMatrix(0, 3L, 3L)
    for (k in 2:s1p) {
      b2mbetag[k-1,1] <<- mbetag[k,1]
      b2Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      b2mbetag[k-2,1] <<- mbetag[k,1]
      b2Dbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    b2Xt <<- t(b2X)                                                 # transpose bX to use below
    b2prbeta <<- b2Dbetag %*% b2mbetag                              # contribution to posterior mean from prior
    
    sumb2XtWX <<- nimMatrix(0, 3L, 3L)                              # initialize cumulative sums to use in common case
    sumb2zbeta <<- nimMatrix(0, 3L, 1L)
    
    # linear-linear with no intercept
    for (j in 1:n) {
      for (k in 2:s1pm1) {
        b1X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        b1X[j,k-3] <<- X[j,k]
      }
    }
    b1Dbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 2:s1pm1) {
      b1mbetag[k-1,1] <<- mbetag[k,1]
      b1Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      b1mbetag[k-3,1] <<- mbetag[k,1]
      b1Dbetag[k-3,k-3] <<- Dbetag[k,k]
    }
    b1Xt <<- t(b1X)                                                 # transpose bX to use below
    b1prbeta <<- b1Dbetag %*% b1mbetag                              # contribution to posterior mean from prior
    
    sumb1XtWX <<- nimMatrix(0, 2L, 2L)                              # initialize cumulative sums to use in common case
    sumb1zbeta <<- nimMatrix(0, 2L, 1L)
    
    ################################################################################
    # Model weights (log-scale) for group-specific trend models with no intercepts #
    ################################################################################
    
    # prior weight for intercepts-only model
    pwts[5] <<- 0                                                   # remains zero
    
    # prior weight for group-specific quad-linear trend model
    pwts[1] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf features g times
    pwts[1] <<- pwts[1] - 0.5*g*b2exp[1,1]                          # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[1] <<- pwts[1] + 0.5*g*b2wts                               # already on log scale
    
    # prior weight for group-specific linear-linear trend model
    pwts[2] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf features g times
    pwts[2] <<- pwts[2] - 0.5*g*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[2] <<- pwts[2] + 0.5*g*b1wts                               # already on log scale
    
    # posterior weights from group-specific contributions
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # group-specific quad-linear trend model with no intercept
      b2XtW <<- b2Xt %*% Wg                                         # multiply Xt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate XtWX, the precision matrix from WLS
      b2DXtWX <<- b2Dbetag + b2XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b2zbeta <<- b2XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b2pbeta <<- b2prbeta + b2zbeta                                # sum of prior and WLS contributions
      b2tpbeta <<- t(b2pbeta)                                       # transpose
      b2CI <<- inverse(b2DXtWX)                                     # inverse of posterior precision matrix
      b2pbeta <<- b2CI %*% b2pbeta                                  # re-scale psbeta
      b2exp <<- b2tpbeta %*% b2pbeta                                # exponent from normal pdf
      pwts[1] <<- pwts[1] + 0.5*b2exp[1,1]                          # log scale
      b2wts <<- logdet(b2DXtWX)                                     # log-determinant for normalizing constant
      pwts[1] <<- pwts[1] - 0.5*b2wts                               # already on log scale
      
      # group-specific linear-linear trend model with no intercept
      b1XtW <<- b1Xt %*% Wg                                         # multiply Xt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate XtWX, the precision matrix from WLS
      b1DXtWX <<- b1Dbetag + b1XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b1zbeta <<- b1XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b1pbeta <<- b1prbeta + b1zbeta                                # sum of prior and WLS contributions
      b1tpbeta <<- t(b1pbeta)                                       # transpose
      b1CI <<- inverse(b1DXtWX)                                     # inverse of posterior precision matrix
      b1pbeta <<- b1CI %*% b1pbeta                                  # re-scale psbeta
      b1exp <<- b1tpbeta %*% b1pbeta                                # exponent from normal pdf
      pwts[2] <<- pwts[2] + 0.5*b1exp[1,1]                          # log scale
      b1wts <<- logdet(b1DXtWX)                                     # log-determinant for normalizing constant
      pwts[2] <<- pwts[2] - 0.5*b1wts                               # already on log scale
      
    }                                                               # end cycle through groups
    
    ############################################################
    # Model weights for common trend models with no intercepts #
    ############################################################
    
    # prior weight for common quad-linear trend model
    pwts[3] <<- 0
    b2tmbetag <<- t(b2mbetag)
    b2exp <<- b2tmbetag %*% b2prbeta                                # exponent from normal pdf
    pwts[3] <<- pwts[3] - 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2Dbetag)                                      # log-determinant of prior precision matrix
    pwts[3] <<- pwts[3] + 0.5*b2wts                                 # already on log scale
    
    # prior weight for common linear-linear trend model
    pwts[4] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf
    pwts[4] <<- pwts[4] - 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix
    pwts[4] <<- pwts[4] + 0.5*b1wts                                 # already on log scale
    
    # cumulative posterior contributions over groups
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # common quad-linear trend model with no intercepts
      b2XtW <<- b2Xt %*% Wg                                         # multiply bXt and Wg
      b2XtWX <<- b2XtW %*% b2X                                      # calculate bXtWX
      sumb2XtWX <<- sumb2XtWX + b2XtWX                              # cumulative matrix sum
      b2zbeta <<- b2XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb2zbeta <<- sumb2zbeta + b2zbeta                           # cumulative matrix sum
      
      # common linear-linear trend model with no intercepts
      b1XtW <<- b1Xt %*% Wg                                         # multiply bXt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate bXtWX
      sumb1XtWX <<- sumb1XtWX + b1XtWX                              # cumulative matrix sum
      b1zbeta <<- b1XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb1zbeta <<- sumb1zbeta + b1zbeta                           # cumulative matrix sum
      
    }                                                               # end cycle through groups
    
    # posterior weight for common quad-linear trend with no intercepts
    b2DXtWX <<- b2Dbetag + sumb2XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b2pbeta <<- b2prbeta + sumb2zbeta                               # sum of prior and the cumulative WLS contributions
    b2tpbeta <<- t(b2pbeta)                                         # transpose
    b2CI <<- inverse(b2DXtWX)                                       # inverse of posterior precision matrix
    b2pbeta <<- b2CI %*% b2pbeta                                    # re-scale psbeta
    b2exp <<- b2tpbeta %*% b2pbeta                                  # exponent from normal pdf
    pwts[3] <<- pwts[3] + 0.5*b2exp[1,1]                            # log scale
    b2wts <<- logdet(b2DXtWX)                                       # log-determinant for normalizing constant
    pwts[3] <<- pwts[3] - 0.5*b2wts                                 # already on log-scale
    
    # posterior weight for common linear-linear trend with no intercepts
    b1DXtWX <<- b1Dbetag + sumb1XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b1pbeta <<- b1prbeta + sumb1zbeta                               # sum of prior and the cumulative WLS contributions
    b1tpbeta <<- t(b1pbeta)                                         # transpose
    b1CI <<- inverse(b1DXtWX)                                       # inverse of posterior precision matrix
    b1pbeta <<- b1CI %*% b1pbeta                                    # re-scale psbeta
    b1exp <<- b1tpbeta %*% b1pbeta                                  # exponent from normal pdf
    pwts[4] <<- pwts[4] + 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1DXtWX)                                       # log-determinant for normalizing constant
    pwts[4] <<- pwts[4] - 0.5*b1wts                                 # already on log-scale
    
    #############################################
    # Rescaled posterior model weights and draw #
    #############################################
    
    # calculated using differences on log-scale (a.k.a., Bayes factors) for numerical stability
    for (m in 1:fp) {
      qwtsum <- 0
      for (l in 1:fp) {
        qwtsum <- qwtsum + (wts[l]/wts[m])*exp(pwts[l]-pwts[m])
      }
      qwts[m] <<- 1/qwtsum
    }
    
    # check for any missing values
    qwtsum <- NaN
    if (any_na(qwts)) {                                             # set qwtsum to zero if any missings
      qwtsum <- 0
    }
    if (!is.na(qwtsum)) {                                           # if there were missing values
      for (m in 1:fp) {
        if (is.na(qwts[m])) {
          qwts[m] <<- 0                                             # replace them with zeroes
        }
        qwtsum <- qwtsum + qwts[m]
      }
      if (qwtsum > 0) {                                             # rescale to sum to 1
        for (m in 1:fp) {
          qwts[m] <<- qwts[m]/qwtsum
        }
      } else {                                                      # (qwtsum == 0) all weights are zero: use prior weights
        for (m in 1:fp) {
          qwts[m] <<- wts[m]
        }
      }
    }
    
    # sample from discrete posterior distribution of model flags (conditional on intercepts)
    flg <- as.integer(rcat(1, qwts))
    
    if (flg != model[['flg']]) {
      
      # Update model with new flag value(s)
      model[['flg']] <<- flg
      
      # Re-calculate prior probabilities (in case they were not uniform)
      model$calculate(target)
      
      # Re-calculate segment-specific flags 's1flg' and 's2flg'
      model$calculate(deterministicDependents)
      
      # Copy from model to mvSaved objects
      nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
      nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
      
    }
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_FP_xptf_bmaq_bmal (sampler_CPb_xptf_bmaq_bmal will update trend coefficients according to new 's1flg' and 's2flg' values)

# Custom Gibbs sampler for model flag, conditional on intercepts, in the linear BMA model
sampler_FP_bmal <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Make sure the 'target' value is correctly specified
    if (lt != 1) {
      nimStop("Only the model flag 'flg' can be sampled using 'sampler_FP_bmal'.")
    } else {
      if (target[1] != 'flg') {
        nimStop("Target node must be the model flag (integer) 'flg'.")
      }
    }
    
    # Get constants from model to use in calculations
    p <- model$getConstants()$p
    if (p != 2) {
      nimStop("Expecting design matrix to have 2 columns in 'sampler_FP_bmal'.")
    }
    fp <- model$getConstants()$fp
    if (fp != 3) {
      nimStop("Expecting a total of 3 possible trend models in 'sampler_FP_bmal'.")
    }
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    pwts <- nimNumeric(fp, init = FALSE)
    qwts <- nimNumeric(fp, init = FALSE)
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1mbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1tmbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1Dbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1X <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    
    b1Xt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b1XtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1DXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1prbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1pbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1tpbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1wts <- 0
    
    # Confirm target is a single scalar
    target <- model$expandNodeNames(target, returnScalarComponents = TRUE)
    if (length(target) > 1) {
      nimStop("Expecting a single model flag as target in 'sampler_FP_bmal'.")
    }
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get prior model weights, intercepts, and current AR(1) parameters (draws are conditional on these values)
    wts <- model[['wts']]
    ag <- model[['ag']]
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    l <- 0L
    m <- 0L
    irho <- 0
    inu <- 0
    qwtsum <- 0
    flg <- 0L
    
    ############################################################
    # Required conformal matrices for each possible model flag #
    ############################################################
    
    # intercepts
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
    }
    
    # linear with no intercept
    for (j in 1:n) {
      b1X[j,1] <<- X[j,2]
    }
    b1mbetag[1,1] <<- mbetag[2,1]
    b1Dbetag[1,1] <<- Dbetag[2,2]
    b1Xt <<- t(b1X)                                                 # transpose bX to use below
    b1prbeta <<- b1Dbetag %*% b1mbetag                              # contribution to posterior mean from prior
    
    sumb1XtWX[1,1] <<- 0                                            # initialize cumulative sums to use in common case
    sumb1zbeta[1,1] <<- 0
    
    ################################################################################
    # Model weights (log-scale) for group-specific trend models with no intercepts #
    ################################################################################
    
    # prior weight for intercepts-only model
    pwts[3] <<- 0                                                   # remains zero
    
    # prior weight for group-specific linear trend model
    pwts[1] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf features g times
    pwts[1] <<- pwts[1] - 0.5*g*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[1] <<- pwts[1] + 0.5*g*b1wts                               # already on log scale
    
    # posterior weights from group-specific contributions
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- ag[i]                                          # group-specific abeta vector
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # group-specific linear trend model with no intercept
      b1XtW <<- b1Xt %*% Wg                                         # multiply Xt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate XtWX, the precision matrix from WLS
      b1DXtWX <<- b1Dbetag + b1XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b1zbeta <<- b1XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b1pbeta <<- b1prbeta + b1zbeta                                # sum of prior and WLS contributions
      b1tpbeta <<- t(b1pbeta)                                       # transpose
      b1CI <<- inverse(b1DXtWX)                                     # inverse of posterior precision matrix
      b1pbeta <<- b1CI %*% b1pbeta                                  # re-scale psbeta
      b1exp <<- b1tpbeta %*% b1pbeta                                # exponent from normal pdf
      pwts[1] <<- pwts[1] + 0.5*b1exp[1,1]                          # log scale
      b1wts <<- logdet(b1DXtWX)                                     # log-determinant for normalizing constant
      pwts[1] <<- pwts[1] - 0.5*b1wts                               # already on log scale
      
    }                                                               # end cycle through groups
    
    ############################################################
    # Model weights for common trend models with no intercepts #
    ############################################################
    
    # prior weight for common linear trend model
    pwts[2] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf
    pwts[2] <<- pwts[2] - 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix
    pwts[2] <<- pwts[2] + 0.5*b1wts                                 # already on log scale
    
    # cumulative posterior contributions over groups
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- ag[i]                                          # group-specific abeta vector
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # common linear trend model with no intercepts
      b1XtW <<- b1Xt %*% Wg                                         # multiply bXt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate bXtWX
      sumb1XtWX <<- sumb1XtWX + b1XtWX                              # cumulative matrix sum
      b1zbeta <<- b1XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb1zbeta <<- sumb1zbeta + b1zbeta                           # cumulative matrix sum
      
    }                                                               # end cycle through groups
    
    # posterior weight for common linear trend with no intercepts
    b1DXtWX <<- b1Dbetag + sumb1XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b1pbeta <<- b1prbeta + sumb1zbeta                               # sum of prior and the cumulative WLS contributions
    b1tpbeta <<- t(b1pbeta)                                         # transpose
    b1CI <<- inverse(b1DXtWX)                                       # inverse of posterior precision matrix
    b1pbeta <<- b1CI %*% b1pbeta                                    # re-scale psbeta
    b1exp <<- b1tpbeta %*% b1pbeta                                  # exponent from normal pdf
    pwts[2] <<- pwts[2] + 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1DXtWX)                                       # log-determinant for normalizing constant
    pwts[2] <<- pwts[2] - 0.5*b1wts                                 # already on log-scale
    
    #############################################
    # Rescaled posterior model weights and draw #
    #############################################
    
    # calculated using differences on log-scale (Bayes factors) for numerical stability
    for (m in 1:fp) {
      qwtsum <- 0
      for (l in 1:fp) {
        qwtsum <- qwtsum + (wts[l]/wts[m])*exp(pwts[l]-pwts[m])
      }
      qwts[m] <<- 1/qwtsum
    }
    
    # check for any missing values
    qwtsum <- NaN
    if (any_na(qwts)) {                                             # set qwtsum to zero if any missings
      qwtsum <- 0
    }
    if (!is.na(qwtsum)) {                                           # if there were missing values
      for (m in 1:fp) {
        if (is.na(qwts[m])) {
          qwts[m] <<- 0                                             # replace them with zeroes
        }
        qwtsum <- qwtsum + qwts[m]
      }
      if (qwtsum > 0) {                                             # rescale to sum to 1
        for (m in 1:fp) {
          qwts[m] <<- qwts[m]/qwtsum
        }
      } else {                                                      # (qwtsum == 0) all weights are zero: use prior weights
        for (m in 1:fp) {
          qwts[m] <<- wts[m]
        }
      }
    }
    
    # sample from discrete posterior distribution of model flags (conditional on intercepts)
    flg <- as.integer(rcat(1, qwts))
    
    if (flg != model[['flg']]) {
      
      # Update model with new flag value
      model[['flg']] <<- flg
      
      # Re-calculate prior probabilities (in case they were not uniform)
      model$calculate(target)
      
      # Copy from model to mvSaved objects
      nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
      
    }
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_FP_bmal (sampler_CPb_bmal will update trend coefficients according to new 'flg' value)

# Custom Gibbs sampler for model flag, conditional on intercepts, in the linear BMA model with a level shift
sampler_FP_xptl_bmal <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Make sure the 'target' value is correctly specified
    if (lt != 1) {
      nimStop("Only the model flag 'flg' can be sampled using 'sampler_FP_xptl_bmal'.")
    } else {
      if (target[1] != 'flg') {
        nimStop("Target node must be the model flag (integer) 'flg'.")
      }
    }
    
    # Get constants from model to use in calculations
    p <- model$getConstants()$p
    if (p != 3) {
      nimStop("Expecting design matrix to have 3 columns in 'sampler_FP_xptl_bmal'.")
    }
    fp <- model$getConstants()$fp
    if (fp != 3) {
      nimStop("Expecting a total of 3 possible trend models in 'sampler_FP_xptl_bmal'.")
    }
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    pwts <- nimNumeric(fp, init = FALSE)
    qwts <- nimNumeric(fp, init = FALSE)
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    sumb1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1mbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1tmbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1Dbetag <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1X <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    
    b1Xt <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b1XtW <- nimMatrix(nrow=1L, ncol=n, init = FALSE)
    
    b1XtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1DXtWX <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1prbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1pbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1tpbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1zbeta <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1CI <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1wts <- 0
    
    # Confirm target is a single scalar
    target <- model$expandNodeNames(target, returnScalarComponents = TRUE)
    if (length(target) > 1) {
      nimStop("Expecting a single model flag as target in 'sampler_FP_xptl_bmal'.")
    }
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get prior model weights, intercepts, and current AR(1) parameters (draws are conditional on these values)
    wts <- model[['wts']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    l <- 0L
    m <- 0L
    irho <- 0
    inu <- 0
    qwtsum <- 0
    flg <- 0L
    
    ############################################################
    # Required conformal matrices for each possible model flag #
    ############################################################
    
    # intercepts
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,2]
    }
    
    # linear with no intercept
    for (j in 1:n) {
      b1X[j,1] <<- X[j,3]
    }
    b1mbetag[1,1] <<- mbetag[3,1]
    b1Dbetag[1,1] <<- Dbetag[3,3]
    b1Xt <<- t(b1X)                                                 # transpose bX to use below
    b1prbeta <<- b1Dbetag %*% b1mbetag                              # contribution to posterior mean from prior
    
    sumb1XtWX[1,1] <<- 0                                            # initialize cumulative sums to use in common case
    sumb1zbeta[1,1] <<- 0
    
    ################################################################################
    # Model weights (log-scale) for group-specific trend models with no intercepts #
    ################################################################################
    
    # prior weight for intercepts-only model
    pwts[3] <<- 0                                                   # remains zero
    
    # prior weight for group-specific linear trend model
    pwts[1] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf features g times
    pwts[1] <<- pwts[1] - 0.5*g*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[1] <<- pwts[1] + 0.5*g*b1wts                               # already on log scale
    
    # posterior weights from group-specific contributions
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # group-specific linear trend model with no intercept
      b1XtW <<- b1Xt %*% Wg                                         # multiply Xt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate XtWX, the precision matrix from WLS
      b1DXtWX <<- b1Dbetag + b1XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b1zbeta <<- b1XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b1pbeta <<- b1prbeta + b1zbeta                                # sum of prior and WLS contributions
      b1tpbeta <<- t(b1pbeta)                                       # transpose
      b1CI <<- inverse(b1DXtWX)                                     # inverse of posterior precision matrix
      b1pbeta <<- b1CI %*% b1pbeta                                  # re-scale psbeta
      b1exp <<- b1tpbeta %*% b1pbeta                                # exponent from normal pdf
      pwts[1] <<- pwts[1] + 0.5*b1exp[1,1]                          # log scale
      b1wts <<- logdet(b1DXtWX)                                     # log-determinant for normalizing constant
      pwts[1] <<- pwts[1] - 0.5*b1wts                               # already on log scale
      
    }                                                               # end cycle through groups
    
    ############################################################
    # Model weights for common trend models with no intercepts #
    ############################################################
    
    # prior weight for common linear trend model
    pwts[2] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf
    pwts[2] <<- pwts[2] - 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix
    pwts[2] <<- pwts[2] + 0.5*b1wts                                 # already on log scale
    
    # cumulative posterior contributions over groups
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # common linear trend model with no intercepts
      b1XtW <<- b1Xt %*% Wg                                         # multiply bXt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate bXtWX
      sumb1XtWX <<- sumb1XtWX + b1XtWX                              # cumulative matrix sum
      b1zbeta <<- b1XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb1zbeta <<- sumb1zbeta + b1zbeta                           # cumulative matrix sum
      
    }                                                               # end cycle through groups
    
    # posterior weight for common linear trend with no intercepts
    b1DXtWX <<- b1Dbetag + sumb1XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b1pbeta <<- b1prbeta + sumb1zbeta                               # sum of prior and the cumulative WLS contributions
    b1tpbeta <<- t(b1pbeta)                                         # transpose
    b1CI <<- inverse(b1DXtWX)                                       # inverse of posterior precision matrix
    b1pbeta <<- b1CI %*% b1pbeta                                    # re-scale psbeta
    b1exp <<- b1tpbeta %*% b1pbeta                                  # exponent from normal pdf
    pwts[2] <<- pwts[2] + 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1DXtWX)                                       # log-determinant for normalizing constant
    pwts[2] <<- pwts[2] - 0.5*b1wts                                 # already on log-scale
    
    #############################################
    # Rescaled posterior model weights and draw #
    #############################################
    
    # calculated using differences on log-scale (Bayes factors) for numerical stability
    for (m in 1:fp) {
      qwtsum <- 0
      for (l in 1:fp) {
        qwtsum <- qwtsum + (wts[l]/wts[m])*exp(pwts[l]-pwts[m])
      }
      qwts[m] <<- 1/qwtsum
    }
    
    # check for any missing values
    qwtsum <- NaN
    if (any_na(qwts)) {                                             # set qwtsum to zero if any missings
      qwtsum <- 0
    }
    if (!is.na(qwtsum)) {                                           # if there were missing values
      for (m in 1:fp) {
        if (is.na(qwts[m])) {
          qwts[m] <<- 0                                             # replace them with zeroes
        }
        qwtsum <- qwtsum + qwts[m]
      }
      if (qwtsum > 0) {                                             # rescale to sum to 1
        for (m in 1:fp) {
          qwts[m] <<- qwts[m]/qwtsum
        }
      } else {                                                      # (qwtsum == 0) all weights are zero: use prior weights
        for (m in 1:fp) {
          qwts[m] <<- wts[m]
        }
      }
    }
    
    # sample from discrete posterior distribution of model flags (conditional on intercepts)
    flg <- as.integer(rcat(1, qwts))
    
    if (flg != model[['flg']]) {
      
      # Update model with new flag value
      model[['flg']] <<- flg
      
      # Re-calculate prior probabilities (in case they were not uniform)
      model$calculate(target)
      
      # Copy from model to mvSaved objects
      nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
      
    }
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_FP_xptl_bmal (sampler_CPb_xptl_bmal will update trend coefficients according to new 'flg' value)

# Custom Gibbs sampler for model flags, conditional on intercepts, in the linear-linear BMA model with a full trend break
sampler_FP_xptf_bmal_bmal <- nimbleFunction(
  
  contains = sampler_BASE,
  
  setup = function(model, mvSaved, target, control) {
    
    # Length of target
    lt <- 0L
    lt <- length(target)
    
    # Make sure the 'target' value is correctly specified
    if (lt != 1) {
      nimStop("Only the model flag 'flg' can be sampled using 'sampler_FP_xptf_bmal_bmal'. Segment-specific flags 's1flg' and 's2flg' are derived deterministically from 'flg'.")
    } else {
      if (target[1] != 'flg') {
        nimStop("Target node must be the model flag (integer) 'flg'.")
      }
    }
    
    # Get constants from model to use in calculations
    s1pp2 <- 0L
    s1p <- model$getConstants()$s1p
    s1pp2 <- s1p + 2
    p <- model$getConstants()$p
    if (p != 4) {
      nimStop("Expecting design matrix to have 4 columns in 'sampler_FP_xptf_bmal_bmal'.")
    }
    fp <- model$getConstants()$fp
    if (fp != 3) {
      nimStop("Expecting a total of 3 possible trend models in 'sampler_FP_xptf_bmal_bmal'.")
    }
    g <- model$getConstants()$g
    n <- model$getConstants()$n
    X <- model$getConstants()$X
    rts <- model$getConstants()$rts
    
    ###############################################
    # Preallocate storage for matrix calculations #
    ###############################################
    
    pwts <- nimNumeric(fp, init = FALSE)
    qwts <- nimNumeric(fp, init = FALSE)
    
    Zvec <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    VgD <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Vg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    Wg <- nimMatrix(nrow=n, ncol=n, init = FALSE)
    
    aX <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    aXbeta <- nimMatrix(nrow=n, ncol=1L, init = FALSE)
    abeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    sumb1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    sumb1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b1mbetag <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b1tmbetag <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    
    b1Dbetag <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b1X <- nimMatrix(nrow=n, ncol=2L, init = FALSE)
    
    b1Xt <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b1XtW <- nimMatrix(nrow=2L, ncol=n, init = FALSE)
    
    b1XtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b1DXtWX <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b1prbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b1pbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b1tpbeta <- nimMatrix(nrow=1L, ncol=2L, init = FALSE)
    
    b1zbeta <- nimMatrix(nrow=2L, ncol=1L, init = FALSE)
    
    b1CI <- nimMatrix(nrow=2L, ncol=2L, init = FALSE)
    
    b1exp <- nimMatrix(nrow=1L, ncol=1L, init = FALSE)
    
    b1wts <- 0
    
    # Confirm target is scalar
    target <- model$expandNodeNames(target, returnScalarComponents = TRUE)
    if (length(target) > 1) {
      nimStop("Expecting a single flag as the target in 'sampler_FP_xptf_bmal_bmal'.")
    }
    
    # Get dependent nodes 's1flg' and 's2flg'
    deterministicDependents <- model$getDependencies(target, determOnly = TRUE)
    
  },
  
  run = function() {
    
    # Get data
    Yarr <- model[['Yarr']]
    S2arr <- model[['S2arr']]
    mbetag <- model[['mbetag']]
    Dbetag <- model[['Dbetag']]
    
    # Get prior model weights, intercepts, and current AR(1) parameters (draws are conditional on these values)
    wts <- model[['wts']]
    s1ag <- model[['s1ag']]
    s2ag <- model[['s2ag']]
    rhoarr <- model[['rhoarr']]
    nuarr <- model[['nuarr']]
    
    # Working variables
    i <- 0L
    j <- 0L
    k <- 0L
    l <- 0L
    m <- 0L
    irho <- 0
    inu <- 0
    qwtsum <- 0
    s1flg <- 0L
    s2flg <- 0L
    flg <- 0L
    
    ############################################################
    # Required conformal matrices for each possible model flag #
    ############################################################
    
    # intercepts
    for (j in 1:n) {
      aX[j,1] <<- X[j,1]
      aX[j,2] <<- X[j,1+s1p]
    }
    
    # linear-linear with no intercept
    for (j in 1:n) {
      for (k in 2:s1p) {
        b1X[j,k-1] <<- X[j,k]
      }
      for (k in s1pp2:p) {
        b1X[j,k-2] <<- X[j,k]
      }
    }
    b1Dbetag <<- nimMatrix(0, 2L, 2L)
    for (k in 2:s1p) {
      b1mbetag[k-1,1] <<- mbetag[k,1]
      b1Dbetag[k-1,k-1] <<- Dbetag[k,k]
    }
    for (k in s1pp2:p) {
      b1mbetag[k-2,1] <<- mbetag[k,1]
      b1Dbetag[k-2,k-2] <<- Dbetag[k,k]
    }
    b1Xt <<- t(b1X)                                                 # transpose bX to use below
    b1prbeta <<- b1Dbetag %*% b1mbetag                              # contribution to posterior mean from prior
    
    sumb1XtWX <<- nimMatrix(0, 2L, 2L)                              # initialize cumulative sums to use in common case
    sumb1zbeta <<- nimMatrix(0, 2L, 1L)
    
    ################################################################################
    # Model weights (log-scale) for group-specific trend models with no intercepts #
    ################################################################################
    
    # prior weight for intercepts-only model
    pwts[3] <<- 0                                                   # remains zero
    
    # prior weight for group-specific linear-linear trend model
    pwts[1] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf features g times
    pwts[1] <<- pwts[1] - 0.5*g*b1exp[1,1]                          # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix features g times
    pwts[1] <<- pwts[1] + 0.5*g*b1wts                               # already on log scale
    
    # posterior weights from group-specific contributions
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # group-specific linear-linear trend model with no intercept
      b1XtW <<- b1Xt %*% Wg                                         # multiply Xt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate XtWX, the precision matrix from WLS
      b1DXtWX <<- b1Dbetag + b1XtWX                                 # posterior precision matrix for beta is qDbetag + XtWX
      b1zbeta <<- b1XtW %*% Zvec                                    # contribution to posterior mean from WLS
      b1pbeta <<- b1prbeta + b1zbeta                                # sum of prior and WLS contributions
      b1tpbeta <<- t(b1pbeta)                                       # transpose
      b1CI <<- inverse(b1DXtWX)                                     # inverse of posterior precision matrix
      b1pbeta <<- b1CI %*% b1pbeta                                  # re-scale psbeta
      b1exp <<- b1tpbeta %*% b1pbeta                                # exponent from normal pdf
      pwts[1] <<- pwts[1] + 0.5*b1exp[1,1]                          # log scale
      b1wts <<- logdet(b1DXtWX)                                     # log-determinant for normalizing constant
      pwts[1] <<- pwts[1] - 0.5*b1wts                               # already on log scale
      
    }                                                               # end cycle through groups
    
    ############################################################
    # Model weights for common trend models with no intercepts #
    ############################################################
    
    # prior weight for common linear-linear trend model
    pwts[2] <<- 0
    b1tmbetag <<- t(b1mbetag)
    b1exp <<- b1tmbetag %*% b1prbeta                                # exponent from normal pdf
    pwts[2] <<- pwts[2] - 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1Dbetag)                                      # log-determinant of prior precision matrix
    pwts[2] <<- pwts[2] + 0.5*b1wts                                 # already on log scale
    
    # cumulative posterior contributions over groups
    for (i in 1:g) {                                                # cycle through each group independently
      
      abeta[1,1] <<- s1ag[i]                                        # group-specific abeta vector, segment 1
      abeta[2,1] <<- s2ag[i]                                        # group-specific abeta vector, segment 2
      aXbeta <<- aX %*% abeta                                       # vector of intercepts a
      
      VgD <<- nimMatrix(0, n, n)
      for (j in 1:n) {
        Zvec[j,1] <<- Yarr[(i-1)*n+j] - aXbeta[j,1]                 # populate nx1 data vector Zvec = Yvec - a
        VgD[j,j] <<- S2arr[(i-1)*n+j]                               # (Re-)set VgD to diagonal matrix of sampling variances
      }
      irho <- rhoarr[i]
      inu <- nuarr[i]
      Vg <<- ar1_cov_matrix(rts, irho, inu)                         # add AR covariance matrix Vgamma
      Vg <<- VgD + Vg
      Wg <<- inverse(Vg)                                            # Wg = Vg^{-1}
      
      # common linear-linear trend model with no intercepts
      b1XtW <<- b1Xt %*% Wg                                         # multiply bXt and Wg
      b1XtWX <<- b1XtW %*% b1X                                      # calculate bXtWX
      sumb1XtWX <<- sumb1XtWX + b1XtWX                              # cumulative matrix sum
      b1zbeta <<- b1XtW %*% Zvec                                    # contributions to posterior mean from WLS
      sumb1zbeta <<- sumb1zbeta + b1zbeta                           # cumulative matrix sum
      
    }                                                               # end cycle through groups
    
    # posterior weight for common linear-linear trend with no intercepts
    b1DXtWX <<- b1Dbetag + sumb1XtWX                                # posterior precision matrix for common beta is bDbetag + sumbXtWX
    b1pbeta <<- b1prbeta + sumb1zbeta                               # sum of prior and the cumulative WLS contributions
    b1tpbeta <<- t(b1pbeta)                                         # transpose
    b1CI <<- inverse(b1DXtWX)                                       # inverse of posterior precision matrix
    b1pbeta <<- b1CI %*% b1pbeta                                    # re-scale psbeta
    b1exp <<- b1tpbeta %*% b1pbeta                                  # exponent from normal pdf
    pwts[2] <<- pwts[2] + 0.5*b1exp[1,1]                            # log scale
    b1wts <<- logdet(b1DXtWX)                                       # log-determinant for normalizing constant
    pwts[2] <<- pwts[2] - 0.5*b1wts                                 # already on log-scale
    
    #############################################
    # Rescaled posterior model weights and draw #
    #############################################
    
    # calculated using differences on log-scale (a.k.a., Bayes factors) for numerical stability
    for (m in 1:fp) {
      qwtsum <- 0
      for (l in 1:fp) {
        qwtsum <- qwtsum + (wts[l]/wts[m])*exp(pwts[l]-pwts[m])
      }
      qwts[m] <<- 1/qwtsum
    }
    
    # check for any missing values
    qwtsum <- NaN
    if (any_na(qwts)) {                                             # set qwtsum to zero if any missings
      qwtsum <- 0
    }
    if (!is.na(qwtsum)) {                                           # if there were missing values
      for (m in 1:fp) {
        if (is.na(qwts[m])) {
          qwts[m] <<- 0                                             # replace them with zeroes
        }
        qwtsum <- qwtsum + qwts[m]
      }
      if (qwtsum > 0) {                                             # rescale to sum to 1
        for (m in 1:fp) {
          qwts[m] <<- qwts[m]/qwtsum
        }
      } else {                                                      # (qwtsum == 0) all weights are zero: use prior weights
        for (m in 1:fp) {
          qwts[m] <<- wts[m]
        }
      }
    }
    
    # sample from discrete posterior distribution of model flags (conditional on intercepts)
    flg <- as.integer(rcat(1, qwts))
    
    if (flg != model[['flg']]) {
      
      # Update model with new flag value(s)
      model[['flg']] <<- flg
      
      # Re-calculate prior probabilities (in case they were not uniform)
      model$calculate(target)
      
      # Re-calculate segment-specific flags 's1flg' and 's2flg'
      model$calculate(deterministicDependents)
      
      # Copy from model to mvSaved objects
      nimCopy(from = model, to = mvSaved, row = 1, nodes = target, logProb = TRUE)
      nimCopy(from = model, to = mvSaved, row = 1, nodes = deterministicDependents, logProb = FALSE)
      
    }
    
  },
  
  methods = list(reset = function() {})
  
) # sampler_FP_xptf_bmal_bmal (sampler_CPb_xptf_bmal_bmal will update trend coefficients according to new 's1flg' and 's2flg' values)

##############################################################################
# eMKF: Model strings representing the hierarchical models to pass to NIMBLE #
##############################################################################

# BUGS-like model code for single regression models ('bayesfit' in SAS eMKF macro)
bayesfit_modelCode <- nimbleCode({
  
  # Assumes response variable has g groups and n timepoints, sorted by group then timepoint, and provided as a stacked vector (gn x 1).

  # Prior for AR(1) correlation coefficient (canonical dnorm in NIMBLE uses sd parameter)
  if (common_psi) {
    psi ~ T(dnorm(mrho, sd=srho), 0, Inf)  # eMKF: (new) Using truncated normal to ensure nonnegative draws
    for (i in 1:g) {
      psiarr[i] <- psi
    } 
  } else {
    spsil <- 0.0001
    spsiu <- srho
    spsi ~ dunif(spsil, spsiu)
    mpsi ~ T(dnorm(mrho, sd=srho), 0, Inf)
    for (i in 1:g) {
      psiarr[i] ~ T(dnorm(mpsi, sd=spsi), 0, Inf)
    }
  }
  
  # Prior for AR(1) innovation standard deviation
  if (common_tau) {
    tau ~ dunif(taul, tauu)
    tausq <- pow(tau, 2)
    for (i in 1:g) {
      tauarr[i] <- tau
      tausqarr[i] <- tausq
    } 
  } else {
    for (i in 1:g) {
      tauarr[i] ~ dunif(taul, tauu)
      tausqarr[i] <- pow(tauarr[i], 2)
    }
  }

  if (common_ar) { 
    rhoarr[1] <- (exp(psiarr[1])-1)/(exp(psiarr[1])+1)
    nuarr[1] <- tausqarr[1]/(1-pow(rhoarr[1],2))
    for (i in 2:g) {
      rhoarr[i] <- rhoarr[1]
      nuarr[i] <- nuarr[1]
    }
  } else {
    for (i in 1:g) {
      rhoarr[i] <- (exp(psiarr[i])-1)/(exp(psiarr[i])+1)
      nuarr[i] <- tausqarr[i]/(1-pow(rhoarr[i],2))
    }
  }
  
  # For tracking purposes, define a scalar rho in the common_psi case
  if (common_psi) {
    rho <- rhoarr[1]
  }
  
  # Diagonal matrix of prior standard deviations for regression parameters
  if (dropped_trend) {
    Sbetag[1,1] <- sqrt(1/Dbetag[1,1])
  } else {
    Sbetag[1:p,1:p] <- sqrt(inverse(Dbetag[1:p,1:p]))
  }
  
  # Prior for intercepts (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    ag[i] ~ dnorm(mbetag[1,1], sd=Sbetag[1,1])
  }
  
  if (common_trend) { # Priors for common parameters, and trend predictions
    if (cubic_trend) {
      b3 ~ dnorm(mbetag[4,1], sd=Sbetag[4,4]) 
      b2 ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
      b1 ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1 + X[j,3]*b2 + X[j,4]*b3
        }
      }
    } else {
      if (quadratic_trend) {
        b2 ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
        b1 ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
        for (i in 1:g) {
          for (j in 1:n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1 + X[j,3]*b2
          }
        }
      } else {
        if (linear_trend) {
          b1 ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
          for (i in 1:g) {
            for (j in 1:n) {
              etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1
            }
          }
        } else { # intercepts-only model
          for (i in 1:g) {
            for (j in 1:n) {
              etamnarr[(i-1)*n+j] <- X[j,1]*ag[i]
            }
          }
        }
      }
    }
  } else { # Priors for group-specific parameters, and trend predictions
    if (cubic_trend) {
      for (i in 1:g) {
        b3g[i] ~ dnorm(mbetag[4,1], sd=Sbetag[4,4]) 
        b2g[i] ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
        b1g[i] ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
      } 
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1g[i] + X[j,3]*b2g[i] + X[j,4]*b3g[i]
        }
      }
    } else {
      if (quadratic_trend) {
        for (i in 1:g) {
          b2g[i] ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
          b1g[i] ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
        }  
        for (i in 1:g) {
          for (j in 1:n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1g[i] + X[j,3]*b2g[i]
          }
        }
      } else { # linear trend
        for (i in 1:g) {
          b1g[i] ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
        }
        for (i in 1:g) {
          for (j in 1:n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1g[i]
          }
        }
      }
    }
  }
  
  # True states, using recursive formula (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    setaarr[(i-1)*n+1] <- sqrt(nuarr[i])
    metaarr[(i-1)*n+1] <- etamnarr[(i-1)*n+1]
    etaarr[(i-1)*n+1] ~ dnorm(metaarr[(i-1)*n+1], sd=setaarr[(i-1)*n+1])
    for (j in 2:n) {
      setaarr[(i-1)*n+j] <- sqrt(nuarr[i]*(1-pow(rhoarr[i], 2*(rts[j]-rts[j-1]))))
      metaarr[(i-1)*n+j] <- etamnarr[(i-1)*n+j] + pow(rhoarr[i], rts[j]-rts[j-1])*(etaarr[(i-1)*n+j-1] - etamnarr[(i-1)*n+j-1])
      etaarr[(i-1)*n+j] ~ dnorm(metaarr[(i-1)*n+j], sd=setaarr[(i-1)*n+j])
    }
  }
  
  # Likelihood: Joint distribution of sample means (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    for (j in 1:n) {
      S1arr[(i-1)*n+j] <- sqrt(S2arr[(i-1)*n+j])
      Yarr[(i-1)*n+j] ~ dnorm(etaarr[(i-1)*n+j], sd=S1arr[(i-1)*n+j])
    }
  }
  
  if (randomVars) {
    
    # Variance parameters from inverse Gamma distribution (canonical dinvgamma in NIMBLE uses the rate)
    vrate <- 1/vscale
    for (i in 1:g) {
      varr[i] ~ dinvgamma(shape=vshape, rate=vrate)
    }
    
    # Likelihood: Joint distribution of sample variances (canonical dgamma in NIMBLE uses the scale)
    for (i in 1:g) {
      for (j in 1:n) {
        S2arrShape[(i-1)*n+j] <- (Narr[(i-1)*n+j]-1)/2
        S2arrScale[(i-1)*n+j] <- (2*varr[i])/(Narr[(i-1)*n+j]-1)
        S2arr[(i-1)*n+j] ~ dgamma(shape=S2arrShape[(i-1)*n+j], scale=S2arrScale[(i-1)*n+j])
      }
    }
    
  }
          
}) # bayesfit_modelCode

# BUGS-like model code for single regression models with a level shift in trend ('bayesfitxptl' in SAS eMKF macro)
bayesfitxptl_modelCode <- nimbleCode({
  
  # Assumes response variable has g groups and n timepoints, sorted by group then timepoint, and provided as a stacked vector (gn x 1).
  
  # Prior for AR(1) correlation coefficient (canonical dnorm in NIMBLE uses sd parameter)
  if (common_psi) {
    psi ~ T(dnorm(mrho, sd=srho), 0, Inf)  # eMKF: (new) Using truncated normal to ensure nonnegative draws
    for (i in 1:g) {
      psiarr[i] <- psi
    } 
  } else {
    spsil <- 0.0001
    spsiu <- srho
    spsi ~ dunif(spsil, spsiu)
    mpsi ~ T(dnorm(mrho, sd=srho), 0, Inf)
    for (i in 1:g) {
      psiarr[i] ~ T(dnorm(mpsi, sd=spsi), 0, Inf)
    }
  }
  
  # Prior for AR(1) innovation standard deviation
  if (common_tau) {
    tau ~ dunif(taul, tauu)
    tausq <- pow(tau, 2)
    for (i in 1:g) {
      tauarr[i] <- tau
      tausqarr[i] <- tausq
    } 
  } else {
    for (i in 1:g) {
      tauarr[i] ~ dunif(taul, tauu)
      tausqarr[i] <- pow(tauarr[i], 2)
    }
  }
  
  if (common_ar) { 
    rhoarr[1] <- (exp(psiarr[1])-1)/(exp(psiarr[1])+1)
    nuarr[1] <- tausqarr[1]/(1-pow(rhoarr[1],2))
    for (i in 2:g) {
      rhoarr[i] <- rhoarr[1]
      nuarr[i] <- nuarr[1]
    }
  } else {
    for (i in 1:g) {
      rhoarr[i] <- (exp(psiarr[i])-1)/(exp(psiarr[i])+1)
      nuarr[i] <- tausqarr[i]/(1-pow(rhoarr[i],2))
    }
  }
  
  # For tracking purposes, define a scalar rho in the common_psi case
  if (common_psi) {
    rho <- rhoarr[1]
  }
  
  # Diagonal matrix of prior standard deviations for regression parameters
  Sbetag[1:p,1:p] <- sqrt(inverse(Dbetag[1:p,1:p]))
  
  # Prior for intercepts (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    s1ag[i] ~ dnorm(mbetag[1,1], sd=Sbetag[1,1])
    s2ag[i] ~ dnorm(mbetag[2,1], sd=Sbetag[2,2])
  }
  
  if (common_trend) { # Priors for common parameters, and trend predictions
    if (cubic_trend) {
      b3 ~ dnorm(mbetag[5,1], sd=Sbetag[5,5]) 
      b2 ~ dnorm(mbetag[4,1], sd=Sbetag[4,4]) 
      b1 ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1 + X[j,4]*b2 + X[j,5]*b3
        }
      }
    } else {
      if (quadratic_trend) {
        b2 ~ dnorm(mbetag[4,1], sd=Sbetag[4,4]) 
        b1 ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
        for (i in 1:g) {
          for (j in 1:n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1 + X[j,4]*b2
          }
        }
      } else {
        if (linear_trend) {
          b1 ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
          for (i in 1:g) {
            for (j in 1:n) {
              etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1
            }
          }
        } else { # intercepts-only model
          for (i in 1:g) {
            for (j in 1:n) {
              etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i]
            }
          }
        }
      }
    }
  } else { # Priors for group-specific parameters, and trend predictions
    if (cubic_trend) {
      for (i in 1:g) {
        b3g[i] ~ dnorm(mbetag[5,1], sd=Sbetag[5,5]) 
        b2g[i] ~ dnorm(mbetag[4,1], sd=Sbetag[4,4]) 
        b1g[i] ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
      } 
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1g[i] + X[j,4]*b2g[i] + X[j,5]*b3g[i]
        }
      }
    } else {
      if (quadratic_trend) {
        for (i in 1:g) {
          b2g[i] ~ dnorm(mbetag[4,1], sd=Sbetag[4,4]) 
          b1g[i] ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
        }  
        for (i in 1:g) {
          for (j in 1:n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1g[i] + X[j,4]*b2g[i]
          }
        }
      } else { # linear trend
        for (i in 1:g) {
          b1g[i] ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
        }
        for (i in 1:g) {
          for (j in 1:n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1g[i]
          }
        }
      }
    }
  }
  
  # True states, using recursive formula (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    setaarr[(i-1)*n+1] <- sqrt(nuarr[i])
    metaarr[(i-1)*n+1] <- etamnarr[(i-1)*n+1]
    etaarr[(i-1)*n+1] ~ dnorm(metaarr[(i-1)*n+1], sd=setaarr[(i-1)*n+1])
    for (j in 2:n) {
      setaarr[(i-1)*n+j] <- sqrt(nuarr[i]*(1-pow(rhoarr[i], 2*(rts[j]-rts[j-1]))))
      metaarr[(i-1)*n+j] <- etamnarr[(i-1)*n+j] + pow(rhoarr[i], rts[j]-rts[j-1])*(etaarr[(i-1)*n+j-1] - etamnarr[(i-1)*n+j-1])
      etaarr[(i-1)*n+j] ~ dnorm(metaarr[(i-1)*n+j], sd=setaarr[(i-1)*n+j])
    }
  }
  
  # Likelihood: Joint distribution of sample means (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    for (j in 1:n) {
      S1arr[(i-1)*n+j] <- sqrt(S2arr[(i-1)*n+j])
      Yarr[(i-1)*n+j] ~ dnorm(etaarr[(i-1)*n+j], sd=S1arr[(i-1)*n+j])
    }
  }
  
  if (randomVars) {
    
    # Variance parameters from inverse Gamma distribution (canonical dinvgamma in NIMBLE uses the rate)
    vrate <- 1/vscale
    for (i in 1:g) {
      varr[i] ~ dinvgamma(shape=vshape, rate=vrate)
    }
    
    # Likelihood: Joint distribution of sample variances (canonical dgamma in NIMBLE uses the scale)
    for (i in 1:g) {
      for (j in 1:n) {
        S2arrShape[(i-1)*n+j] <- (Narr[(i-1)*n+j]-1)/2
        S2arrScale[(i-1)*n+j] <- (2*varr[i])/(Narr[(i-1)*n+j]-1)
        S2arr[(i-1)*n+j] ~ dgamma(shape=S2arrShape[(i-1)*n+j], scale=S2arrScale[(i-1)*n+j])
      }
    }
    
  }
  
}) # bayesfitxptl_modelCode

# BUGS-like model code for single regression models with a full break in trend ('bayesfitxptf' in SAS eMKF macro)
bayesfitxptf_modelCode <- nimbleCode({
  
  # Assumes response variable has g groups and n timepoints, sorted by group then timepoint, and provided as a stacked vector (gn x 1).
  
  # Prior for AR(1) correlation coefficient (canonical dnorm in NIMBLE uses sd parameter)
  if (common_psi) {
    psi ~ T(dnorm(mrho, sd=srho), 0, Inf)  # eMKF: (new) Using truncated normal to ensure nonnegative draws
    for (i in 1:g) {
      psiarr[i] <- psi
    } 
  } else {
    spsil <- 0.0001
    spsiu <- srho
    spsi ~ dunif(spsil, spsiu)
    mpsi ~ T(dnorm(mrho, sd=srho), 0, Inf)
    for (i in 1:g) {
      psiarr[i] ~ T(dnorm(mpsi, sd=spsi), 0, Inf)
    }
  }
  
  # Prior for AR(1) innovation standard deviation
  if (common_tau) {
    tau ~ dunif(taul, tauu)
    tausq <- pow(tau, 2)
    for (i in 1:g) {
      tauarr[i] <- tau
      tausqarr[i] <- tausq
    } 
  } else {
    for (i in 1:g) {
      tauarr[i] ~ dunif(taul, tauu)
      tausqarr[i] <- pow(tauarr[i], 2)
    }
  }
  
  if (common_ar) { 
    rhoarr[1] <- (exp(psiarr[1])-1)/(exp(psiarr[1])+1)
    nuarr[1] <- tausqarr[1]/(1-pow(rhoarr[1],2))
    for (i in 2:g) {
      rhoarr[i] <- rhoarr[1]
      nuarr[i] <- nuarr[1]
    }
  } else {
    for (i in 1:g) {
      rhoarr[i] <- (exp(psiarr[i])-1)/(exp(psiarr[i])+1)
      nuarr[i] <- tausqarr[i]/(1-pow(rhoarr[i],2))
    }
  }
  
  # For tracking purposes, define a scalar rho in the common_psi case
  if (common_psi) {
    rho <- rhoarr[1]
  }
  
  # Diagonal matrix of prior standard deviations for regression parameters
  Sbetag[1:p,1:p] <- sqrt(inverse(Dbetag[1:p,1:p]))
  
  # Prior for intercepts (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    s1ag[i] ~ dnorm(mbetag[1,1], sd=Sbetag[1,1])
    s2ag[i] ~ dnorm(mbetag[1+s1p,1], sd=Sbetag[1+s1p,1+s1p])
  }
  
  if (common_trend) { # Priors for common parameters, and trend predictions
    if (cubic_cubic_trend) {
      s1b3 ~ dnorm(mbetag[4,1], sd=Sbetag[4,4]) 
      s1b2 ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
      s1b1 ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
      s2b3 ~ dnorm(mbetag[4+s1p,1], sd=Sbetag[4+s1p,4+s1p]) 
      s2b2 ~ dnorm(mbetag[3+s1p,1], sd=Sbetag[3+s1p,3+s1p]) 
      s2b1 ~ dnorm(mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
      for (i in 1:g) {
        for (j in 1:s1n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1 + X[j,3]*s1b2 + X[j,4]*s1b3
        }
        for (j in (1+s1n):n) {
          etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1 + X[j,3+s1p]*s2b2 + X[j,4+s1p]*s2b3
        }
      }
    } else {
      if (cubic_quadratic_trend) {
        s1b3 ~ dnorm(mbetag[4,1], sd=Sbetag[4,4]) 
        s1b2 ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
        s1b1 ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
        s2b2 ~ dnorm(mbetag[3+s1p,1], sd=Sbetag[3+s1p,3+s1p]) 
        s2b1 ~ dnorm(mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
        for (i in 1:g) {
          for (j in 1:s1n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1 + X[j,3]*s1b2 + X[j,4]*s1b3
          }
          for (j in (1+s1n):n) {
            etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1 + X[j,3+s1p]*s2b2
          }
        }
      } else {
        if (cubic_linear_trend) {
          s1b3 ~ dnorm(mbetag[4,1], sd=Sbetag[4,4]) 
          s1b2 ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
          s1b1 ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
          s2b1 ~ dnorm(mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
          for (i in 1:g) {
            for (j in 1:s1n) {
              etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1 + X[j,3]*s1b2 + X[j,4]*s1b3
            }
            for (j in (1+s1n):n) {
              etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1
            }
          }
        } else {
          if (quadratic_quadratic_trend) {
            s1b2 ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
            s1b1 ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
            s2b2 ~ dnorm(mbetag[3+s1p,1], sd=Sbetag[3+s1p,3+s1p]) 
            s2b1 ~ dnorm(mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p]) 
            for (i in 1:g) {
              for (j in 1:s1n) {
                etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1 + X[j,3]*s1b2
              }
              for (j in (1+s1n):n) {
                etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1 + X[j,3+s1p]*s2b2
              }
            }
          } else {
            if (quadratic_linear_trend) {
              s1b2 ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
              s1b1 ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
              s2b1 ~ dnorm(mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p]) 
              for (i in 1:g) {
                for (j in 1:s1n) {
                  etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1 + X[j,3]*s1b2
                }
                for (j in (1+s1n):n) {
                  etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1
                }
              }
            } else {
              if (linear_linear_trend) {
                s1b1 ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
                s2b1 ~ dnorm(mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p]) 
                for (i in 1:g) {
                  for (j in 1:s1n) {
                    etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1
                  }
                  for (j in (1+s1n):n) {
                    etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1
                  }
                }
              } else { # intercepts-only model
                for (i in 1:g) {
                  for (j in 1:s1n) {
                    etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i]
                  }
                  for (j in (1+s1n):n) {
                    etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i]
                  }
                }
              }
            }
          }
        }
      }
    }
  } else { # Priors for group-specific parameters, and trend predictions
    if (cubic_cubic_trend) {
      for (i in 1:g) {
        s1b3g[i] ~ dnorm(mbetag[4,1], sd=Sbetag[4,4]) 
        s1b2g[i] ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
        s1b1g[i] ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
        s2b3g[i] ~ dnorm(mbetag[4+s1p,1], sd=Sbetag[4+s1p,4+s1p]) 
        s2b2g[i] ~ dnorm(mbetag[3+s1p,1], sd=Sbetag[3+s1p,3+s1p]) 
        s2b1g[i] ~ dnorm(mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
      }
      for (i in 1:g) {
        for (j in 1:s1n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i] + X[j,4]*s1b3g[i]
        }
        for (j in (1+s1n):n) {
          etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i] + X[j,3+s1p]*s2b2g[i] + X[j,4+s1p]*s2b3g[i]
        }
      }
    } else {
      if (cubic_quadratic_trend) {
        for (i in 1:g) {
          s1b3g[i] ~ dnorm(mbetag[4,1], sd=Sbetag[4,4]) 
          s1b2g[i] ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
          s1b1g[i] ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
          s2b2g[i] ~ dnorm(mbetag[3+s1p,1], sd=Sbetag[3+s1p,3+s1p]) 
          s2b1g[i] ~ dnorm(mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
        }
        for (i in 1:g) {
          for (j in 1:s1n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i] + X[j,4]*s1b3g[i]
          }
          for (j in (1+s1n):n) {
            etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i] + X[j,3+s1p]*s2b2g[i]
          }
        }
      } else {
        if (cubic_linear_trend) {
          for (i in 1:g) {
            s1b3g[i] ~ dnorm(mbetag[4,1], sd=Sbetag[4,4]) 
            s1b2g[i] ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
            s1b1g[i] ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
            s2b1g[i] ~ dnorm(mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
          }
          for (i in 1:g) {
            for (j in 1:s1n) {
              etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i] + X[j,4]*s1b3g[i]
            }
            for (j in (1+s1n):n) {
              etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i]
            }
          }
        } else {
          if (quadratic_quadratic_trend) {
            for (i in 1:g) {
              s1b2g[i] ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
              s1b1g[i] ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
              s2b2g[i] ~ dnorm(mbetag[3+s1p,1], sd=Sbetag[3+s1p,3+s1p]) 
              s2b1g[i] ~ dnorm(mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
            }
            for (i in 1:g) {
              for (j in 1:s1n) {
                etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i]
              }
              for (j in (1+s1n):n) {
                etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i] + X[j,3+s1p]*s2b2g[i]
              }
            }
          } else {
            if (quadratic_linear_trend) {
              for (i in 1:g) {
                s1b2g[i] ~ dnorm(mbetag[3,1], sd=Sbetag[3,3]) 
                s1b1g[i] ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
                s2b1g[i] ~ dnorm(mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
              }
              for (i in 1:g) {
                for (j in 1:s1n) {
                  etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i]
                }
                for (j in (1+s1n):n) {
                  etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i]
                }
              }
            } else {
              if (linear_linear_trend) {
                for (i in 1:g) {
                  s1b1g[i] ~ dnorm(mbetag[2,1], sd=Sbetag[2,2]) 
                  s2b1g[i] ~ dnorm(mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
                }
                for (i in 1:g) {
                  for (j in 1:s1n) {
                    etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i]
                  }
                  for (j in (1+s1n):n) {
                    etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i]
                  }
                }
              }
            }
          }
        }
      }
    }
  }
  
  # True states, using recursive formula (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    setaarr[(i-1)*n+1] <- sqrt(nuarr[i])
    metaarr[(i-1)*n+1] <- etamnarr[(i-1)*n+1]
    etaarr[(i-1)*n+1] ~ dnorm(metaarr[(i-1)*n+1], sd=setaarr[(i-1)*n+1])
    for (j in 2:n) {
      setaarr[(i-1)*n+j] <- sqrt(nuarr[i]*(1-pow(rhoarr[i], 2*(rts[j]-rts[j-1]))))
      metaarr[(i-1)*n+j] <- etamnarr[(i-1)*n+j] + pow(rhoarr[i], rts[j]-rts[j-1])*(etaarr[(i-1)*n+j-1] - etamnarr[(i-1)*n+j-1])
      etaarr[(i-1)*n+j] ~ dnorm(metaarr[(i-1)*n+j], sd=setaarr[(i-1)*n+j])
    }
  }
  
  # Likelihood: Joint distribution of sample means (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    for (j in 1:n) {
      S1arr[(i-1)*n+j] <- sqrt(S2arr[(i-1)*n+j])
      Yarr[(i-1)*n+j] ~ dnorm(etaarr[(i-1)*n+j], sd=S1arr[(i-1)*n+j])
    }
  }
  
  if (randomVars) {
    
    # Variance parameters from inverse Gamma distribution (canonical dinvgamma in NIMBLE uses the rate)
    vrate <- 1/vscale
    for (i in 1:g) {
      varr[i] ~ dinvgamma(shape=vshape, rate=vrate)
    }
    
    # Likelihood: Joint distribution of sample variances (canonical dgamma in NIMBLE uses the scale)
    for (i in 1:g) {
      for (j in 1:n) {
        S2arrShape[(i-1)*n+j] <- (Narr[(i-1)*n+j]-1)/2
        S2arrScale[(i-1)*n+j] <- (2*varr[i])/(Narr[(i-1)*n+j]-1)
        S2arr[(i-1)*n+j] ~ dgamma(shape=S2arrShape[(i-1)*n+j], scale=S2arrScale[(i-1)*n+j])
      }
    }
    
  }
  
}) # bayesfitxptf_modelCode

# BUGS-like model code for BMA regression models ('bayesBMA' in SAS eMKF macro)
bayesBMA_modelCode <- nimbleCode({
  
  # Assumes response variable has g groups and n timepoints, sorted by group then timepoint, and provided as a stacked vector (gn x 1). 
  # There are fp = 7, 5, and 3 models in the bma_cubic, bma_quad, and bma_linear cases, respectively.
  
  # Prior for AR(1) correlation coefficient (canonical dnorm in NIMBLE uses sd parameter)
  if (common_psi) {
    psi ~ T(dnorm(mrho, sd=srho), 0, Inf)  # eMKF: (new) Using truncated normal to ensure nonnegative draws
    for (i in 1:g) {
      psiarr[i] <- psi
    } 
  } else {
    spsil <- 0.0001
    spsiu <- srho
    spsi ~ dunif(spsil, spsiu)
    mpsi ~ T(dnorm(mrho, sd=srho), 0, Inf)
    for (i in 1:g) {
      psiarr[i] ~ T(dnorm(mpsi, sd=spsi), 0, Inf)
    }
  }
  
  # Prior for AR(1) innovation standard deviation
  if (common_tau) {
    tau ~ dunif(taul, tauu)
    tausq <- pow(tau, 2)
    for (i in 1:g) {
      tauarr[i] <- tau
      tausqarr[i] <- tausq
    } 
  } else {
    for (i in 1:g) {
      tauarr[i] ~ dunif(taul, tauu)
      tausqarr[i] <- pow(tauarr[i], 2)
    }
  }
  
  if (common_ar) { 
    rhoarr[1] <- (exp(psiarr[1])-1)/(exp(psiarr[1])+1)
    nuarr[1] <- tausqarr[1]/(1-pow(rhoarr[1],2))
    for (i in 2:g) {
      rhoarr[i] <- rhoarr[1]
      nuarr[i] <- nuarr[1]
    }
  } else {
    for (i in 1:g) {
      rhoarr[i] <- (exp(psiarr[i])-1)/(exp(psiarr[i])+1)
      nuarr[i] <- tausqarr[i]/(1-pow(rhoarr[i],2))
    }
  }
  
  # For tracking purposes, define a scalar rho in the common_psi case
  if (common_psi) {
    rho <- rhoarr[1]
  }
  
  # Prior weights for model flags (wtsshape currently redundant to accommodate Dirichlet for wts in future versions of macro)
  for (m in 1:fp) {
    wtsshape[m] <- wshape
    wtsTMP[m] <- wtsshape[m]
  }
  wtssum <- sum(wtsTMP[1:fp])
  for (m in 1:fp) {
    wts[m] <- wtsTMP[m]/wtssum
  }

  # Prior for model flag
  flg ~ dcat(wts[1:fp])
  
  # Diagonal matrix of prior standard deviations for regression parameters
  Sbetag[1:p,1:p] <- sqrt(inverse(Dbetag[1:p,1:p]))
  
  # Prior for intercepts (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    ag[i] ~ dnorm(mbetag[1,1], sd=Sbetag[1,1])
  }
  
  # Mixture priors for regression coefficients
  if (cubic_trend) {
    b3g[1:(g+1)] ~ dmixbeta3(lg=g, pmn = mbetag[4,1], psd = Sbetag[4,4], ind=flg, bma = 3L)
    b2g[1:(g+1)] ~ dmixbeta2(lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=flg, bma = 3L)
    b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=flg, bma = 3L)

    # Trend predictions
    for (i in 1:g) {
      for (j in 1:n) {
        etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1g[i] + X[j,3]*b2g[i] + X[j,4]*b3g[i]
      }
    }
    
  } else {
    if (quadratic_trend) {
      b2g[1:(g+1)] ~ dmixbeta2(lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=flg, bma = 2L)
      b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=flg, bma = 2L)

      # Trend predictions
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1g[i] + X[j,3]*b2g[i]
        }
      }
      
    } else { # linear_trend
      b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=flg, bma = 1L)

      # Trend predictions
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1g[i]
        }
      }
      
    }
  }
  
  # True states, using recursive formula (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    setaarr[(i-1)*n+1] <- sqrt(nuarr[i])
    metaarr[(i-1)*n+1] <- etamnarr[(i-1)*n+1]
    etaarr[(i-1)*n+1] ~ dnorm(metaarr[(i-1)*n+1], sd=setaarr[(i-1)*n+1])
    for (j in 2:n) {
      setaarr[(i-1)*n+j] <- sqrt(nuarr[i]*(1-pow(rhoarr[i], 2*(rts[j]-rts[j-1]))))
      metaarr[(i-1)*n+j] <- etamnarr[(i-1)*n+j] + pow(rhoarr[i], rts[j]-rts[j-1])*(etaarr[(i-1)*n+j-1] - etamnarr[(i-1)*n+j-1])
      etaarr[(i-1)*n+j] ~ dnorm(metaarr[(i-1)*n+j], sd=setaarr[(i-1)*n+j])
    }
  }
  
  # Likelihood: Joint distribution of sample means (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    for (j in 1:n) {
      S1arr[(i-1)*n+j] <- sqrt(S2arr[(i-1)*n+j])
      Yarr[(i-1)*n+j] ~ dnorm(etaarr[(i-1)*n+j], sd=S1arr[(i-1)*n+j])
    }
  }
  
  if (randomVars) {
    
    # Variance parameters from inverse Gamma distribution (canonical dinvgamma in NIMBLE uses the rate)
    vrate <- 1/vscale
    for (i in 1:g) {
      varr[i] ~ dinvgamma(shape=vshape, rate=vrate)
    }
    
    # Likelihood: Joint distribution of sample variances (canonical dgamma in NIMBLE uses the scale)
    for (i in 1:g) {
      for (j in 1:n) {
        S2arrShape[(i-1)*n+j] <- (Narr[(i-1)*n+j]-1)/2
        S2arrScale[(i-1)*n+j] <- (2*varr[i])/(Narr[(i-1)*n+j]-1)
        S2arr[(i-1)*n+j] ~ dgamma(shape=S2arrShape[(i-1)*n+j], scale=S2arrScale[(i-1)*n+j])
      }
    }
    
  }
  
}) # bayesBMA_modelCode

# BUGS-like model code for BMA regression models a level shift in trend ('bayesBMAxptl' in SAS eMKF macro)
bayesBMAxptl_modelCode <- nimbleCode({
  
  # Assumes response variable has g groups and n timepoints, sorted by group then timepoint, and provided as a stacked vector (gn x 1). 
  # There are fp = 7, 5, and 3 models in the bma_cubic, bma_quad, and bma_linear cases, respectively.
  
  # Prior for AR(1) correlation coefficient (canonical dnorm in NIMBLE uses sd parameter)
  if (common_psi) {
    psi ~ T(dnorm(mrho, sd=srho), 0, Inf)  # eMKF: (new) Using truncated normal to ensure nonnegative draws
    for (i in 1:g) {
      psiarr[i] <- psi
    } 
  } else {
    spsil <- 0.0001
    spsiu <- srho
    spsi ~ dunif(spsil, spsiu)
    mpsi ~ T(dnorm(mrho, sd=srho), 0, Inf)
    for (i in 1:g) {
      psiarr[i] ~ T(dnorm(mpsi, sd=spsi), 0, Inf)
    }
  }
  
  # Prior for AR(1) innovation standard deviation
  if (common_tau) {
    tau ~ dunif(taul, tauu)
    tausq <- pow(tau, 2)
    for (i in 1:g) {
      tauarr[i] <- tau
      tausqarr[i] <- tausq
    } 
  } else {
    for (i in 1:g) {
      tauarr[i] ~ dunif(taul, tauu)
      tausqarr[i] <- pow(tauarr[i], 2)
    }
  }
  
  if (common_ar) { 
    rhoarr[1] <- (exp(psiarr[1])-1)/(exp(psiarr[1])+1)
    nuarr[1] <- tausqarr[1]/(1-pow(rhoarr[1],2))
    for (i in 2:g) {
      rhoarr[i] <- rhoarr[1]
      nuarr[i] <- nuarr[1]
    }
  } else {
    for (i in 1:g) {
      rhoarr[i] <- (exp(psiarr[i])-1)/(exp(psiarr[i])+1)
      nuarr[i] <- tausqarr[i]/(1-pow(rhoarr[i],2))
    }
  }
  
  # For tracking purposes, define a scalar rho in the common_psi case
  if (common_psi) {
    rho <- rhoarr[1]
  }
  
  # Prior weights for model flags (wtsshape currently redundant to accommodate Dirichlet for wts in future versions of macro)
  for (m in 1:fp) {
    wtsshape[m] <- wshape
    wtsTMP[m] <- wtsshape[m]
  }
  wtssum <- sum(wtsTMP[1:fp])
  for (m in 1:fp) {
    wts[m] <- wtsTMP[m]/wtssum
  }
  
  # Prior for model flag
  flg ~ dcat(wts[1:fp])
  
  # Diagonal matrix of prior standard deviations for regression parameters
  Sbetag[1:p,1:p] <- sqrt(inverse(Dbetag[1:p,1:p]))
  
  # Prior for intercepts (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    s1ag[i] ~ dnorm(mbetag[1,1], sd=Sbetag[1,1])
    s2ag[i] ~ dnorm(mbetag[2,1], sd=Sbetag[2,2])
  }
  
  # Mixture priors for regression coefficients
  if (cubic_trend) {
    b3g[1:(g+1)] ~ dmixbeta3(lg=g, pmn = mbetag[5,1], psd = Sbetag[5,5], ind=flg, bma = 3L)
    b2g[1:(g+1)] ~ dmixbeta2(lg=g, pmn = mbetag[4,1], psd = Sbetag[4,4], ind=flg, bma = 3L)
    b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=flg, bma = 3L)

    # Trend predictions
    for (i in 1:g) {
      for (j in 1:n) {
        etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1g[i] + X[j,4]*b2g[i] + X[j,5]*b3g[i]
      }
    }
    
  } else {
    if (quadratic_trend) {
      b2g[1:(g+1)] ~ dmixbeta2(lg=g, pmn = mbetag[4,1], psd = Sbetag[4,4], ind=flg, bma = 2L)
      b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=flg, bma = 2L)

      # Trend predictions
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1g[i] + X[j,4]*b2g[i]
        }
      }
      
    } else { # linear_trend
      b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=flg, bma = 1L)

      # Trend predictions
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1g[i]
        }
      }
      
    }
  }
  
  # True states, using recursive formula (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    setaarr[(i-1)*n+1] <- sqrt(nuarr[i])
    metaarr[(i-1)*n+1] <- etamnarr[(i-1)*n+1]
    etaarr[(i-1)*n+1] ~ dnorm(metaarr[(i-1)*n+1], sd=setaarr[(i-1)*n+1])
    for (j in 2:n) {
      setaarr[(i-1)*n+j] <- sqrt(nuarr[i]*(1-pow(rhoarr[i], 2*(rts[j]-rts[j-1]))))
      metaarr[(i-1)*n+j] <- etamnarr[(i-1)*n+j] + pow(rhoarr[i], rts[j]-rts[j-1])*(etaarr[(i-1)*n+j-1] - etamnarr[(i-1)*n+j-1])
      etaarr[(i-1)*n+j] ~ dnorm(metaarr[(i-1)*n+j], sd=setaarr[(i-1)*n+j])
    }
  }
  
  # Likelihood: Joint distribution of sample means (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    for (j in 1:n) {
      S1arr[(i-1)*n+j] <- sqrt(S2arr[(i-1)*n+j])
      Yarr[(i-1)*n+j] ~ dnorm(etaarr[(i-1)*n+j], sd=S1arr[(i-1)*n+j])
    }
  }
  
  if (randomVars) {
    
    # Variance parameters from inverse Gamma distribution (canonical dinvgamma in NIMBLE uses the rate)
    vrate <- 1/vscale
    for (i in 1:g) {
      varr[i] ~ dinvgamma(shape=vshape, rate=vrate)
    }
    
    # Likelihood: Joint distribution of sample variances (canonical dgamma in NIMBLE uses the scale)
    for (i in 1:g) {
      for (j in 1:n) {
        S2arrShape[(i-1)*n+j] <- (Narr[(i-1)*n+j]-1)/2
        S2arrScale[(i-1)*n+j] <- (2*varr[i])/(Narr[(i-1)*n+j]-1)
        S2arr[(i-1)*n+j] ~ dgamma(shape=S2arrShape[(i-1)*n+j], scale=S2arrScale[(i-1)*n+j])
      }
    }
    
  }
  
}) # bayesBMAxptl_modelCode

# BUGS-like model code for BMA regression models with a full break in trend ('bayesBMAxptf' in SAS eMKF macro)
bayesBMAxptf_modelCode <- nimbleCode({
  
  # Assumes response variable has g groups and n timepoints, sorted by group then timepoint, and provided as a stacked vector (gn x 1). 
  # There are fp = 13, 11, 7, 7, 5, and 3 models in the bma_cubic_cubic, _cubic_quad, _cubic_linear, _quad_quad, _quad_linear, and _linear_linear cases, respectively.
  
  # Prior for AR(1) correlation coefficient (canonical dnorm in NIMBLE uses sd parameter)
  if (common_psi) {
    psi ~ T(dnorm(mrho, sd=srho), 0, Inf)  # eMKF: (new) Using truncated normal to ensure nonnegative draws
    for (i in 1:g) {
      psiarr[i] <- psi
    } 
  } else {
    spsil <- 0.0001
    spsiu <- srho
    spsi ~ dunif(spsil, spsiu)
    mpsi ~ T(dnorm(mrho, sd=srho), 0, Inf)
    for (i in 1:g) {
      psiarr[i] ~ T(dnorm(mpsi, sd=spsi), 0, Inf)
    }
  }
  
  # Prior for AR(1) innovation standard deviation
  if (common_tau) {
    tau ~ dunif(taul, tauu)
    tausq <- pow(tau, 2)
    for (i in 1:g) {
      tauarr[i] <- tau
      tausqarr[i] <- tausq
    } 
  } else {
    for (i in 1:g) {
      tauarr[i] ~ dunif(taul, tauu)
      tausqarr[i] <- pow(tauarr[i], 2)
    }
  }
  
  if (common_ar) { 
    rhoarr[1] <- (exp(psiarr[1])-1)/(exp(psiarr[1])+1)
    nuarr[1] <- tausqarr[1]/(1-pow(rhoarr[1],2))
    for (i in 2:g) {
      rhoarr[i] <- rhoarr[1]
      nuarr[i] <- nuarr[1]
    }
  } else {
    for (i in 1:g) {
      rhoarr[i] <- (exp(psiarr[i])-1)/(exp(psiarr[i])+1)
      nuarr[i] <- tausqarr[i]/(1-pow(rhoarr[i],2))
    }
  }
  
  # For tracking purposes, define a scalar rho in the common_psi case
  if (common_psi) {
    rho <- rhoarr[1]
  }
  
  # Prior weights for model flags (wtsshape currently redundant to accommodate Dirichlet for wts in future versions of macro)
  for (m in 1:fp) {
    wtsshape[m] <- wshape
    wtsTMP[m] <- wtsshape[m]
  }
  wtssum <- sum(wtsTMP[1:fp])
  for (m in 1:fp) {
    wts[m] <- wtsTMP[m]/wtssum
  }
  
  # Prior for model flag with allowed combinations of segment-specific flags
  flg ~ dcat(wts[1:fp])
  
  # Values for segment-specific flags
  if (cubic_cubic_trend) {
    s1flg <- (flg == 1 | flg == 2 | flg == 3)*1 + (flg == 4 | flg == 5)*2 + (flg == 6)*3 + (flg == 7 | flg == 8 | flg == 9)*4 + (flg == 10 | flg == 11)*5 + (flg == 12)*6 + (flg == 13)*7  
    s2flg <- (flg == 1)*1 + (flg == 2 | flg == 4)*2 + (flg == 3 | flg == 5 | flg == 6)*3 + (flg == 7)*4 + (flg == 8 | flg == 10)*5 + (flg == 9 | flg == 11 | flg == 12)*6 + (flg == 13)*7  
  } else {
    if (cubic_quadratic_trend) {
      s1flg <- (flg == 1 | flg == 2)*1 + (flg == 3 | flg == 4)*2 + (flg == 5)*3 + (flg == 6 | flg == 7)*4 + (flg == 8 | flg == 9)*5 + (flg == 10)*6 + (flg == 11)*7  
      s2flg <- (flg == 1 | flg == 3)*1 + (flg == 2 | flg == 4 | flg == 5)*2 + (flg == 6 | flg == 8)*3 + (flg == 7 | flg == 9 | flg == 10)*4 + (flg == 11)*5  
    } else {
      if (cubic_linear_trend) {
        s1flg <- (flg == 1)*1 + (flg == 2)*2 + (flg == 3)*3 + (flg == 4)*4 + (flg == 5)*5 + (flg == 6)*6 + (flg == 7)*7  
        s2flg <- (flg == 1 | flg == 2 | flg == 3)*1 + (flg == 4 | flg == 5 | flg == 6)*2 + (flg == 7)*3  
      } else {
        if (quadratic_quadratic_trend) {
          s1flg <- (flg == 1 | flg == 2)*1 + (flg == 3)*2 + (flg == 4 | flg == 5)*3 + (flg == 6)*4 + (flg == 7)*5  
          s2flg <- (flg == 1)*1 + (flg == 2 | flg == 3)*2 + (flg == 4)*3 + (flg == 5 | flg == 6)*4 + (flg == 7)*5  
        } else {
          if (quadratic_linear_trend) {
            s1flg <- (flg == 1)*1 + (flg == 2)*2 + (flg == 3)*3 + (flg == 4)*4 + (flg == 5)*5  
            s2flg <- (flg == 1 | flg == 2)*1 + (flg == 3 | flg == 4)*2 + (flg == 5)*3   
          } else { # (linear_linear_trend)
            s1flg <- (flg == 1)*1 + (flg == 2)*2 + (flg == 3)*3
            s2flg <- (flg == 1)*1 + (flg == 2)*2 + (flg == 3)*3   
          }
        }
      }
    }
  }
  
  # Diagonal matrix of prior standard deviations for regression parameters
  Sbetag[1:p,1:p] <- sqrt(inverse(Dbetag[1:p,1:p]))
  
  # Prior for intercepts (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    s1ag[i] ~ dnorm(mbetag[1,1], sd=Sbetag[1,1])
    s2ag[i] ~ dnorm(mbetag[1+s1p,1], sd=Sbetag[1+s1p,1+s1p])
  }
  
  # Mixture priors for regression coefficients
  if (cubic_cubic_trend) {
    
    # Parameters for segment 1
    s1b3g[1:(g+1)] ~ dmixbeta3(lg=g, pmn = mbetag[4,1], psd = Sbetag[4,4], ind=s1flg, bma = 3L)
    s1b2g[1:(g+1)] ~ dmixbeta2(lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=s1flg, bma = 3L)
    s1b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=s1flg, bma = 3L)

    # Parameters for segment 2
    s2b3g[1:(g+1)] ~ dmixbeta3(lg=g, pmn = mbetag[4+s1p,1], psd = Sbetag[4+s1p,4+s1p], ind=s2flg, bma = 3L)
    s2b2g[1:(g+1)] ~ dmixbeta2(lg=g, pmn = mbetag[3+s1p,1], psd = Sbetag[3+s1p,3+s1p], ind=s2flg, bma = 3L)
    s2b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[2+s1p,1], psd = Sbetag[2+s1p,2+s1p], ind=s2flg, bma = 3L)

    # Trend predictions
    for (i in 1:g) {
      for (j in 1:s1n) {
        etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i] + X[j,4]*s1b3g[i]
      }
      for (j in (1+s1n):n) {
        etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i] + X[j,3+s1p]*s2b2g[i] + X[j,4+s1p]*s2b3g[i]
      }
    }
    
  } else {
    if (cubic_quadratic_trend) {
      
      # Parameters for segment 1
      s1b3g[1:(g+1)] ~ dmixbeta3(lg=g, pmn = mbetag[4,1], psd = Sbetag[4,4], ind=s1flg, bma = 3L)
      s1b2g[1:(g+1)] ~ dmixbeta2(lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=s1flg, bma = 3L)
      s1b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=s1flg, bma = 3L)

      # Parameters for segment 2
      s2b2g[1:(g+1)] ~ dmixbeta2(lg=g, pmn = mbetag[3+s1p,1], psd = Sbetag[3+s1p,3+s1p], ind=s2flg, bma = 2L)
      s2b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[2+s1p,1], psd = Sbetag[2+s1p,2+s1p], ind=s2flg, bma = 2L)

      # Trend predictions
      for (i in 1:g) {
        for (j in 1:s1n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i] + X[j,4]*s1b3g[i]
        }
        for (j in (1+s1n):n) {
          etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i] + X[j,3+s1p]*s2b2g[i]
        }
      }
      
    } else {
      if (cubic_linear_trend) {
        
        # Parameters for segment 1
        s1b3g[1:(g+1)] ~ dmixbeta3(lg=g, pmn = mbetag[4,1], psd = Sbetag[4,4], ind=s1flg, bma = 3L)
        s1b2g[1:(g+1)] ~ dmixbeta2(lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=s1flg, bma = 3L)
        s1b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=s1flg, bma = 3L)

        # Parameters for segment 2
        s2b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[2+s1p,1], psd = Sbetag[2+s1p,2+s1p], ind=s2flg, bma = 1L)

        # Trend predictions
        for (i in 1:g) {
          for (j in 1:s1n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i] + X[j,4]*s1b3g[i]
          }
          for (j in (1+s1n):n) {
            etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i]
          }
        }
        
      } else {
        if (quadratic_quadratic_trend) {
          
          # Parameters for segment 1
          s1b2g[1:(g+1)] ~ dmixbeta2(lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=s1flg, bma = 2L)
          s1b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=s1flg, bma = 2L)

          # Parameters for segment 2
          s2b2g[1:(g+1)] ~ dmixbeta2(lg=g, pmn = mbetag[3+s1p,1], psd = Sbetag[3+s1p,3+s1p], ind=s2flg, bma = 2L)
          s2b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[2+s1p,1], psd = Sbetag[2+s1p,2+s1p], ind=s2flg, bma = 2L)

          # Trend predictions
          for (i in 1:g) {
            for (j in 1:s1n) {
              etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i]
            }
            for (j in (1+s1n):n) {
              etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i] + X[j,3+s1p]*s2b2g[i]
            }
          }
          
        } else {
          if (quadratic_linear_trend) {
            
            # Parameters for segment 1
            s1b2g[1:(g+1)] ~ dmixbeta2(lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=s1flg, bma = 2L)
            s1b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=s1flg, bma = 2L)

            # Parameters for segment 2
            s2b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[2+s1p,1], psd = Sbetag[2+s1p,2+s1p], ind=s2flg, bma = 1L)

            # Trend predictions
            for (i in 1:g) {
              for (j in 1:s1n) {
                etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i]
              }
              for (j in (1+s1n):n) {
                etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i]
              }
            }
            
          } else { # (linear_linear_trend)

            # Parameters for segment 1
            s1b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=s1flg, bma = 1L)

            # Parameters for segment 2
            s2b1g[1:(g+1)] ~ dmixbeta1(lg=g, pmn = mbetag[2+s1p,1], psd = Sbetag[2+s1p,2+s1p], ind=s2flg, bma = 1L)

            # Trend predictions
            for (i in 1:g) {
              for (j in 1:s1n) {
                etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i]
              }
              for (j in (1+s1n):n) {
                etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i]
              }
            }

          }  
        }
      }
    }
  }
  
  # True states, using recursive formula (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    setaarr[(i-1)*n+1] <- sqrt(nuarr[i])
    metaarr[(i-1)*n+1] <- etamnarr[(i-1)*n+1]
    etaarr[(i-1)*n+1] ~ dnorm(metaarr[(i-1)*n+1], sd=setaarr[(i-1)*n+1])
    for (j in 2:n) {
      setaarr[(i-1)*n+j] <- sqrt(nuarr[i]*(1-pow(rhoarr[i], 2*(rts[j]-rts[j-1]))))
      metaarr[(i-1)*n+j] <- etamnarr[(i-1)*n+j] + pow(rhoarr[i], rts[j]-rts[j-1])*(etaarr[(i-1)*n+j-1] - etamnarr[(i-1)*n+j-1])
      etaarr[(i-1)*n+j] ~ dnorm(metaarr[(i-1)*n+j], sd=setaarr[(i-1)*n+j])
    }
  }
  
  # Likelihood: Joint distribution of sample means (canonical dnorm in NIMBLE uses sd parameter)
  for (i in 1:g) {
    for (j in 1:n) {
      S1arr[(i-1)*n+j] <- sqrt(S2arr[(i-1)*n+j])
      Yarr[(i-1)*n+j] ~ dnorm(etaarr[(i-1)*n+j], sd=S1arr[(i-1)*n+j])
    }
  }
  
  if (randomVars) {
    
    # Variance parameters from inverse Gamma distribution (canonical dinvgamma in NIMBLE uses the rate)
    vrate <- 1/vscale
    for (i in 1:g) {
      varr[i] ~ dinvgamma(shape=vshape, rate=vrate)
    }
    
    # Likelihood: Joint distribution of sample variances (canonical dgamma in NIMBLE uses the scale)
    for (i in 1:g) {
      for (j in 1:n) {
        S2arrShape[(i-1)*n+j] <- (Narr[(i-1)*n+j]-1)/2
        S2arrScale[(i-1)*n+j] <- (2*varr[i])/(Narr[(i-1)*n+j]-1)
        S2arr[(i-1)*n+j] ~ dgamma(shape=S2arrShape[(i-1)*n+j], scale=S2arrScale[(i-1)*n+j])
      }
    }
    
  }
  
}) # bayesBMAxptf_modelCode

#############################################################################################
# R-side functions to set up lists of initial values for stochastic nodes to pass to NIMBLE #
#############################################################################################

# Initial values for stochastic nodes in single regression models ('bayesfit' in SAS eMKF macro)
bayesfit_init <- function(cubic_trend, quadratic_trend, linear_trend, dropped_trend, common_trend, 
                          common_ar, common_psi, common_tau,
                          randomVars,
                          orpoly,
                          g, p, n,
                          rts,
                          mbetag, Dbetag,
                          mrho, srho,
                          taul, tauu,
                          vshape, vscale,
                          init_seed
) {
  
  # Basic error checks
  if (!cubic_trend && !quadratic_trend && !linear_trend && !dropped_trend) {
    stop("Bayesian modeling is only implemented for cubic, quadratic, linear, or intercepts-only models.")
  }
  if (!common_ar && common_psi && common_tau) {
    stop("Inconsistent indicators for AR(1) covariance structure type.")
  }
  
  # Precaution to handle intercepts-only model as a 'common' model (with b3 = b2 = b1 = 0) for initialization purposes
  if (dropped_trend && !common_trend) {
    common_trend <- TRUE
  }
  
  # List of initialized stochastic nodes to pass to NIMBLE
  init_list <- list()
  set.seed(init_seed)
  
  # AR(1) correlation coefficient
  if (common_psi) {
    psi <- msm::rtnorm(1, mrho, sd=srho, lower = 0, upper = Inf)  # using truncated normal in package msm
    psiarr <- rep(psi, g)
    init_list <- c(init_list, list(psi = psi))
  } else {
    spsi <- runif(1, 0.0001, srho)
    mpsi <- msm::rtnorm(1, mrho, sd=srho, lower = 0, upper = Inf)
    psiarr <- msm::rtnorm(g, mpsi, sd=spsi, lower = 0, upper = Inf)
    init_list <- c(init_list, list(spsi = spsi, mpsi = mpsi, psiarr = psiarr))
  }
  
  # AR(1) innovation standard deviation
  if (common_tau) {
    tau <- runif(1, taul, tauu)
    tausq <- pow(tau, 2)
    tauarr <- rep(tau, g)
    tausqarr <- rep(tausq, g)
    init_list <- c(init_list, list(tau = tau))
  } else {
    tauarr <- runif(g, taul, tauu)
    tausqarr <- pow(tauarr, 2)
    init_list <- c(init_list, list(tauarr = tauarr))
  }
  
  # Group-specific AR(1) correlation parameters and innovation variances for calculations
  rhoarr <- expm1(psiarr)/(exp(psiarr)+1)
  nuarr <- tausqarr/(1-pow(rhoarr, 2))
  
  # Predictor matrix X is n x p (including intercept)
  X <- polyDesignMatrix(rts, degree = p-1L, orpol = as.integer(orpoly))
  
  # Diagonal matrix of prior standard deviations for regression parameters
  Sbetag <- sqrt(inverse(Dbetag))
  
  # Intercepts
  ag <- rnorm(g, mbetag[1,1], sd=Sbetag[1,1])
  init_list <- c(init_list, list(ag = ag))
  
  # Vector to hold trend predictions
  etamnarr <- vector("numeric", g*n)
  
  # Initial regression parameters and trend predictions
  if (common_trend) {
    if (cubic_trend) {
      b3 <- rnorm(1, mbetag[4,1], sd=Sbetag[4,4])
      b2 <- rnorm(1, mbetag[3,1], sd=Sbetag[3,3])
      b1 <- rnorm(1, mbetag[2,1], sd=Sbetag[2,2])
      init_list <- c(init_list, list(b1 = b1, b2 = b2, b3 = b3))
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1 + X[j,3]*b2 + X[j,4]*b3
        }
      }
    } else {
      if (quadratic_trend) {
        b2 <- rnorm(1, mbetag[3,1], sd=Sbetag[3,3])
        b1 <- rnorm(1, mbetag[2,1], sd=Sbetag[2,2]) 
        init_list <- c(init_list, list(b1 = b1, b2 = b2))
        for (i in 1:g) {
          for (j in 1:n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1 + X[j,3]*b2
          }
        }
      } else { 
        if (linear_trend) {
          b1 <- rnorm(1, mbetag[2,1], sd=Sbetag[2,2])
          init_list <- c(init_list, list(b1 = b1))
          for (i in 1:g) {
            for (j in 1:n) {
              etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1
            }
          }
        } else { # intercepts-only model
          for (i in 1:g) {
            for (j in 1:n) {
              etamnarr[(i-1)*n+j] <- X[j,1]*ag[i]
            }
          }
        }
      }
    }
  } else {
    if (cubic_trend) {
      b3g <- rnorm(g, mbetag[4,1], sd=Sbetag[4,4])
      b2g <- rnorm(g, mbetag[3,1], sd=Sbetag[3,3])
      b1g <- rnorm(g, mbetag[2,1], sd=Sbetag[2,2])
      init_list <- c(init_list, list(b1g = b1g, b2g = b2g, b3g = b3g))
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1g[i] + X[j,3]*b2g[i] + X[j,4]*b3g[i]
        }
      }
    } else {
      if (quadratic_trend) {
        b2g <- rnorm(g, mbetag[3,1], sd=Sbetag[3,3])
        b1g <- rnorm(g, mbetag[2,1], sd=Sbetag[2,2])
        init_list <- c(init_list, list(b1g = b1g, b2g = b2g))
        for (i in 1:g) {
          for (j in 1:n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1g[i] + X[j,3]*b2g[i]
          }
        }
      } else { # linear trend
        b1g <- rnorm(g, mbetag[2,1], sd=Sbetag[2,2])
        init_list <- c(init_list, list(b1g = b1g))
        for (i in 1:g) {
          for (j in 1:n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1g[i]
          }
        }
      }
    }
  }
  
  # True states, using recursive formula
  etaarr <- vector("numeric", g*n)
  setaarr <- vector("numeric", g*n)
  metaarr <- vector("numeric", g*n)
  for (i in 1:g) {
    setaarr[(i-1)*n+1] <- sqrt(nuarr[i])
    metaarr[(i-1)*n+1] <- etamnarr[(i-1)*n+1]
    etaarr[(i-1)*n+1] <- rnorm(1, metaarr[(i-1)*n+1], sd=setaarr[(i-1)*n+1])
    for (j in 2:n) {
      setaarr[(i-1)*n+j] <- sqrt(nuarr[i]*(1-pow(rhoarr[i], 2*(rts[j]-rts[j-1]))))
      metaarr[(i-1)*n+j] <- etamnarr[(i-1)*n+j] + pow(rhoarr[i], rts[j]-rts[j-1])*(etaarr[(i-1)*n+j-1] - etamnarr[(i-1)*n+j-1])
      etaarr[(i-1)*n+j] <- rnorm(1, metaarr[(i-1)*n+j], sd=setaarr[(i-1)*n+j])
    }
  }
  init_list <- c(init_list, list(etaarr = etaarr))
  
  # Variance parameters from inverse Gamma distribution
  if (randomVars) {
    varr <- rinvgamma(g, vshape, rate=1/vscale)
    init_list <- c(init_list, list(varr = varr))
  }
  
  return(init_list) 
  
} # bayesfit_init

# Initial values for stochastic nodes in single regression models with a level shift ('bayesfitxptl' in SAS eMKF macro)
bayesfitxptl_init <- function(cubic_trend, quadratic_trend, linear_trend, dropped_trend, common_trend, 
                              common_ar, common_psi, common_tau,
                              randomVars,
                              orpoly,
                              g, p, n, s1n,
                              rts,
                              mbetag, Dbetag,
                              mrho, srho,
                              taul, tauu,
                              vshape, vscale,
                              init_seed
) {
  
  # Basic error checks
  if (!cubic_trend && !quadratic_trend && !linear_trend && !dropped_trend) {
    stop("Bayesian level-shift trend modeling is only implemented for cubic, quadratic, linear, or intercepts-only models.")
  }
  if (!common_ar && common_psi && common_tau) {
    stop("Inconsistent indicators for AR(1) covariance structure type.")
  }
  
  # Precaution to handle intercepts-only model as a 'common' model (with b3 = b2 = b1 = 0) for initialization purposes
  if (dropped_trend && !common_trend) {
    common_trend <- TRUE
  }
  
  # List of initialized stochastic nodes to pass to NIMBLE
  init_list <- list()
  set.seed(init_seed)
  
  # AR(1) correlation coefficient
  if (common_psi) {
    psi <- msm::rtnorm(1, mrho, sd=srho, lower = 0, upper = Inf)  # using truncated normal in package msm
    psiarr <- rep(psi, g)
    init_list <- c(init_list, list(psi = psi))
  } else {
    spsi <- runif(1, 0.0001, srho)
    mpsi <- msm::rtnorm(1, mrho, sd=srho, lower = 0, upper = Inf)
    psiarr <- msm::rtnorm(g, mpsi, sd=spsi, lower = 0, upper = Inf)
    init_list <- c(init_list, list(spsi = spsi, mpsi = mpsi, psiarr = psiarr))
  }
  
  # AR(1) innovation standard deviation
  if (common_tau) {
    tau <- runif(1, taul, tauu)
    tausq <- pow(tau, 2)
    tauarr <- rep(tau, g)
    tausqarr <- rep(tausq, g)
    init_list <- c(init_list, list(tau = tau))
  } else {
    tauarr <- runif(g, taul, tauu)
    tausqarr <- pow(tauarr, 2)
    init_list <- c(init_list, list(tauarr = tauarr))
  }
  
  # Group-specific AR(1) correlation parameters and innovation variances for calculations
  rhoarr <- expm1(psiarr)/(exp(psiarr)+1)
  nuarr <- tausqarr/(1-pow(rhoarr, 2))
  
  # Predictor matrix X is n x p (including two intercepts)
  X <- polyDesignMatrix(rts, xpos = s1n+1L, xmod = 1L, degree = p-2L, orpol = as.integer(orpoly))
  
  # Diagonal matrix of prior standard deviations for regression parameters
  Sbetag <- sqrt(inverse(Dbetag))
  
  # Intercepts
  s1ag <- rnorm(g, mbetag[1,1], sd=Sbetag[1,1])
  s2ag <- rnorm(g, mbetag[2,1], sd=Sbetag[2,2])
  init_list <- c(init_list, list(s1ag = s1ag, s2ag = s2ag))
  
  # Vector to hold trend predictions
  etamnarr <- vector("numeric", g*n)
  
  # Initial regression parameters and trend predictions
  if (common_trend) {
    if (cubic_trend) {
      b3 <- rnorm(1, mbetag[5,1], sd=Sbetag[5,5])
      b2 <- rnorm(1, mbetag[4,1], sd=Sbetag[4,4])
      b1 <- rnorm(1, mbetag[3,1], sd=Sbetag[3,3])
      init_list <- c(init_list, list(b1 = b1, b2 = b2, b3 = b3))
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1 + X[j,4]*b2 + X[j,5]*b3
        }
      }
    } else {
      if (quadratic_trend) {
        b2 <- rnorm(1, mbetag[4,1], sd=Sbetag[4,4])
        b1 <- rnorm(1, mbetag[3,1], sd=Sbetag[3,3]) 
        init_list <- c(init_list, list(b1 = b1, b2 = b2))
        for (i in 1:g) {
          for (j in 1:n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1 + X[j,4]*b2
          }
        }
      } else { 
        if (linear_trend) {
          b1 <- rnorm(1, mbetag[3,1], sd=Sbetag[3,3])
          init_list <- c(init_list, list(b1 = b1))
          for (i in 1:g) {
            for (j in 1:n) {
              etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1
            }
          }
        } else { # intercepts-only model
          for (i in 1:g) {
            for (j in 1:n) {
              etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i]
            }
          }
        }
      }
    }
  } else {
    if (cubic_trend) {
      b3g <- rnorm(g, mbetag[5,1], sd=Sbetag[5,5])
      b2g <- rnorm(g, mbetag[4,1], sd=Sbetag[4,4])
      b1g <- rnorm(g, mbetag[3,1], sd=Sbetag[3,3])
      init_list <- c(init_list, list(b1g = b1g, b2g = b2g, b3g = b3g))
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1g[i] + X[j,4]*b2g[i] + X[j,5]*b3g[i]
        }
      }
    } else {
      if (quadratic_trend) {
        b2g <- rnorm(g, mbetag[4,1], sd=Sbetag[4,4])
        b1g <- rnorm(g, mbetag[3,1], sd=Sbetag[3,3])
        init_list <- c(init_list, list(b1g = b1g, b2g = b2g))
        for (i in 1:g) {
          for (j in 1:n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1g[i] + X[j,4]*b2g[i]
          }
        }
      } else { # linear trend
        b1g <- rnorm(g, mbetag[3,1], sd=Sbetag[3,3])
        init_list <- c(init_list, list(b1g = b1g))
        for (i in 1:g) {
          for (j in 1:n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1g[i]
          }
        }
      }
    }
  }
  
  # True states, using recursive formula
  etaarr <- vector("numeric", g*n)
  setaarr <- vector("numeric", g*n)
  metaarr <- vector("numeric", g*n)
  for (i in 1:g) {
    setaarr[(i-1)*n+1] <- sqrt(nuarr[i])
    metaarr[(i-1)*n+1] <- etamnarr[(i-1)*n+1]
    etaarr[(i-1)*n+1] <- rnorm(1, metaarr[(i-1)*n+1], sd=setaarr[(i-1)*n+1])
    for (j in 2:n) {
      setaarr[(i-1)*n+j] <- sqrt(nuarr[i]*(1-pow(rhoarr[i], 2*(rts[j]-rts[j-1]))))
      metaarr[(i-1)*n+j] <- etamnarr[(i-1)*n+j] + pow(rhoarr[i], rts[j]-rts[j-1])*(etaarr[(i-1)*n+j-1] - etamnarr[(i-1)*n+j-1])
      etaarr[(i-1)*n+j] <- rnorm(1, metaarr[(i-1)*n+j], sd=setaarr[(i-1)*n+j])
    }
  }
  init_list <- c(init_list, list(etaarr = etaarr))
  
  # Variance parameters from inverse Gamma distribution
  if (randomVars) {
    varr <- rinvgamma(g, vshape, rate=1/vscale)
    init_list <- c(init_list, list(varr = varr))
  }
  
  return(init_list) 
  
} # bayesfitxptl_init

# Initial values for stochastic nodes in single regression models with a full break in trend ('bayesfitxptf' in SAS eMKF macro)
bayesfitxptf_init <- function(cubic_cubic_trend, cubic_quadratic_trend, cubic_linear_trend, quadratic_quadratic_trend, 
                              quadratic_linear_trend, linear_linear_trend, dropped_dropped_trend, common_trend, 
                              common_ar, common_psi, common_tau,
                              randomVars,
                              orpoly,
                              g, p, n, s1p, s1n,
                              rts,
                              mbetag, Dbetag,
                              mrho, srho,
                              taul, tauu,
                              vshape, vscale,
                              init_seed
) {
  
  # Basic error checks
  if (!cubic_cubic_trend && !cubic_quadratic_trend && !cubic_linear_trend && 
      !quadratic_quadratic_trend && !quadratic_linear_trend && !linear_linear_trend && !dropped_dropped_trend) {
    stop("Bayesian full trend break modeling is only implemented for cubic_cubic, cubic_quadratic, cubic_linear, quadratic_quadratic, quadratic_linear, linear_linear, or intercepts-only models.")
  }
  if (!common_ar && common_psi && common_tau) {
    stop("Inconsistent indicators for AR(1) covariance structure type.")
  }
  
  # Precaution to handle intercepts-only model as a 'common' model (with b3 = b2 = b1 = 0) for initialization purposes
  if (dropped_dropped_trend && !common_trend) {
    common_trend <- TRUE
  }
  
  # List of initialized stochastic nodes to pass to NIMBLE
  init_list <- list()
  set.seed(init_seed)
  
  # AR(1) correlation coefficient
  if (common_psi) {
    psi <- msm::rtnorm(1, mrho, sd=srho, lower = 0, upper = Inf)  # using truncated normal in package msm
    psiarr <- rep(psi, g)
    init_list <- c(init_list, list(psi = psi))
  } else {
    spsi <- runif(1, 0.0001, srho)
    mpsi <- msm::rtnorm(1, mrho, sd=srho, lower = 0, upper = Inf)
    psiarr <- msm::rtnorm(g, mpsi, sd=spsi, lower = 0, upper = Inf)
    init_list <- c(init_list, list(spsi = spsi, mpsi = mpsi, psiarr = psiarr))
  }
  
  # AR(1) innovation standard deviation
  if (common_tau) {
    tau <- runif(1, taul, tauu)
    tausq <- pow(tau, 2)
    tauarr <- rep(tau, g)
    tausqarr <- rep(tausq, g)
    init_list <- c(init_list, list(tau = tau))
  } else {
    tauarr <- runif(g, taul, tauu)
    tausqarr <- pow(tauarr, 2)
    init_list <- c(init_list, list(tauarr = tauarr))
  }
  
  # Group-specific AR(1) correlation parameters and innovation variances for calculations
  rhoarr <- expm1(psiarr)/(exp(psiarr)+1)
  nuarr <- tausqarr/(1-pow(rhoarr, 2))
  
  # Predictor matrix X is n x p: block-matrix by segment, with degree(segment 2) <= degree(segment 1)
  X <- polyDesignMatrix(rts, xpos = s1n+1L, xmod = 2L, degree = s1p-1L, orpol = as.integer(orpoly))
  
  # Diagonal matrix of prior standard deviations for regression parameters
  Sbetag <- sqrt(inverse(Dbetag))
  
  # Intercepts
  s1ag <- rnorm(g, mbetag[1,1], sd=Sbetag[1,1])
  s2ag <- rnorm(g, mbetag[1+s1p,1], sd=Sbetag[1+s1p,1+s1p])
  init_list <- c(init_list, list(s1ag = s1ag, s2ag = s2ag))
  
  # Vector to hold trend predictions
  etamnarr <- vector("numeric", g*n)
  
  # Initial regression parameters and trend predictions
  if (common_trend) {
    if (cubic_cubic_trend) {
      s1b3 <- rnorm(1, mbetag[4,1], sd=Sbetag[4,4])
      s1b2 <- rnorm(1, mbetag[3,1], sd=Sbetag[3,3])
      s1b1 <- rnorm(1, mbetag[2,1], sd=Sbetag[2,2])
      s2b3 <- rnorm(1, mbetag[4+s1p,1], sd=Sbetag[4+s1p,4+s1p])
      s2b2 <- rnorm(1, mbetag[3+s1p,1], sd=Sbetag[3+s1p,3+s1p])
      s2b1 <- rnorm(1, mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
      init_list <- c(init_list, list(s1b1 = s1b1, s1b2 = s1b2, s1b3 = s1b3, s2b1 = s2b1, s2b2 = s2b2, s2b3 = s2b3))
      for (i in 1:g) {
        for (j in 1:s1n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1 + X[j,3]*s1b2 + X[j,4]*s1b3
        }
        for (j in (1+s1n):n) {
          etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1 + X[j,3+s1p]*s2b2 + X[j,4+s1p]*s2b3
        }
      }
    } else {
      if (cubic_quadratic_trend) {
        s1b3 <- rnorm(1, mbetag[4,1], sd=Sbetag[4,4])
        s1b2 <- rnorm(1, mbetag[3,1], sd=Sbetag[3,3])
        s1b1 <- rnorm(1, mbetag[2,1], sd=Sbetag[2,2])
        s2b2 <- rnorm(1, mbetag[3+s1p,1], sd=Sbetag[3+s1p,3+s1p])
        s2b1 <- rnorm(1, mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
        init_list <- c(init_list, list(s1b1 = s1b1, s1b2 = s1b2, s1b3 = s1b3, s2b1 = s2b1, s2b2 = s2b2))
        for (i in 1:g) {
          for (j in 1:s1n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1 + X[j,3]*s1b2 + X[j,4]*s1b3
          }
          for (j in (1+s1n):n) {
            etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1 + X[j,3+s1p]*s2b2
          }
        }
      } else { 
        if (cubic_linear_trend) {
          s1b3 <- rnorm(1, mbetag[4,1], sd=Sbetag[4,4])
          s1b2 <- rnorm(1, mbetag[3,1], sd=Sbetag[3,3])
          s1b1 <- rnorm(1, mbetag[2,1], sd=Sbetag[2,2])
          s2b1 <- rnorm(1, mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
          init_list <- c(init_list, list(s1b1 = s1b1, s1b2 = s1b2, s1b3 = s1b3, s2b1 = s2b1))
          for (i in 1:g) {
            for (j in 1:s1n) {
              etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1 + X[j,3]*s1b2 + X[j,4]*s1b3
            }
            for (j in (1+s1n):n) {
              etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1
            }
          }
        } else {
          if (quadratic_quadratic_trend) {
            s1b2 <- rnorm(1, mbetag[3,1], sd=Sbetag[3,3])
            s1b1 <- rnorm(1, mbetag[2,1], sd=Sbetag[2,2])
            s2b2 <- rnorm(1, mbetag[3+s1p,1], sd=Sbetag[3+s1p,3+s1p])
            s2b1 <- rnorm(1, mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
            init_list <- c(init_list, list(s1b1 = s1b1, s1b2 = s1b2, s2b1 = s2b1, s2b2 = s2b2))
            for (i in 1:g) {
              for (j in 1:s1n) {
                etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1 + X[j,3]*s1b2
              }
              for (j in (1+s1n):n) {
                etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1 + X[j,3+s1p]*s2b2
              }
            }
          } else {
            if (quadratic_linear_trend) {
              s1b2 <- rnorm(1, mbetag[3,1], sd=Sbetag[3,3])
              s1b1 <- rnorm(1, mbetag[2,1], sd=Sbetag[2,2])
              s2b1 <- rnorm(1, mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
              init_list <- c(init_list, list(s1b1 = s1b1, s1b2 = s1b2, s2b1 = s2b1))
              for (i in 1:g) {
                for (j in 1:s1n) {
                  etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1 + X[j,3]*s1b2
                }
                for (j in (1+s1n):n) {
                  etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1
                }
              }
            } else {
              if (linear_linear_trend) {
                s1b1 <- rnorm(1, mbetag[2,1], sd=Sbetag[2,2])
                s2b1 <- rnorm(1, mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
                init_list <- c(init_list, list(s1b1 = s1b1, s2b1 = s2b1))
                for (i in 1:g) {
                  for (j in 1:s1n) {
                    etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1
                  }
                  for (j in (1+s1n):n) {
                    etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1
                  }
                }
              } else { # intercepts-only model
                for (i in 1:g) {
                  for (j in 1:s1n) {
                    etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i]
                  }
                  for (j in (1+s1n):n) {
                    etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i]
                  }
                }
              }
            }
          }
        }
      }
    }
  } else {
    if (cubic_cubic_trend) {
      s1b3g <- rnorm(g, mbetag[4,1], sd=Sbetag[4,4])
      s1b2g <- rnorm(g, mbetag[3,1], sd=Sbetag[3,3])
      s1b1g <- rnorm(g, mbetag[2,1], sd=Sbetag[2,2])
      s2b3g <- rnorm(g, mbetag[4+s1p,1], sd=Sbetag[4+s1p,4+s1p])
      s2b2g <- rnorm(g, mbetag[3+s1p,1], sd=Sbetag[3+s1p,3+s1p])
      s2b1g <- rnorm(g, mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
      init_list <- c(init_list, list(s1b1g = s1b1g, s1b2g = s1b2g, s1b3g = s1b3g, s2b1g = s2b1g, s2b2g = s2b2g, s2b3g = s2b3g))
      for (i in 1:g) {
        for (j in 1:s1n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i] + X[j,4]*s1b3g[i]
        }
        for (j in (1+s1n):n) {
          etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i] + X[j,3+s1p]*s2b2g[i] + X[j,4+s1p]*s2b3g[i]
        }
      }
    } else {
      if (cubic_quadratic_trend) {
        s1b3g <- rnorm(g, mbetag[4,1], sd=Sbetag[4,4])
        s1b2g <- rnorm(g, mbetag[3,1], sd=Sbetag[3,3])
        s1b1g <- rnorm(g, mbetag[2,1], sd=Sbetag[2,2])
        s2b2g <- rnorm(g, mbetag[3+s1p,1], sd=Sbetag[3+s1p,3+s1p])
        s2b1g <- rnorm(g, mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
        init_list <- c(init_list, list(s1b1g = s1b1g, s1b2g = s1b2g, s1b3g = s1b3g, s2b1g = s2b1g, s2b2g = s2b2g))
        for (i in 1:g) {
          for (j in 1:s1n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i] + X[j,4]*s1b3g[i]
          }
          for (j in (1+s1n):n) {
            etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i] + X[j,3+s1p]*s2b2g[i]
          }
        }
      } else { 
        if (cubic_linear_trend) {
          s1b3g <- rnorm(g, mbetag[4,1], sd=Sbetag[4,4])
          s1b2g <- rnorm(g, mbetag[3,1], sd=Sbetag[3,3])
          s1b1g <- rnorm(g, mbetag[2,1], sd=Sbetag[2,2])
          s2b1g <- rnorm(g, mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
          init_list <- c(init_list, list(s1b1g = s1b1g, s1b2g = s1b2g, s1b3g = s1b3g, s2b1g = s2b1g))
          for (i in 1:g) {
            for (j in 1:s1n) {
              etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i] + X[j,4]*s1b3g[i]
            }
            for (j in (1+s1n):n) {
              etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i]
            }
          }
        } else {
          if (quadratic_quadratic_trend) {
            s1b2g <- rnorm(g, mbetag[3,1], sd=Sbetag[3,3])
            s1b1g <- rnorm(g, mbetag[2,1], sd=Sbetag[2,2])
            s2b2g <- rnorm(g, mbetag[3+s1p,1], sd=Sbetag[3+s1p,3+s1p])
            s2b1g <- rnorm(g, mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
            init_list <- c(init_list, list(s1b1g = s1b1g, s1b2g = s1b2g, s2b1g = s2b1g, s2b2g = s2b2g))
            for (i in 1:g) {
              for (j in 1:s1n) {
                etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i]
              }
              for (j in (1+s1n):n) {
                etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i] + X[j,3+s1p]*s2b2g[i]
              }
            }
          } else {
            if (quadratic_linear_trend) {
              s1b2g <- rnorm(g, mbetag[3,1], sd=Sbetag[3,3])
              s1b1g <- rnorm(g, mbetag[2,1], sd=Sbetag[2,2])
              s2b1g <- rnorm(g, mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
              init_list <- c(init_list, list(s1b1g = s1b1g, s1b2g = s1b2g, s2b1g = s2b1g))
              for (i in 1:g) {
                for (j in 1:s1n) {
                  etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i]
                }
                for (j in (1+s1n):n) {
                  etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i]
                }
              }
            } else { # linear_linear
              s1b1g <- rnorm(g, mbetag[2,1], sd=Sbetag[2,2])
              s2b1g <- rnorm(g, mbetag[2+s1p,1], sd=Sbetag[2+s1p,2+s1p])
              init_list <- c(init_list, list(s1b1g = s1b1g, s2b1g = s2b1g))
              for (i in 1:g) {
                for (j in 1:s1n) {
                  etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i]
                }
                for (j in (1+s1n):n) {
                  etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i]
                }
              }
            }
          }
        }
      }
    }
  }
  
  # True states, using recursive formula
  etaarr <- vector("numeric", g*n)
  setaarr <- vector("numeric", g*n)
  metaarr <- vector("numeric", g*n)
  for (i in 1:g) {
    setaarr[(i-1)*n+1] <- sqrt(nuarr[i])
    metaarr[(i-1)*n+1] <- etamnarr[(i-1)*n+1]
    etaarr[(i-1)*n+1] <- rnorm(1, metaarr[(i-1)*n+1], sd=setaarr[(i-1)*n+1])
    for (j in 2:n) {
      setaarr[(i-1)*n+j] <- sqrt(nuarr[i]*(1-pow(rhoarr[i], 2*(rts[j]-rts[j-1]))))
      metaarr[(i-1)*n+j] <- etamnarr[(i-1)*n+j] + pow(rhoarr[i], rts[j]-rts[j-1])*(etaarr[(i-1)*n+j-1] - etamnarr[(i-1)*n+j-1])
      etaarr[(i-1)*n+j] <- rnorm(1, metaarr[(i-1)*n+j], sd=setaarr[(i-1)*n+j])
    }
  }
  init_list <- c(init_list, list(etaarr = etaarr))
  
  # Variance parameters from inverse Gamma distribution
  if (randomVars) {
    varr <- rinvgamma(g, vshape, rate=1/vscale)
    init_list <- c(init_list, list(varr = varr))
  }
  
  return(init_list) 
  
} # bayesfitxptf_init

# Initial values for stochastic nodes in BMA regression models ('bayesBMA' in SAS eMKF macro)
bayesBMA_init <- function(cubic_trend, quadratic_trend, linear_trend, 
                          common_ar, common_psi, common_tau,
                          randomVars,
                          orpoly,
                          fp,
                          g, p, n,
                          rts,
                          mbetag, Dbetag,                          
                          mrho, srho,
                          taul, tauu,
                          vshape, vscale,
                          wshape,
                          init_seed
) {
  
  # Basic error checks
  if (!cubic_trend && !quadratic_trend && !linear_trend) {
    stop("BMA is only implemented for cubic, quadratic, or linear parent models.")
  }
  if (!common_ar && common_psi && common_tau) {
    stop("Inconsistent indicators for AR(1) covariance structure type.")
  }
    
  # List of initialized stochastic nodes to pass to NIMBLE
  init_list <- list()
  set.seed(init_seed)
  
  # AR(1) correlation coefficient
  if (common_psi) {
    psi <- msm::rtnorm(1, mrho, sd=srho, lower = 0, upper = Inf)  # using truncated normal in package msm
    psiarr <- rep(psi, g)
    init_list <- c(init_list, list(psi = psi))
  } else {
    spsi <- runif(1, 0.0001, srho)
    mpsi <- msm::rtnorm(1, mrho, sd=srho, lower = 0, upper = Inf)
    psiarr <- msm::rtnorm(g, mpsi, sd=spsi, lower = 0, upper = Inf)
    init_list <- c(init_list, list(spsi = spsi, mpsi = mpsi, psiarr = psiarr))
  }
  
  # AR(1) innovation standard deviation
  if (common_tau) {
    tau <- runif(1, taul, tauu)
    tausq <- pow(tau, 2)
    tauarr <- rep(tau, g)
    tausqarr <- rep(tausq, g)
    init_list <- c(init_list, list(tau = tau))
  } else {
    tauarr <- runif(g, taul, tauu)
    tausqarr <- pow(tauarr, 2)
    init_list <- c(init_list, list(tauarr = tauarr))
  }
  
  # Group-specific AR(1) correlation parameters and innovation variances for calculations
  rhoarr <- expm1(psiarr)/(exp(psiarr)+1)
  nuarr <- tausqarr/(1-pow(rhoarr, 2))
  
  # Predictor matrix X is n x p (including intercept)
  X <- polyDesignMatrix(rts, degree = p-1L, orpol = as.integer(orpoly))
  
  # Model flag (wtsshape currently redundant to accommodate Dirichlet for wts at a later stage)
  wtsshape <- rep(wshape, fp)
  wts <- wtsshape
  wts <- wts/sum(wts)
  
  flg <- as.integer(rcat(n=1, wts))
  init_list <- c(init_list, list(flg = flg))
  
  # Diagonal matrix of prior standard deviations for regression parameters
  Sbetag <- sqrt(inverse(Dbetag))
  
  # Intercepts
  ag <- rnorm(g, mbetag[1,1], sd=Sbetag[1,1])
  init_list <- c(init_list, list(ag = ag))
  
  # Vector to hold model predictions
  etamnarr <- vector("numeric", g*n)
  
  # Group-specific regression coefficients
  if (cubic_trend) {
    b3g <- rmixbeta3(n=1, lg=g, pmn = mbetag[4,1], psd = Sbetag[4,4], ind=flg, bma = 3L)
    b2g <- rmixbeta2(n=1, lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=flg, bma = 3L)
    b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=flg, bma = 3L)
    
    init_list <- c(init_list, list(b1g = b1g, b2g = b2g, b3g = b3g))
    
    # Trend predictions
    for (i in 1:g) {
      for (j in 1:n) {
        etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1g[i] + X[j,3]*b2g[i] + X[j,4]*b3g[i]
      }
    }
    
  } else {
    if (quadratic_trend) {
      b2g <- rmixbeta2(n=1, lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=flg, bma = 2L)
      b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=flg, bma = 2L)
      
      init_list <- c(init_list, list(b1g = b1g, b2g = b2g))
      
      # Trend predictions
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1g[i] + X[j,3]*b2g[i]
        }
      }
      
    } else { # linear_trend
      b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=flg, bma = 1L)
      
      init_list <- c(init_list, list(b1g = b1g))
      
      # Trend predictions
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*ag[i] + X[j,2]*b1g[i]
        }
      }
      
    }
  }
  
  # True states, using recursive formula
  etaarr <- vector("numeric", g*n)
  setaarr <- vector("numeric", g*n)
  metaarr <- vector("numeric", g*n)
  for (i in 1:g) {
    setaarr[(i-1)*n+1] <- sqrt(nuarr[i])
    metaarr[(i-1)*n+1] <- etamnarr[(i-1)*n+1]
    etaarr[(i-1)*n+1] <- rnorm(1, metaarr[(i-1)*n+1], sd=setaarr[(i-1)*n+1])
    for (j in 2:n) {
      setaarr[(i-1)*n+j] <- sqrt(nuarr[i]*(1-pow(rhoarr[i], 2*(rts[j]-rts[j-1]))))
      metaarr[(i-1)*n+j] <- etamnarr[(i-1)*n+j] + pow(rhoarr[i], rts[j]-rts[j-1])*(etaarr[(i-1)*n+j-1] - etamnarr[(i-1)*n+j-1])
      etaarr[(i-1)*n+j] <- rnorm(1, metaarr[(i-1)*n+j], sd=setaarr[(i-1)*n+j])
    }
  }
  init_list <- c(init_list, list(etaarr = etaarr))
  
  # Variance parameters from inverse Gamma distribution
  if (randomVars) {
    varr <- rinvgamma(g, vshape, rate=1/vscale)
    init_list <- c(init_list, list(varr = varr))
  }
  
  return(init_list) 
  
} # bayesBMA_init

# Initial values for stochastic nodes in BMA regression models with a level shift ('bayesBMAxptl' in SAS eMKF macro)
bayesBMAxptl_init <- function(cubic_trend, quadratic_trend, linear_trend, 
                              common_ar, common_psi, common_tau,
                              randomVars,
                              orpoly,
                              fp,
                              g, p, n, s1n,
                              rts,
                              mbetag, Dbetag,                          
                              mrho, srho,
                              taul, tauu,
                              vshape, vscale,
                              wshape,
                              init_seed
) {
  
  # Basic error checks
  if (!cubic_trend && !quadratic_trend && !linear_trend) {
    stop("BMA with a level shift in trend is only implemented for cubic, quadratic, or linear parent models.")
  }
  if (!common_ar && common_psi && common_tau) {
    stop("Inconsistent indicators for AR(1) covariance structure type.")
  }
  
  # List of initialized stochastic nodes to pass to NIMBLE
  init_list <- list()
  set.seed(init_seed)
  
  # AR(1) correlation coefficient
  if (common_psi) {
    psi <- msm::rtnorm(1, mrho, sd=srho, lower = 0, upper = Inf)  # using truncated normal in package msm
    psiarr <- rep(psi, g)
    init_list <- c(init_list, list(psi = psi))
  } else {
    spsi <- runif(1, 0.0001, srho)
    mpsi <- msm::rtnorm(1, mrho, sd=srho, lower = 0, upper = Inf)
    psiarr <- msm::rtnorm(g, mpsi, sd=spsi, lower = 0, upper = Inf)
    init_list <- c(init_list, list(spsi = spsi, mpsi = mpsi, psiarr = psiarr))
  }
  
  # AR(1) innovation standard deviation
  if (common_tau) {
    tau <- runif(1, taul, tauu)
    tausq <- pow(tau, 2)
    tauarr <- rep(tau, g)
    tausqarr <- rep(tausq, g)
    init_list <- c(init_list, list(tau = tau))
  } else {
    tauarr <- runif(g, taul, tauu)
    tausqarr <- pow(tauarr, 2)
    init_list <- c(init_list, list(tauarr = tauarr))
  }
  
  # Group-specific AR(1) correlation parameters and innovation variances for calculations
  rhoarr <- expm1(psiarr)/(exp(psiarr)+1)
  nuarr <- tausqarr/(1-pow(rhoarr, 2))
  
  # Predictor matrix X is n x p (including two intercepts)
  X <- polyDesignMatrix(rts, xpos = s1n+1L, xmod = 1L, degree = p-2L, orpol = as.integer(orpoly))
  
  # Model flag (wtsshape currently redundant to accommodate Dirichlet for wts at a later stage)
  wtsshape <- rep(wshape, fp)
  wts <- wtsshape
  wts <- wts/sum(wts)
  
  flg <- as.integer(rcat(n=1, wts))
  init_list <- c(init_list, list(flg = flg))
  
  # Diagonal matrix of prior standard deviations for regression parameters
  Sbetag <- sqrt(inverse(Dbetag))
  
  # Intercepts
  s1ag <- rnorm(g, mbetag[1,1], sd=Sbetag[1,1])
  s2ag <- rnorm(g, mbetag[2,1], sd=Sbetag[2,2])
  init_list <- c(init_list, list(s1ag = s1ag, s2ag = s2ag))
  
  # Vector to hold model predictions
  etamnarr <- vector("numeric", g*n)
  
  # Group-specific regression coefficients
  if (cubic_trend) {
    b3g <- rmixbeta3(n=1, lg=g, pmn = mbetag[5,1], psd = Sbetag[5,5], ind=flg, bma = 3L)
    b2g <- rmixbeta2(n=1, lg=g, pmn = mbetag[4,1], psd = Sbetag[4,4], ind=flg, bma = 3L)
    b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=flg, bma = 3L)
    
    init_list <- c(init_list, list(b1g = b1g, b2g = b2g, b3g = b3g))
    
    # Trend predictions
    for (i in 1:g) {
      for (j in 1:n) {
        etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1g[i] + X[j,4]*b2g[i] + X[j,5]*b3g[i]
      }
    }
    
  } else {
    if (quadratic_trend) {
      b2g <- rmixbeta2(n=1, lg=g, pmn = mbetag[4,1], psd = Sbetag[4,4], ind=flg, bma = 2L)
      b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=flg, bma = 2L)

      init_list <- c(init_list, list(b1g = b1g, b2g = b2g))
      
      # Trend predictions
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1g[i] + X[j,4]*b2g[i]
        }
      }
      
    } else { # linear_trend
      b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=flg, bma = 1L)

      init_list <- c(init_list, list(b1g = b1g))
      
      # Trend predictions
      for (i in 1:g) {
        for (j in 1:n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s2ag[i] + X[j,3]*b1g[i]
        }
      }
      
    }
  }
  
  # True states, using recursive formula
  etaarr <- vector("numeric", g*n)
  setaarr <- vector("numeric", g*n)
  metaarr <- vector("numeric", g*n)
  for (i in 1:g) {
    setaarr[(i-1)*n+1] <- sqrt(nuarr[i])
    metaarr[(i-1)*n+1] <- etamnarr[(i-1)*n+1]
    etaarr[(i-1)*n+1] <- rnorm(1, metaarr[(i-1)*n+1], sd=setaarr[(i-1)*n+1])
    for (j in 2:n) {
      setaarr[(i-1)*n+j] <- sqrt(nuarr[i]*(1-pow(rhoarr[i], 2*(rts[j]-rts[j-1]))))
      metaarr[(i-1)*n+j] <- etamnarr[(i-1)*n+j] + pow(rhoarr[i], rts[j]-rts[j-1])*(etaarr[(i-1)*n+j-1] - etamnarr[(i-1)*n+j-1])
      etaarr[(i-1)*n+j] <- rnorm(1, metaarr[(i-1)*n+j], sd=setaarr[(i-1)*n+j])
    }
  }
  init_list <- c(init_list, list(etaarr = etaarr))
  
  # Variance parameters from inverse Gamma distribution
  if (randomVars) {
    varr <- rinvgamma(g, vshape, rate=1/vscale)
    init_list <- c(init_list, list(varr = varr))
  }
  
  return(init_list) 
  
} # bayesBMAxptl_init

# Initial values for stochastic nodes in BMA regression models with a full break in trend ('bayesBMAxptf' in SAS eMKF macro)
bayesBMAxptf_init <- function(cubic_cubic_trend, cubic_quadratic_trend, cubic_linear_trend, 
                              quadratic_quadratic_trend, quadratic_linear_trend, linear_linear_trend,
                              common_ar, common_psi, common_tau,
                              randomVars,
                              orpoly,
                              fp,
                              g, p, n, s1p, s1n,
                              rts,
                              mbetag, Dbetag,                          
                              mrho, srho,
                              taul, tauu,
                              vshape, vscale,
                              wshape,
                              init_seed
) {
  
  # Basic error checks
  if (!cubic_cubic_trend && !cubic_quadratic_trend && !cubic_linear_trend && 
      !quadratic_quadratic_trend && !quadratic_linear_trend && !linear_linear_trend) {
    stop("BMA full trend break modeling is only implemented for cubic_cubic, cubic_quadratic, cubic_linear, quadratic_quadratic, quadratic_linear, or linear_linear parent models.")
  }
  if (!common_ar && common_psi && common_tau) {
    stop("Inconsistent indicators for AR(1) covariance structure type.")
  }
  
  # List of initialized stochastic nodes to pass to NIMBLE
  init_list <- list()
  set.seed(init_seed)
  
  # AR(1) correlation coefficient
  if (common_psi) {
    psi <- msm::rtnorm(1, mrho, sd=srho, lower = 0, upper = Inf)  # using truncated normal in package msm
    psiarr <- rep(psi, g)
    init_list <- c(init_list, list(psi = psi))
  } else {
    spsi <- runif(1, 0.0001, srho)
    mpsi <- msm::rtnorm(1, mrho, sd=srho, lower = 0, upper = Inf)
    psiarr <- msm::rtnorm(g, mpsi, sd=spsi, lower = 0, upper = Inf)
    init_list <- c(init_list, list(spsi = spsi, mpsi = mpsi, psiarr = psiarr))
  }
  
  # AR(1) innovation standard deviation
  if (common_tau) {
    tau <- runif(1, taul, tauu)
    tausq <- pow(tau, 2)
    tauarr <- rep(tau, g)
    tausqarr <- rep(tausq, g)
    init_list <- c(init_list, list(tau = tau))
  } else {
    tauarr <- runif(g, taul, tauu)
    tausqarr <- pow(tauarr, 2)
    init_list <- c(init_list, list(tauarr = tauarr))
  }
  
  # Group-specific AR(1) correlation parameters and innovation variances for calculations
  rhoarr <- expm1(psiarr)/(exp(psiarr)+1)
  nuarr <- tausqarr/(1-pow(rhoarr, 2))
  
  # Predictor matrix X is n x p: block-matrix by segment, with degree(segment 2) <= degree(segment 1)
  X <- polyDesignMatrix(rts, xpos = s1n+1L, xmod = 2L, degree = s1p-1L, orpol = as.integer(orpoly))
  
  # Prior weights for internal model flag (wtsshape currently redundant to accommodate Dirichlet for wts at a later stage)
  wtsshape <- rep(wshape, fp)
  wts <- wtsshape
  wts <- wts/sum(wts)
  
  # Model flag for allowed combinations of segment-specific flags
  flg <- as.integer(rcat(n=1, wts))
  
  # Values for segment-specific flags
  if (cubic_cubic_trend) {
    s1flg <- (flg == 1 || flg == 2 || flg == 3)*1 + (flg == 4 || flg == 5)*2 + (flg == 6)*3 + (flg == 7 || flg == 8 || flg == 9)*4 + (flg == 10 || flg == 11)*5 + (flg == 12)*6 + (flg == 13)*7  
    s2flg <- (flg == 1)*1 + (flg == 2 || flg == 4)*2 + (flg == 3 || flg == 5 || flg == 6)*3 + (flg == 7)*4 + (flg == 8 || flg == 10)*5 + (flg == 9 || flg == 11 || flg == 12)*6 + (flg == 13)*7  
  } else {
    if (cubic_quadratic_trend) {
      s1flg <- (flg == 1 || flg == 2)*1 + (flg == 3 || flg == 4)*2 + (flg == 5)*3 + (flg == 6 || flg == 7)*4 + (flg == 8 || flg == 9)*5 + (flg == 10)*6 + (flg == 11)*7  
      s2flg <- (flg == 1 || flg == 3)*1 + (flg == 2 || flg == 4 || flg == 5)*2 + (flg == 6 || flg == 8)*3 + (flg == 7 || flg == 9 || flg == 10)*4 + (flg == 11)*5  
    } else {
      if (cubic_linear_trend) {
        s1flg <- (flg == 1)*1 + (flg == 2)*2 + (flg == 3)*3 + (flg == 4)*4 + (flg == 5)*5 + (flg == 6)*6 + (flg == 7)*7  
        s2flg <- (flg == 1 || flg == 2 || flg == 3)*1 + (flg == 4 || flg == 5 || flg == 6)*2 + (flg == 7)*3  
      } else {
        if (quadratic_quadratic_trend) {
          s1flg <- (flg == 1 || flg == 2)*1 + (flg == 3)*2 + (flg == 4 || flg == 5)*3 + (flg == 6)*4 + (flg == 7)*5  
          s2flg <- (flg == 1)*1 + (flg == 2 || flg == 3)*2 + (flg == 4)*3 + (flg == 5 || flg == 6)*4 + (flg == 7)*5  
        } else {
          if (quadratic_linear_trend) {
            s1flg <- (flg == 1)*1 + (flg == 2)*2 + (flg == 3)*3 + (flg == 4)*4 + (flg == 5)*5  
            s2flg <- (flg == 1 || flg == 2)*1 + (flg == 3 || flg == 4)*2 + (flg == 5)*3   
          } else { # (linear_linear_trend)
            s1flg <- (flg == 1)*1 + (flg == 2)*2 + (flg == 3)*3
            s2flg <- (flg == 1)*1 + (flg == 2)*2 + (flg == 3)*3   
          }
        }
      }
    }
  }
  s1flg <- as.integer(s1flg)
  s2flg <- as.integer(s2flg)
  init_list <- c(init_list, list(flg = flg, s1flg = s1flg, s2flg = s2flg))
  
  # Diagonal matrix of prior standard deviations for regression parameters
  Sbetag <- sqrt(inverse(Dbetag))
  
  # Intercepts
  s1ag <- rnorm(g, mbetag[1,1], sd=Sbetag[1,1])
  s2ag <- rnorm(g, mbetag[1+s1p,1], sd=Sbetag[1+s1p,1+s1p])
  init_list <- c(init_list, list(s1ag = s1ag, s2ag = s2ag))
  
  # Vector to hold model predictions
  etamnarr <- vector("numeric", g*n)
  
  # Group-specific regression coefficients
  if (cubic_cubic_trend) {
    
    # Parameters for segment 1
    s1b3g <- rmixbeta3(n=1, lg=g, pmn = mbetag[4,1], psd = Sbetag[4,4], ind=s1flg, bma = 3L)
    s1b2g <- rmixbeta2(n=1, lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=s1flg, bma = 3L)
    s1b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=s1flg, bma = 3L)
    
    # Parameters for segment 2
    s2b3g <- rmixbeta3(n=1, lg=g, pmn = mbetag[4+s1p,1], psd = Sbetag[4+s1p,4+s1p], ind=s2flg, bma = 3L)
    s2b2g <- rmixbeta2(n=1, lg=g, pmn = mbetag[3+s1p,1], psd = Sbetag[3+s1p,3+s1p], ind=s2flg, bma = 3L)
    s2b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[2+s1p,1], psd = Sbetag[2+s1p,2+s1p], ind=s2flg, bma = 3L)
    
    init_list <- c(init_list, list(s1b1g = s1b1g, s1b2g = s1b2g, s1b3g = s1b3g, s2b1g = s2b1g, s2b2g = s2b2g, s2b3g = s2b3g))
    
    # Trend predictions
    for (i in 1:g) {
      for (j in 1:s1n) {
        etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i] + X[j,4]*s1b3g[i]
      }
      for (j in (1+s1n):n) {
        etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i] + X[j,3+s1p]*s2b2g[i] + X[j,4+s1p]*s2b3g[i]
      }
    }
    
  } else {
    if (cubic_quadratic_trend) {
      
      # Parameters for segment 1
      s1b3g <- rmixbeta3(n=1, lg=g, pmn = mbetag[4,1], psd = Sbetag[4,4], ind=s1flg, bma = 3L)
      s1b2g <- rmixbeta2(n=1, lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=s1flg, bma = 3L)
      s1b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=s1flg, bma = 3L)
      
      # Parameters for segment 2
      s2b2g <- rmixbeta2(n=1, lg=g, pmn = mbetag[3+s1p,1], psd = Sbetag[3+s1p,3+s1p], ind=s2flg, bma = 2L)
      s2b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[2+s1p,1], psd = Sbetag[2+s1p,2+s1p], ind=s2flg, bma = 2L)
      
      init_list <- c(init_list, list(s1b1g = s1b1g, s1b2g = s1b2g, s1b3g = s1b3g, s2b1g = s2b1g, s2b2g = s2b2g))
      
      # Trend predictions
      for (i in 1:g) {
        for (j in 1:s1n) {
          etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i] + X[j,4]*s1b3g[i]
        }
        for (j in (1+s1n):n) {
          etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i] + X[j,3+s1p]*s2b2g[i]
        }
      }
      
    } else {
      if (cubic_linear_trend) {
        
        # Parameters for segment 1
        s1b3g <- rmixbeta3(n=1, lg=g, pmn = mbetag[4,1], psd = Sbetag[4,4], ind=s1flg, bma = 3L)
        s1b2g <- rmixbeta2(n=1, lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=s1flg, bma = 3L)
        s1b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=s1flg, bma = 3L)
        
        # Parameters for segment 2
        s2b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[2+s1p,1], psd = Sbetag[2+s1p,2+s1p], ind=s2flg, bma = 1L)
        
        init_list <- c(init_list, list(s1b1g = s1b1g, s1b2g = s1b2g, s1b3g = s1b3g, s2b1g = s2b1g))
        
        # Trend predictions
        for (i in 1:g) {
          for (j in 1:s1n) {
            etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i] + X[j,4]*s1b3g[i]
          }
          for (j in (1+s1n):n) {
            etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i]
          }
        }
        
      } else {
        if (quadratic_quadratic_trend) {
          
          # Parameters for segment 1
          s1b2g <- rmixbeta2(n=1, lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=s1flg, bma = 2L)
          s1b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=s1flg, bma = 2L)
          
          # Parameters for segment 2
          s2b2g <- rmixbeta2(n=1, lg=g, pmn = mbetag[3+s1p,1], psd = Sbetag[3+s1p,3+s1p], ind=s2flg, bma = 2L)
          s2b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[2+s1p,1], psd = Sbetag[2+s1p,2+s1p], ind=s2flg, bma = 2L)
          
          init_list <- c(init_list, list(s1b1g = s1b1g, s1b2g = s1b2g, s2b1g = s2b1g, s2b2g = s2b2g))
          
          # Trend predictions
          for (i in 1:g) {
            for (j in 1:s1n) {
              etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i]
            }
            for (j in (1+s1n):n) {
              etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i] + X[j,3+s1p]*s2b2g[i]
            }
          }
          
        } else {
          if (quadratic_linear_trend) {
            
            # Parameters for segment 1
            s1b2g <- rmixbeta2(n=1, lg=g, pmn = mbetag[3,1], psd = Sbetag[3,3], ind=s1flg, bma = 2L)
            s1b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=s1flg, bma = 2L)
            
            # Parameters for segment 2
            s2b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[2+s1p,1], psd = Sbetag[2+s1p,2+s1p], ind=s2flg, bma = 1L)
            
            init_list <- c(init_list, list(s1b1g = s1b1g, s1b2g = s1b2g, s2b1g = s2b1g))
            
            # Trend predictions
            for (i in 1:g) {
              for (j in 1:s1n) {
                etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i] + X[j,3]*s1b2g[i]
              }
              for (j in (1+s1n):n) {
                etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i]
              }
            }
            
          } else { # (linear_linear_trend)
            
            # Parameters for segment 1
            s1b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[2,1], psd = Sbetag[2,2], ind=s1flg, bma = 1L)
            
            # Parameters for segment 2
            s2b1g <- rmixbeta1(n=1, lg=g, pmn = mbetag[2+s1p,1], psd = Sbetag[2+s1p,2+s1p], ind=s2flg, bma = 1L)
            
            init_list <- c(init_list, list(s1b1g = s1b1g, s2b1g = s2b1g))
            
            # Trend predictions
            for (i in 1:g) {
              for (j in 1:s1n) {
                etamnarr[(i-1)*n+j] <- X[j,1]*s1ag[i] + X[j,2]*s1b1g[i]
              }
              for (j in (1+s1n):n) {
                etamnarr[(i-1)*n+j] <- X[j,1+s1p]*s2ag[i] + X[j,2+s1p]*s2b1g[i]
              }
            }
            
          }
        }
      }
    }
  }
  
  # True states, using recursive formula
  etaarr <- vector("numeric", g*n)
  setaarr <- vector("numeric", g*n)
  metaarr <- vector("numeric", g*n)
  for (i in 1:g) {
    setaarr[(i-1)*n+1] <- sqrt(nuarr[i])
    metaarr[(i-1)*n+1] <- etamnarr[(i-1)*n+1]
    etaarr[(i-1)*n+1] <- rnorm(1, metaarr[(i-1)*n+1], sd=setaarr[(i-1)*n+1])
    for (j in 2:n) {
      setaarr[(i-1)*n+j] <- sqrt(nuarr[i]*(1-pow(rhoarr[i], 2*(rts[j]-rts[j-1]))))
      metaarr[(i-1)*n+j] <- etamnarr[(i-1)*n+j] + pow(rhoarr[i], rts[j]-rts[j-1])*(etaarr[(i-1)*n+j-1] - etamnarr[(i-1)*n+j-1])
      etaarr[(i-1)*n+j] <- rnorm(1, metaarr[(i-1)*n+j], sd=setaarr[(i-1)*n+j])
    }
  }
  init_list <- c(init_list, list(etaarr = etaarr))
  
  # Variance parameters from inverse Gamma distribution
  if (randomVars) {
    varr <- rinvgamma(g, vshape, rate=1/vscale)
    init_list <- c(init_list, list(varr = varr))
  }
  
  return(init_list) 
  
} # bayesBMAxptf_init

###############################################################
# R-side function to set up data list for use in NIMBLE model #
###############################################################

eMKFdataList <- function(dataName, 
                         subset,
                         p, fp, g, n, s1p, s1n, outcome, se, neff,
                         malpha, palpha, mbeta1, pbeta1, mbeta2, pbeta2, mbeta3, pbeta3,
                         s2malpha, s2palpha, s2mbeta1, s2pbeta1, s2mbeta2, s2pbeta2, s2mbeta3, s2pbeta3,
                         mrho, prho, taul, tauu, randomVars, vshape, vscale, wshape,
                         envir = .GlobalEnv) {
  
  # Extract data vectors
  Yarr <- get(dataName, envir = envir)[subset, outcome]
  S2arr <- get(dataName, envir = envir)[subset, se]^2
  if (randomVars) {
    Narr <- get(dataName, envir = envir)[subset, neff]
  } else {
    Narr <- rep(as.numeric(NA), g * n)
  }
  
  # Data quantiles to use for setting prior parameters
  rangeY <- diff(range(Yarr))
  if (s1n < n) {
    s1rangeY <- diff(range(Yarr[1:s1n]))
    s2rangeY <- diff(range(Yarr[(1+s1n):n]))
  } else {
    s1rangeY <- as.numeric(NA)
    s2rangeY <- as.numeric(NA)
  }
  if (randomVars) {
    qrangeV <- IQR(S2arr)
    medianV <- median(S2arr)
  } else {
    qrangeV <- as.numeric(NA)
    medianV <- as.numeric(NA)
  }
  
  # Prior mean parameters for regression coefficients
  if (s1n < n) {
    if (is.na(malpha)) { s1bmalpha <- 0.5 * s1rangeY } else {s1bmalpha <- malpha}
    if (is.na(s2malpha)) { s2bmalpha <- 0.5 * s2rangeY } else {s2bmalpha <- s2malpha}
  } else {
    if (is.na(malpha)) { bmalpha <- 0.5 * rangeY } else {bmalpha <- malpha}
  }
  if (p > 1+s1p) {
    if (is.na(mbeta1)) { s1bmbeta1 <- 0 } else {s1bmbeta1 <- mbeta1}
    if (is.na(mbeta2)) { s1bmbeta2 <- 0 } else {s1bmbeta2 <- mbeta2}
    if (is.na(mbeta3)) { s1bmbeta3 <- 0 } else {s1bmbeta3 <- mbeta3}
    if (is.na(s2mbeta1)) { s2bmbeta1 <- 0 } else {s2bmbeta1 <- s2mbeta1}
    if (is.na(s2mbeta2)) { s2bmbeta2 <- 0 } else {s2bmbeta2 <- s2mbeta2}
    if (is.na(s2mbeta3)) { s2bmbeta3 <- 0 } else {s2bmbeta3 <- s2mbeta3}
  } else {
    if (is.na(mbeta1)) { bmbeta1 <- 0 } else {bmbeta1 <- mbeta1}
    if (is.na(mbeta2)) { bmbeta2 <- 0 } else {bmbeta2 <- mbeta2}
    if (is.na(mbeta3)) { bmbeta3 <- 0 } else {bmbeta3 <- mbeta3}
  }
  
  # Set cubic and quadratic precisions so that the coefficients tend to be smaller in magnitude as the degree increases
  if (s1n < n) {
    if (is.na(palpha)) { s1bpalpha <- 0.000001 / (s1rangeY^2) } else {s1bpalpha <- palpha}
    if (is.na(s2palpha)) { s2bpalpha <- 0.000001 / (s2rangeY^2) } else {s2bpalpha <- s2palpha}    
  } else {
    if (is.na(palpha)) { bpalpha <- 0.000001 / (rangeY^2) } else {bpalpha <- palpha}
  }
  if (p > 1+s1p) {
    if (is.na(pbeta1)) { s1bpbeta1 <- 0.000001 / (s1rangeY^2) } else {s1bpbeta1 <- pbeta1}
    if (is.na(pbeta2)) { s1bpbeta2 <- 2 * s1bpbeta1 } else {s1bpbeta2 <- pbeta2}
    if (is.na(pbeta3)) { s1bpbeta3 <- 4 * s1bpbeta1 } else {s1bpbeta3 <- pbeta3}
    if (is.na(s2pbeta1)) { s2bpbeta1 <- 0.000001 / (s2rangeY^2) } else {s2bpbeta1 <- s2pbeta1}
    if (is.na(s2pbeta2)) { s2bpbeta2 <- 2 * s2bpbeta1 } else {s2bpbeta2 <- s2pbeta2}
    if (is.na(s2pbeta3)) { s2bpbeta3 <- 4 * s2bpbeta1 } else {s2bpbeta3 <- s2pbeta3}
  } else {
    if (is.na(pbeta1)) { bpbeta1 <- 0.000001 / (rangeY^2) } else {bpbeta1 <- pbeta1}
    if (is.na(pbeta2)) { bpbeta2 <- 2 * bpbeta1 } else {bpbeta2 <- pbeta2}
    if (is.na(pbeta3)) { bpbeta3 <- 4 * bpbeta1 } else {bpbeta3 <- pbeta3}
  }
  
  # Set up hyperprior mean vector and precision matrix to pass as constants to NIMBLE model
  if (p > (1+s1p)) {
    if (s1p == 4) {
      if (p == 8) {
        mbetag <- matrix(c(s1bmalpha, s1bmbeta1, s1bmbeta2, s1bmbeta3, s2bmalpha, s2bmbeta1, s2bmbeta2, s2bmbeta3), 8, 1)
        Dbetag <- diag(c(s1bpalpha, s1bpbeta1, s1bpbeta2, s1bpbeta3, s2bpalpha, s2bpbeta1, s2bpbeta2, s2bpbeta3))
      } else {
        if (p == 7) {
          mbetag <- matrix(c(s1bmalpha, s1bmbeta1, s1bmbeta2, s1bmbeta3, s2bmalpha, s2bmbeta1, s2bmbeta2), 7, 1)
          Dbetag <- diag(c(s1bpalpha, s1bpbeta1, s1bpbeta2, s1bpbeta3, s2bpalpha, s2bpbeta1, s2bpbeta2))
        } else {
          mbetag <- matrix(c(s1bmalpha, s1bmbeta1, s1bmbeta2, s1bmbeta3, s2bmalpha, s2bmbeta1), 6, 1)
          Dbetag <- diag(c(s1bpalpha, s1bpbeta1, s1bpbeta2, s1bpbeta3, s2bpalpha, s2bpbeta1))
        }
      }
    } else {
      if (s1p == 3) {
        if (p == 6) {
          mbetag <- matrix(c(s1bmalpha, s1bmbeta1, s1bmbeta2, s2bmalpha, s2bmbeta1, s2bmbeta2), 6, 1)
          Dbetag <- diag(c(s1bpalpha, s1bpbeta1, s1bpbeta2, s2bpalpha, s2bpbeta1, s2bpbeta2))
        } else {
          mbetag <- matrix(c(s1bmalpha, s1bmbeta1, s1bmbeta2, s2bmalpha, s2bmbeta1), 5, 1)
          Dbetag <- diag(c(s1bpalpha, s1bpbeta1, s1bpbeta2, s2bpalpha, s2bpbeta1))
        }
      } else {
        mbetag <- matrix(c(s1bmalpha, s1bmbeta1, s2bmalpha, s2bmbeta1), 4, 1)
        Dbetag <- diag(c(s1bpalpha, s1bpbeta1, s2bpalpha, s2bpbeta1))
      }
    }
  } else {
    if (p == (1+s1p)) {
      if (p == 5) {
        mbetag <- matrix(c(s1bmalpha, s2bmalpha, bmbeta1, bmbeta2, bmbeta3), 5, 1)
        Dbetag <- diag(c(s1bpalpha, s2bpalpha, bpbeta1, bpbeta2, bpbeta3))
      } else {
        if (p == 4) {
          mbetag <- matrix(c(s1bmalpha, s2bmalpha, bmbeta1, bmbeta2), 4, 1)
          Dbetag <- diag(c(s1bpalpha, s2bpalpha, bpbeta1, bpbeta2))
        } else {
          if (p == 3) {
            mbetag <- matrix(c(s1bmalpha, s2bmalpha, bmbeta1), 3, 1)
            Dbetag <- diag(c(s1bpalpha, s2bpalpha, bpbeta1))
          } else {
            mbetag <- matrix(c(s1bmalpha, s2bmalpha), 2, 1)
            Dbetag <- diag(c(s1bpalpha, s2bpalpha))
          }
        }
      }
    } else {
      if (p == 4) {
        mbetag <- matrix(c(bmalpha, bmbeta1, bmbeta2, bmbeta3), 4, 1)
        Dbetag <- diag(c(bpalpha, bpbeta1, bpbeta2, bpbeta3))
      } else {
        if (p == 3) {
          mbetag <- matrix(c(bmalpha, bmbeta1, bmbeta2), 3, 1)
          Dbetag <- diag(c(bpalpha, bpbeta1, bpbeta2))
        } else {
          if (p == 2) {
            mbetag <- matrix(c(bmalpha, bmbeta1), 2, 1)
            Dbetag <- diag(c(bpalpha, bpbeta1))
          } else {
            mbetag <- matrix(bmalpha, 1, 1)
            Dbetag <- matrix(bpalpha, 1, 1)
          }
        }
      }
    }
  }
  
  # Parameters for priors of transformed AR(1) coefficients
  if (!is.na(mrho) && mrho < 0) {
    message("Prior mean 'mrho' is expected to be positive: its absolute value will be used instead.")
  }
  if (is.na(mrho)) { bmrho <- 0 } else {bmrho <- abs(mrho)}
  if (is.na(prho)) { bprho <- 1 } else {bprho <- prho}
  bsrho <- sqrt(1/prho)
  if (is.na(taul)) { btaul <- 0.0001 } else {btaul <- taul}
  if (is.na(tauu)) { btauu <- 0.1 * rangeY } else {btauu <- tauu}
  
  # Use median for mean and 10 times IQR for standard deviation of sampling variances (inverse gamma prior)
  if (randomVars) {
    if (is.na(vshape)) { bvshape <- 2 + (medianV^2 /((10 * qrangeV)^2)) } else {bvshape <- vshape}
    if (is.na(vscale)) { bvscale <- (bvshape - 1) / medianV } else {bvscale <- vscale}
  } else {
    bvshape <- as.numeric(NA)
    bvscale <- as.numeric(NA)
  }
  
  # Shape for Dirichlet (defaults to all equal)
  if (is.na(wshape)) { bwshape <- 2 } else {bwshape <- wshape}
  
  # Data list to pass to NIMBLE
  bdata_list <- list(Yarr = Yarr, S2arr = S2arr, mbetag = mbetag, Dbetag = Dbetag,
                     mrho = bmrho, srho = bsrho, taul = btaul, tauu = btauu)
  
  # Append 'Narr', vshape, and vscale to use with random variances option
  if (randomVars) {
    bdata_list <- c(bdata_list, list(Narr = Narr, vshape = bvshape, vscale = bvscale))
  }
  # Add 'wshape' to the data for BMA models
  if (fp > 1) { bdata_list <- c(bdata_list, list(wshape = bwshape)) } 
  
  return(bdata_list)
} # eMKFdataList

##################################################################
# R-side function for post-processing calculation of disparities #
##################################################################

eMKFpostDisparities <- function(dataName, colNames, envir = .GlobalEnv) {
  
  groupData <- get(dataName, envir = envir)
  
  stopifnot(coda::is.mcmc.list(groupData) && is.character(colNames))
  
  if (length(setdiff(colNames, coda::varnames(groupData))) > 0) {
    stop("The names in 'colNames' are not a proper subset of the sampled variable names.")
  }
  
  groupData <- groupData[, colNames, drop = FALSE]
  
  g <- coda::nvar(groupData)
  nchains <- coda::nchain(groupData)
  nmcmc <- coda::niter(groupData)
  nthin <- coda::thin(groupData)
  nstart <- start(groupData)
  nend <- end(groupData)
  
  i1 <- 0L
  i2 <- 0L
  chain <- 0L
  
  # New column names and mcmc.list object to hold disparities calculations
  newCols <- c("MIN", "MAX", "AVG_excl_MIN", "AVG_excl_MAX", 
               "MRD", "MRR", "SRD_adv", "SRR_adv", "SRD_fav", "SRR_fav")
  for (i1 in 1:g) {
    for (i2 in 1:g) {
      newCols <- c(newCols, paste("diff", i1, i2, sep="_"))
    }
  }
  for (i1 in 1:g) {
    for (i2 in 1:g) {
      newCols <- c(newCols, paste("ratio", i1, i2, sep="_"))
    }
  }
  for (i1 in 1:g) {
    newCols <- c(newCols, paste("diff", i1, "MIN", sep="_"))
  }
  for (i1 in 1:g) {
    newCols <- c(newCols, paste("ratio", i1, "MIN", sep="_"))
  }
  for (i2 in 1:g) {
    newCols <- c(newCols, paste("diff", "MAX", i2, sep="_"))
  }
  for (i2 in 1:g) {
    newCols <- c(newCols, paste("ratio", "MAX", i2, sep="_"))
  }
  dispData <- vector("list", nchains)
  names(dispData) <- names(groupData)
  for (chain in 1:nchains) {
    dispData[[chain]] <- coda::mcmc(matrix(nrow = nmcmc, ncol = as.integer(10+2*g*g+4*g), 
                                           dimnames = list(NULL, newCols)), 
                                    start = nstart, end = nend, thin = nthin)
  }
  
  # Calculate disparities for each chain
  for (chain in 1:nchains) {
    
    # Min and max
    dispData[[chain]][,"MIN"] <- apply(groupData[[chain]], 1, min)
    dispData[[chain]][,"MAX"] <- apply(groupData[[chain]], 1, max)
    
    # Averages excluding min and max
    dispData[[chain]][,"AVG_excl_MIN"] <- (apply(groupData[[chain]], 1, sum) - dispData[[chain]][,"MIN"])/(g-1)
    dispData[[chain]][,"AVG_excl_MAX"] <- (apply(groupData[[chain]], 1, sum) - dispData[[chain]][,"MAX"])/(g-1)
    
    # Maximal and summary measures
    dispData[[chain]][,"MRD"] <- dispData[[chain]][,"MAX"] - dispData[[chain]][,"MIN"]
    dispData[[chain]][,"MRR"] <- ifelse(abs(dispData[[chain]][,"MIN"]) > 0,  
                                        dispData[[chain]][,"MAX"]/dispData[[chain]][,"MIN"], NA)
    dispData[[chain]][,"SRD_adv"] <- dispData[[chain]][,"AVG_excl_MIN"] - dispData[[chain]][,"MIN"]
    dispData[[chain]][,"SRR_adv"] <- ifelse(abs(dispData[[chain]][,"MIN"]) > 0, 
                                            dispData[[chain]][,"AVG_excl_MIN"]/dispData[[chain]][,"MIN"], NA)
    dispData[[chain]][,"SRD_fav"] <- dispData[[chain]][,"MAX"] - dispData[[chain]][,"AVG_excl_MAX"]
    dispData[[chain]][,"SRR_fav"] <- ifelse(abs(dispData[[chain]][,"AVG_excl_MAX"]) > 0, 
                                            dispData[[chain]][,"MAX"]/dispData[[chain]][,"AVG_excl_MAX"], NA)
    
    # All pairwise measures
    for (i1 in 1:g) {
      for (i2 in 1:g) {
        dispData[[chain]][, paste("diff",i1,i2,sep="_")] <- groupData[[chain]][,i1] - groupData[[chain]][,i2]
      }
    }
    for (i1 in 1:g) {
      for (i2 in 1:g) {
        dispData[[chain]][, paste("ratio",i1,i2,sep="_")] <- ifelse(abs(groupData[[chain]][,i2]) > 0, 
                                                                    groupData[[chain]][,i1]/groupData[[chain]][,i2], NA)
      }
    }
    
    # Comparisons to the min
    for (i1 in 1:g) {
      dispData[[chain]][, paste("diff",i1,"MIN",sep="_")] <- groupData[[chain]][,i1] - dispData[[chain]][,"MIN"]
    }
    for (i1 in 1:g) {
      dispData[[chain]][, paste("ratio",i1,"MIN",sep="_")] <- ifelse(abs(dispData[[chain]][,"MIN"]) > 0, 
                                                                     groupData[[chain]][,i1]/dispData[[chain]][,"MIN"], NA)
    }
    
    # Comparisons to the max
    for (i2 in 1:g) {
      dispData[[chain]][, paste("diff","MAX",i2,sep="_")] <- dispData[[chain]][,"MAX"] - groupData[[chain]][,i2]
    }
    for (i2 in 1:g) {
      dispData[[chain]][, paste("ratio","MAX",i2,sep="_")] <- ifelse(abs(groupData[[chain]][,i2]) > 0, 
                                                                     dispData[[chain]][,"MAX"]/groupData[[chain]][,i2], NA)
    }
    
  }
  rm(groupData)
  return(coda::as.mcmc.list(dispData))
  
} # eMKFpostDisparities

##############################################################################################
# R-side function for calculation of rank-normalized scores using the Blom method            #                                                                                            #
#                                                                                            #
# Implementation mimics behavior of SAS PROC RANK with TIES = AVERAGE and NORMAL = BLOM.     #
# From SAS manual: PROC RANK computes normal score from the ranks based on non-tied values   #
# and applies the TIES= specification to the resulting score. Thus, the TIES= specification  #
# applies to the normal score, not to the rank that is used to compute the normal score...   #
##############################################################################################

rankNorm <- function(x) {
  xx <- x[!is.na(x)]
  nx <- length(xx)
  qx <- qnorm((rank(x, ties.method = "first", na.last = "keep")-0.375)/(nx+0.25))
  if (anyDuplicated(xx) > 0) {
    xu <- unique(xx[duplicated(xx)])
    for (u in seq_along(xu)) {
      xcond <- !is.na(x) & (x == xu[u]) 
      qx[xcond] <- mean(qx[xcond])
    }
    rm(xcond, xu)
  }
  rm(xx)
  
  return(qx)
} # rankNorm

#########################################################################################
# R-side function for calculation of within- and between-chain variances and split-Rhat #
#########################################################################################

eMKFpostProcessing <- function(dataName, colNames=character(0L), outName = "param",
                               frequencies.only = FALSE, fp = 1L, bayesModel = "",
                               compute.splitRhats = !frequencies.only, 
                               compute.HPDintervals = !frequencies.only,
                               threshold = 1.01,
                               envir = .GlobalEnv) {
  
  postData <- get(dataName, envir = envir)
  
  stopifnot(coda::is.mcmc.list(postData) && is.character(colNames))
  
  if (length(colNames) == 0) { # default to all sampled variables
    colNames <- coda::varnames(postData)
  } else {
    if (length(setdiff(colNames, coda::varnames(postData))) > 0) {
      stop("The names in 'colNames' are not a proper subset of the sampled variable names.")
    }
  }
  
  postData <- postData[, colNames, drop = FALSE]
  
  nvars <- coda::nvar(postData)
  nchains <- coda::nchain(postData)
  nmcmc <- coda::niter(postData)
  nthin <- coda::thin(postData)
  nstart <- start(postData)
  nend <- end(postData)
  
  # Drop first row if chain length is not even
  if ((floor(nmcmc/nthin) %% 2) != 0) {
    postData <- window(postData, start = nstart + 1)
  }
  
  # Split each chain in half
  postDataHalves <- vector("list", 2*nchains)
  names(postDataHalves) <- paste(rep(coda::chanames(postData), rep(2, nchains)), rep(c(1,2), nchains), sep=".")
  for (chain in 1:nchains) {
    postDataHalves[[2*chain-1]] <- window(postData[[chain]], end = floor((nmcmc/nthin)/2))
    postDataHalves[[2*chain]] <- window(postData[[chain]], start = floor((nmcmc/nthin)/2) + 1)
    attributes(postDataHalves[[2*chain]])$mcpar <- coda::mcpar(postDataHalves[[2*chain - 1]])
  }
  postDataHalves <- coda::as.mcmc.list(postDataHalves)
  rm(postData)
  
  if (frequencies.only) {
    
    #########################################################
    # Posterior frequencies (only for the model indicators) #
    #########################################################
    
    # Error/consistency checks
    fp <- as.integer(fp)
    stopifnot(length(fp) == 1)
    stopifnot(bayesModel != "")
    
    if (bayesModel == "bma_cubic") { stopifnot(fp == 7, length(colNames) == 1) }
    if (bayesModel == "bma_quad") { stopifnot(fp == 5, length(colNames) == 1) }
    if (bayesModel == "bma_linear") { stopifnot(fp == 3, length(colNames) == 1) }
    if (bayesModel == "bma_cubic_cubic") { stopifnot(fp == 13, length(colNames) == 2); s1fp <- 7L; s2fp <- 7L }
    if (bayesModel == "bma_cubic_quad") { stopifnot(fp == 11, length(colNames) == 2); s1fp <- 7L; s2fp <- 5L }
    if (bayesModel == "bma_cubic_linear") { stopifnot(fp == 7, length(colNames) == 2); s1fp <- 7L; s2fp <- 3L }
    if (bayesModel == "bma_quad_quad") { stopifnot(fp == 7, length(colNames) == 2); s1fp <- 5L; s2fp <- 5L }
    if (bayesModel == "bma_quad_linear") { stopifnot(fp == 5, length(colNames) == 2); s1fp <- 5L; s2fp <- 3L }
    if (bayesModel == "bma_linear_linear") { stopifnot(fp == 3, length(colNames) == 2); s1fp <- 3L; s2fp <- 3L }
    
    if (length(colNames) == 1) {
      stopifnot(all(sapply(postDataHalves, 
                           function(chamat) { all(chamat[,1] <= fp, na.rm = TRUE) && all(chamat[,1] >= 1, na.rm = TRUE) }))) 
    }
    if (length(colNames) == 2) {
      stopifnot(all(sapply(postDataHalves, 
                           function(chamat) { all(chamat[,1] <= s1fp, na.rm = TRUE) && all(chamat[,1] >= 1, na.rm = TRUE) }))) 
      stopifnot(all(sapply(postDataHalves, 
                           function(chamat) { all(chamat[,2] <= s2fp, na.rm = TRUE) && all(chamat[,2] >= 1, na.rm = TRUE) }))) 
    }

    # Create model labels
    if (bayesModel == "bma_linear") {
      intnames <- c(1:3)
      altnames <- c("indep_linear", "common_linear", "dropped")
    } else {
      if (bayesModel == "bma_quad") {
        intnames <- c(1:5)
        altnames <- c("indep_quad", "indep_linear", "common_quad", "common_linear", "dropped")
      } else {
        if (bayesModel == "bma_cubic") {
          intnames <- c(1:7)
          altnames <- c("indep_cubic", "indep_quad", "indep_linear", "common_cubic", "common_quad", "common_linear", "dropped")
        } else {
          if (bayesModel == "bma_linear_linear") {
            s1intnames <- c(1:3); s2intnames <- c(1:3)
            s1altnames <- c("indep_linear", "common_linear", "dropped")
            s2altnames <- c("indep_linear", "common_linear", "dropped")
          } else {
            if (bayesModel == "bma_quad_linear") {
              s1intnames <- c(1:5); s2intnames <- c(1:3)
              s1altnames <- c("indep_quad", "indep_linear", "common_quad", "common_linear", "dropped")
              s2altnames <- c("indep_linear", "common_linear", "dropped")
            } else {
              if (bayesModel == "bma_quad_quad") {
                s1intnames <- c(1:5); s2intnames <- c(1:5)
                s1altnames <- c("indep_quad", "indep_linear", "common_quad", "common_linear", "dropped")
                s2altnames <- c("indep_quad", "indep_linear", "common_quad", "common_linear", "dropped")
              } else {
                if (bayesModel == "bma_cubic_linear") {
                  s1intnames <- c(1:7); s2intnames <- c(1:3)
                  s1altnames <- c("indep_cubic", "indep_quad", "indep_linear", "common_cubic", "common_quad", "common_linear", "dropped")
                  s2altnames <- c("indep_linear", "common_linear", "dropped")
                } else {
                  if (bayesModel == "bma_cubic_quad") {
                    s1intnames <- c(1:7); s2intnames <- c(1:5)
                    s1altnames <- c("indep_cubic", "indep_quad", "indep_linear", "common_cubic", "common_quad", "common_linear", "dropped")
                    s2altnames <- c("indep_quad", "indep_linear", "common_quad", "common_linear", "dropped")
                  } else {
                    s1intnames <- c(1:7); s2intnames <- c(1:7)
                    s1altnames <- c("indep_cubic", "indep_quad", "indep_linear", "common_cubic", "common_quad", "common_linear", "dropped")
                    s2altnames <- c("indep_cubic", "indep_quad", "indep_linear", "common_cubic", "common_quad", "common_linear", "dropped")
                  }
                }
              }
            }
          }
        }
      }
    }
    
    # Proportions within (half-)chains
    if (length(colNames) == 1) {
      xmn <- sapply(postDataHalves, 
                    function(chamat) { tabulate(chamat[,1], nbins = fp)/sum(!is.na(chamat[,1])) }) 
    }
    if (length(colNames) == 2) {
      xmn <- sapply(postDataHalves, 
                    function(chamat) { tabulate(chamat[,1], nbins = s1fp)/sum(!is.na(chamat[,1])) }) 
      xmn <- rbind(xmn, 
                   sapply(postDataHalves, 
                          function(chamat) { tabulate(chamat[,2], nbins = s2fp)/sum(!is.na(chamat[,2])) })
                   )
    }
    
    # Variances within (half-)chains
    if ((floor(nmcmc/nthin) %% 2) != 0) {
      xvr <- xmn*(1-xmn)/((floor(nmcmc/nthin)-1)/2)
    } else {
      xvr <- xmn*(1-xmn)/(floor(nmcmc/nthin)/2)
    }
    
    # Average within-chain variances
    xwv <- t(t(apply(xvr, 1, function(x) mean(x, na.rm = TRUE))))

    # Between-chain variances
    xbv <- t(t(apply(xmn, 1, function(x) var(x, na.rm = TRUE))))
    
    # Means across chains
    xmn <- t(t(apply(xmn, 1, function(x) mean(x, na.rm = TRUE)))) 
    
    # Pooled posterior variances
    if ((floor(nmcmc/nthin) %% 2) != 0) {
      xpv <- xbv + (1-1/((floor(nmcmc/nthin)-1)/2))*xwv
    } else {
      xpv <- xbv + (1-1/(floor(nmcmc/nthin)/2))*xwv
    }
  
    # Return the final MCMC estimates for the posterior proportions and their SDs and clean up
    if (length(colNames) == 1) {
      ppv <- cbind(intnames, xmn, sqrt(xpv))
      dimnames(ppv) <- list(altnames, c("Indicator", "Weight", "SD"))
    }
    if (length(colNames) == 2) {
      s1altnames <- paste("seg1", s1altnames, sep = "_")
      s2altnames <- paste("seg2", s2altnames, sep = "_")
      ppv <- cbind(c(rep(1, s1fp), rep(2, s2fp)), c(s1intnames, s2intnames), xmn, sqrt(xpv))
      dimnames(ppv) <- list(c(s1altnames, s2altnames), c("Segment", "Indicator", "Weight", "SD"))
    }
    rm(xmn, xbv, xwv, xpv, postDataHalves)
    
    return(ppv)
    
  } else {
    
    #####################################
    # HPD credible intervals using coda #
    #####################################
    
    if (compute.HPDintervals) {
      
      # Call coda::HPDinterval treating all (half-)chains as one single pooled chain
      xhpddata <- coda::as.mcmc(do.call(rbind, postDataHalves))
      xhpd <- coda::HPDinterval(xhpddata)[,1:2]
      rm(xhpddata)
      
    }
    
    #################################
    # Posterior means and variances #
    #################################
    
    # Means within (half-)chains
    xmn <- sapply(postDataHalves, 
                  function(chamat) apply(chamat, 2, function(x) mean(x, na.rm = TRUE)))
    
    # Protection against drop in dimensionality
    if (is.vector(xmn)) {
      xmn <- matrix(xmn, nrow = 1, ncol = 2*nchains, 
                    dimnames = list(colNames, coda::chanames(postDataHalves)))
    }
  
    # Variance within (half-)chains
    xvr <- sapply(postDataHalves, 
                  function(chamat) apply(chamat, 2, function(x) var(x, na.rm = TRUE)))
    
    # Protection against drop in dimensionality
    if (is.vector(xvr)) {
      xvr <- matrix(xvr, nrow = 1, ncol = 2*nchains, 
                    dimnames = list(colNames, coda::chanames(postDataHalves)))
    }
    
    # Average within-chain variances
    xwv <- t(t(apply(xvr, 1, function(x) mean(x, na.rm = TRUE))))
    
    # Between-chain variances
    xbv <- t(t(apply(xmn, 1, function(x) var(x, na.rm = TRUE))))
    
    # Means across chains
    xmn <- t(t(apply(xmn, 1, function(x) mean(x, na.rm = TRUE)))) 
    
    # Pooled posterior variances
    if ((floor(nmcmc/nthin) %% 2) != 0) {
      xpv <- xbv + (1-1/((floor(nmcmc/nthin)-1)/2))*xwv
    } else {
      xpv <- xbv + (1-1/(floor(nmcmc/nthin)/2))*xwv
    }
    
    # Finalize and clean up
    pv <- cbind(xmn, xbv, xwv, xpv)
    dimnames(pv)[[2]] <- c("pmn", "bv", "wv", "pvr")
    rm(xmn, xbv, xwv, xpv)
    
    if (compute.splitRhats) {
      
      ########################################
      # Repeat for the rank-normalized draws #
      ########################################
      
      # Calculate Blom-transformed ranks
      postDataRanks <- lapply(postDataHalves, 
                              function(chamat) apply(chamat, 2, function(x) rankNorm(x)))
      
      # Means within (half-)chains
      xmn <- sapply(postDataRanks, 
                    function(chamat) apply(chamat, 2, function(x) mean(x, na.rm = TRUE)))

      # Protection against drop in dimensionality
      if (is.vector(xmn)) {
        xmn <- matrix(xmn, nrow = 1, ncol = 2*nchains, 
                      dimnames = list(colNames, names(postDataRanks)))
      }
      
      # Variance within (half-)chains
      xvr <- sapply(postDataRanks, 
                    function(chamat) apply(chamat, 2, function(x) var(x, na.rm = TRUE)))

      # Protection against drop in dimensionality
      if (is.vector(xvr)) {
        xvr <- matrix(xvr, nrow = 1, ncol = 2*nchains, 
                      dimnames = list(colNames, names(postDataRanks)))
      }
  
      # Average within-chain variances
      xwv <- t(t(apply(xvr, 1, function(x) mean(x, na.rm = TRUE))))
      
      # Between-chain variances
      xbv <- t(t(apply(xmn, 1, function(x) var(x, na.rm = TRUE))))
      
      # Means across chains
      xmn <- t(t(apply(xmn, 1, function(x) mean(x, na.rm = TRUE)))) 
      
      # Pooled posterior variances
      if ((floor(nmcmc/nthin) %% 2) != 0) {
        xpv <- xbv + (1-1/((floor(nmcmc/nthin)-1)/2))*xwv
      } else {
        xpv <- xbv + (1-1/(floor(nmcmc/nthin)/2))*xwv
      }
  
      # Finalize and clean up
      rkpv <- cbind(xmn, xbv, xwv, xpv)
      dimnames(rkpv)[[2]] <- c("rkpmn", "rkbv", "rkwv", "rkpvr")
      rm(xmn, xbv, xwv, xpv, postDataRanks)
      
      ###############################################
      # Repeat for the rank-normalized folded draws #
      ###############################################
      
      # Center relative to medians and fold using absolute value
      postDataFolds <- lapply(postDataHalves, 
                              function(chamat) apply(chamat, 2, function(x) {
                                  xmed <- median(x, na.rm=TRUE)
                                  abs(x - xmed)
                                  }))
      
      # Calculate Blom-transformed ranks on folded draws and clean up
      postDataFoldedRanks <- lapply(postDataFolds, 
                                    function(chamat) apply(chamat, 2, function(x) rankNorm(x)))
      rm(postDataFolds)
      
      # Means within (half-)chains
      xmn <- sapply(postDataFoldedRanks, 
                    function(chamat) apply(chamat, 2, function(x) mean(x, na.rm = TRUE)))

      # Protection against drop in dimensionality
      if (is.vector(xmn)) {
        xmn <- matrix(xmn, nrow = 1, ncol = 2*nchains, 
                      dimnames = list(colNames, names(postDataFoldedRanks)))
      }
      
      # Variance within (half-)chains
      xvr <- sapply(postDataFoldedRanks, 
                    function(chamat) apply(chamat, 2, function(x) var(x, na.rm = TRUE)))
      
      # Protection against drop in dimensionality
      if (is.vector(xvr)) {
        xvr <- matrix(xvr, nrow = 1, ncol = 2*nchains, 
                      dimnames = list(colNames, names(postDataFoldedRanks)))
      }
      
      # Average within-chain variances
      xwv <- t(t(apply(xvr, 1, function(x) mean(x, na.rm = TRUE))))
      
      # Between-chain variances
      xbv <- t(t(apply(xmn, 1, function(x) var(x, na.rm = TRUE))))
      
      # Means across chains
      xmn <- t(t(apply(xmn, 1, function(x) mean(x, na.rm = TRUE)))) 
      
      # Pooled posterior variances
      if ((floor(nmcmc/nthin) %% 2) != 0) {
        xpv <- xbv + (1-1/((floor(nmcmc/nthin)-1)/2))*xwv
      } else {
        xpv <- xbv + (1-1/(floor(nmcmc/nthin)/2))*xwv
      }
      
      # Finalize and clean up
      flrkpv <- cbind(xmn, xbv, xwv, xpv)
      dimnames(flrkpv)[[2]] <- c("flrkpmn", "flrkbv", "flrkwv", "flrkpvr")
      rm(xmn, xbv, xwv, xpv, postDataFoldedRanks)
      rm(postDataHalves)
      
      ########################################################################################
      # Combine different split-Rhats, calculate diagnostic, and create flag for poor mixing #
      ########################################################################################
      
      splitRhat <- ifelse(pv[,"wv"] > 0, sqrt(pv[,"pvr"]/pv[,"wv"]), NA)
      rksplitRhat <- ifelse(rkpv[,"rkwv"] > 0, sqrt(rkpv[,"rkpvr"]/rkpv[,"rkwv"]), NA)
      flrksplitRhat <- ifelse(flrkpv[,"flrkwv"] > 0, sqrt(flrkpv[,"flrkpvr"]/flrkpv[,"flrkwv"]), NA)
      mixed <- ifelse(is.na(rksplitRhat) | is.na(flrksplitRhat), NA, 
                      ifelse(pmax(rksplitRhat, flrksplitRhat) >= threshold, FALSE, TRUE))
      
      # Issue warning message for poor mixing
      if (any(!mixed, na.rm = TRUE)) {
        cat("\n")
        if (nchains > 1) { 
          nchainsLab <- "chains."
        } else {
          nchainsLab <- "chain."
        }
        message(paste(strwrap(paste("Warning: Gelman-Rubin diagnostics at the threshold", threshold, "suggest poor mixing with", nchains, nchainsLab,
                                    "Model predictions may be unreliable based on this threshold value;",
                                    "see dataset", paste("'", paste(outName, "postSample_GR", sep="_"), "'", sep=""), "for details.",
                                    "Consider investigating chain-specific diagnostics, modifying MCMC options, or using a simpler model.")), 
                      collapse = "\n"))
        cat("\n")
      }
      
      # Assign results to dataset in the specified environment 
      assign(paste(dataName, "GR", sep="_"), 
             cbind(pv[, c("wv", "pvr")], rkpv[, c("rkwv", "rkpvr")], flrkpv[, c("flrkwv", "flrkpvr")], 
                   splitRhat, rksplitRhat, flrksplitRhat, mixed), 
             envir = envir)
      
      rm(rkpv, flrkpv, splitRhat, rksplitRhat, flrksplitRhat, mixed)
      
    }
    
    # Return the final MCMC estimates for the posterior means, corrected posterior variances, and HPD intervals
    if (compute.HPDintervals) {
      pv <- cbind(pv[, "pmn"], sqrt(pv[, "pvr"]), xhpd)
      rm(xhpd)
      dimnames(pv)[[2]] <- c("MKF estimate", "MKF SE", "MKF HPD 95% LL", "MKF HPD 95% UL")
    } else {
      pv <- cbind(pv[, "pmn"], sqrt(pv[, "pvr"]))
      dimnames(pv)[[2]] <- c("MKF estimate", "MKF SE")
    }
    
    return(pv)
  }
  
} # eMKFpostProcessing

######################################################################################################
# R-side function to combine multiple objects into a list and remove them from specified environment #
######################################################################################################

objsToList <- function(obj_names, list_name, short_names, envir = .GlobalEnv) {
  
  # Initialize named list in global environment
  initList <- vector("list", length(obj_names))
  names(initList) <- short_names
  assign(list_name, initList, envir = envir)
  
  # Assign objects sequentially, cleaning up as you go
  for (i in seq_along(obj_names)) {
    obj <- obj_names[i]
    assign(list_name, `[[<-`(get(list_name, envir = envir), i, 
                             get(obj, envir = envir)), envir = envir)
    rm(list = obj, envir = envir)
  }
  
  return(NULL)
} # objsToList

#####################################################################
##              R prototype of the SAS eMKF macro                  ##
#####################################################################

Rmkf <- function(data = "",                                         # (character) Input dataset name should be in quotes as it is passed to Rmkf() by reference, as in SAS, instead of by value
                 outcome = "",                                      # (character) Column name for outcome variable
                 se = "",                                           # (character) Column name for SE of outcome variable
                 neff = "",                                         # (character) Column name for (effective) sample size, to use when randomVars = TRUE -- ignored when randomVars = FALSE
                 by = "",                                           # (character) Column name for stratification variable
                 group = "",                                        # (character) Column name for grouping variable
                 time = "",                                         # (character) Column name for timepoint variable
                 breakPoint = as.numeric(NA),                       # (numeric) Timepoint at which segment 2 starts -- defaults to NA when unspecified
                 breakType = "",                                    # (character) Unless specifically set to 'full_break', it will default to 'level_break' when breakPoint is specified
                 bayesModel = "",                                   # (character) Defaults to 'bma_cubic' if breakPoint is unspecified and 'bma_linear' is specified -- if not "", bayesModel must be one of 'bma_cubic', 'indep_cubic', 'common_cubic', 'bma_quad', 'indep_quad', 'common_quad', 'bma_linear', 'indep_linear', 'common_linear', or 'dropped' 
                 randomVars = TRUE,                                 # (logical) Both fixed (FALSE) and random (TRUE; default) sampling variances are accommodated in the eMKF model
                 ARmodel = "",                                      # (character) Defaults to 'common_ar' -- if not "", ARmodel must be one of 'common_ar', 'common_arh', or 'indep_ar'
                 out = "param",                                     # (character) Prefix to use for naming output datasets
                 comparedTo = "",                                   # (character) Default value is "" -- when not "" (more resource-intensive), disparities are calculated, and comparedTo must be one of 'ALL', 'MIN', 'MAX', or a valid group name from the 'group' column
                 compareData = "",                                  # (character) Name of dataset to use for storing results of disparities calculations -- if left "", Rmkf will create a name
                 modelPrint = FALSE,                                # (logical) Defaults to FALSE. Set to TRUE to print compilation progress and basic information regarding the MCMC
                 finalPrint = TRUE,                                 # (logical) Set to TRUE (default) to generate the output table (text and html) with model-based vs sample predictions at the last timepoint
                 #####################################################
                 pdigit = 4L,                                       # (integer) Number of significant digits to use in output table
                 chains = 4L,                                       # (integer) Number of chains for MCMC
                 GRthreshold = 1.01,                                # (numeric) Threshold to use for the Gelman-Rubin split R-hat diagnostic
                 seed = 1234L,                                      # (integer) Random seed to replicate MCMC run
                 nbi = 10000L,                                      # (integer) Number of burn-in steps
                 nmc = 50000L,                                      # (integer) Number of MCMC samples to retain after burn-in
                 thin = 1L,                                         # (integer) Thinning rate (steps skipped between samples)
                 checkSampleSize = TRUE,                            # (logical) Set to TRUE (default) to check whether number of timepoints per group and segment meets the recommended criteria to avoid a (near-)singular X'X matrix
                 orpoly = TRUE,                                     # (logical) Set to TRUE (default) to use orthogonal polynomial trends
                 #####################################################
                 malpha = as.numeric(NA), palpha = as.numeric(NA),  # (numeric) Prior mean and precision for the intercepts in segment 1 -- determined empirically if left unspecified
                 s2malpha=as.numeric(NA), s2palpha=as.numeric(NA),  # (numeric) Prior mean and precision for the intercepts in segment 2 -- determined empirically if left unspecified
                 mbeta1 = 0, pbeta1 = as.numeric(NA),               # (numeric) Prior mean and precision for the linear regression coefficients in segment 1 -- determined empirically if left unspecified
                 s2mbeta1 = 0, s2pbeta1 = as.numeric(NA),           # (numeric) Prior mean and precision for the linear regression coefficients in segment 2 -- determined empirically if left unspecified
                 mbeta2 = 0, pbeta2 = as.numeric(NA),               # (numeric) Prior mean and precision for the quadratic regression coefficients in segment 1 -- determined empirically if left unspecified
                 s2mbeta2 = 0, s2pbeta2 = as.numeric(NA),           # (numeric) Prior mean and precision for the quadratic regression coefficients in segment 2 -- determined empirically if left unspecified
                 mbeta3 = 0, pbeta3 = as.numeric(NA),               # (numeric) Prior mean and precision for the cubic regression coefficients in segment 1 -- determined empirically if left unspecified
                 s2mbeta3 = 0, s2pbeta3 = as.numeric(NA),           # (numeric) Prior mean and precision for the cubic regression coefficients in segment 2 -- determined empirically if left unspecified
                 mrho = 0, prho = 1,                                # (numeric) Prior mean and precision for the random effects AR(1) autocorrelation coefficient -- defaults to 0 and 1, respectively
                 taul = 0.0001, tauu = as.numeric(NA),              # (numeric) Prior bounds for the random effects AR(1) innovation standard deviation -- determined empirically if left unspecified
                 vshape=as.numeric(NA), vscale=as.numeric(NA),      # (numeric) Prior shape and scale parameters for the prior distribution of the sampling variances -- determined empirically if left unspecified
                 #####################################################
                 wdirichlet = FALSE,                                # (logical) [NOT YET IMPLEMENTED] Set to TRUE for a Dirichlet prior on the model weights in the BMA cases
                 wshape = as.numeric(NA),                           # (numeric) [NOT YET IMPLEMENTED] Shape parameter to use for the Dirichlet prior -- value is set internally to 2 for the uniform prior on the models
                 #####################################################
                 mcmclog = FALSE,                                   # (logical) Defaults to FALSE. Set to TRUE (more resource-intensive) to retain/examine the full posterior samples in addition to the summaries produced by Rmkf
                 mcmcHPDintervals = FALSE,                          # (logical) Defaults to FALSE. Set to TRUE to calculate highest posterior density (HPD) 95% credible intervals for scalar model parameters -- posterior means and SDs will always be calculated
                 eMKFcustomSamplers = TRUE,                         # (logical) Set to TRUE (default) to use the custom eMKF Gibbs samplers (which are required for all BMA cases) instead of NIMBLE's default samplers
                 outloc = "",                                       # (character) Working R directory with read/write access to store output HTML tables. Defaults to getwd() if unspecified.
                 dllloc = "",                                       # (character) Temporary R directory with read/write access to store DLLs and C++ files. Defaults to tempdir() if unspecified.
                 clearDLLs = TRUE                                   # (character) Set to TRUE (default) to unload DLLs and clear C++ files from the specified temporary R directory *after* the user exits the R session
) {
  
  ##########################
  # Check input parameters #
  ##########################
  
  # Error check that parameter modes are correctly specified
  stopifnot(is.character(data), is.character(outcome), is.character(se), is.character(neff), is.character(by), is.character(group), is.character(time), 
            is.character(breakType),
            is.character(bayesModel), is.character(ARmodel), is.character(out), is.character(comparedTo), is.character(compareData), is.character(outloc), is.character(dllloc),
            is.logical(randomVars), is.logical(modelPrint), is.logical(finalPrint), is.logical(checkSampleSize), is.logical(orpoly), is.logical(wdirichlet), 
            is.logical(mcmclog), is.logical(mcmcHPDintervals), is.logical(eMKFcustomSamplers), is.logical(clearDLLs), 
            is.numeric(pdigit), is.numeric(chains), is.numeric(GRthreshold), is.numeric(seed), is.numeric(nbi), is.numeric(nmc), is.numeric(thin),
            is.numeric(breakPoint), 
            is.numeric(malpha), is.numeric(palpha), is.numeric(mbeta1), is.numeric(pbeta1), 
            is.numeric(mbeta2), is.numeric(pbeta2), is.numeric(mbeta3), is.numeric(pbeta3), 
            is.numeric(s2malpha), is.numeric(s2palpha), is.numeric(s2mbeta1), is.numeric(s2pbeta1), 
            is.numeric(s2mbeta2), is.numeric(s2pbeta2), is.numeric(s2mbeta3), is.numeric(s2pbeta3), 
            is.numeric(mrho), is.numeric(prho), is.numeric(taul), is.numeric(tauu), 
            is.numeric(vshape), is.numeric(vscale), is.numeric(wshape)
            )
  
  # Error check that parameters are all of length one
  stopifnot(length(data) == 1, length(outcome) == 1, length(se) == 1, length(neff) == 1, length(by) == 1, length(group) == 1, length(time) == 1, 
            length(breakPoint) == 1, length(breakType) == 1,
            length(bayesModel) == 1, length(ARmodel) == 1, length(out) == 1, length(comparedTo) == 1, length(compareData) == 1, length(outloc) == 1, length(dllloc) == 1,
            length(randomVars) == 1, length(modelPrint) == 1, length(finalPrint) == 1, length(checkSampleSize) == 1, length(orpoly) == 1, length(wdirichlet) == 1, 
            length(mcmclog) == 1, length(mcmcHPDintervals) == 1, length(eMKFcustomSamplers) == 1, length(clearDLLs) == 1, 
            length(pdigit) == 1, length(chains) == 1, length(GRthreshold) == 1, length(seed) == 1, length(nbi) == 1, length(nmc) == 1, length(thin) == 1,
            length(malpha) == 1, length(palpha) == 1, length(mbeta1) == 1, length(pbeta1) == 1, 
            length(mbeta2) == 1, length(pbeta2) == 1, length(mbeta3) == 1, length(pbeta3) == 1, 
            length(s2malpha) == 1, length(s2palpha) == 1, length(s2mbeta1) == 1, length(s2pbeta1) == 1, 
            length(s2mbeta2) == 1, length(s2pbeta2) == 1, length(s2mbeta3) == 1, length(s2pbeta3) == 1, 
            length(mrho) == 1, length(prho) == 1, length(taul) == 1, length(tauu) == 1, 
            length(vshape) == 1, length(vscale) == 1, length(wshape) == 1
  )
  
  # Error check that the 'outloc' directory is valid and has been created
  if (outloc != "") {
    if (!dir.exists(outloc)) {
      stopifnot(dir.create(outloc))
    }
  } else {
    outloc <- getwd()
  }
  
  # Error check that the 'dllloc' directory is valid and has been created
  if (dllloc != "") {
    if (!dir.exists(dllloc)) {
      stopifnot(dir.create(dllloc))
    }
  } else {
    dlloc <- tempdir()
  }
  
  # Error check for the 'out' prefix
  if (out == "") {
    stop("Please specify a non-blank character prefix 'out' to use for naming output datasets.")
  }
  
  # Error check for data specs
  if (group == "" || time == "" || outcome == "" || se == ""  || (randomVars && neff == "")) {
    stop("Data are expected to be in long (stacked) format with all variables correctly named.")
  }
  
  # Local copy of dataset (will return error if input data was not found in the global environment)
  localdata <- as.data.frame(get(data, envir = .GlobalEnv))
  
  # Check specified variable names are in the data
  bayesvars <- names(localdata)
  localvars <- c(group, time, outcome, se)
  stopifnot(is.element(outcome, bayesvars), is.element(se, bayesvars), 
            is.element(group, bayesvars), is.element(time, bayesvars))
  if (randomVars) { 
    localvars <- c(localvars, neff)
    stopifnot(is.element(neff, bayesvars)) 
  }
  if (by != "") { 
    localvars <- c(by, localvars)
    stopifnot(is.element(by, bayesvars)) 
  }

  # Reduce localdata set to just the outcome of interest
  localdata <- localdata[, localvars]
  
  # Check and default to level_break if breakType is unspecified
  if (!is.na(breakPoint)) {
    if (breakType == "") { breakType <- "level_break" }
    breakType <- tolower(breakType)
    if (!is.element(breakType, c("level_break", "full_break"))) {
      stop("The 'breakType' parameter must be either 'level_break' for an intercept-only shift or 'full_break' otherwise.")
    }
  }
  
  # Set breakType to "" if no breakpoint is specified 
  if (is.na(breakPoint) && breakType != "") { breakType <- "" }
  
  # Check for any misspelling/misspecification of the Bayesian model
  if (bayesModel == "") {
    message("Because no trend model was specified, Rmkf will use Bayesian model averaging")
    if (breakType == "") {
      bayesModel <- "bma_cubic"
      message(" over a mixture of polynomial trend models up to the unconstrained cubic.")
    } else {
      bayesModel <- "bma_linear"
      message(" over a mixture of polynomial trend models up to the unconstrained linear.")
    }
  } else {
    bayesModel <- tolower(bayesModel)
    if (!is.element(bayesModel, c("bma_cubic", "bma_quad", "bma_linear", "indep_cubic", "indep_quad", "indep_linear", 
                                   "common_cubic", "common_quad", "common_linear", "dropped"))) {
      stop("The 'bayesModel' parameter must be one of 'bma_cubic', 'bma_quad', 'bma_linear', 'indep_cubic', 'indep_quad', 'indep_linear', 'common_cubic', 'common_quad', 'common_linear', or 'dropped'.")
    }
  }
  
  # Check for any misspelling/misspecification of the AR(1) model
  if (ARmodel == "") {
    message("Because no AR(1) covariance structure was specified, Rmkf will use common AR parameters across groups.")
    ARmodel <- "common_ar"
  } else {
    ARmodel <- tolower(ARmodel)
    if (!is.element(ARmodel, c("common_ar", "common_arh", "indep_ar"))) {
      stop("The 'ARmodel' parameter must be one of 'common_ar', 'common_arh', or 'indep_ar'.")
    }
  }
  
  # Dataset name for differences/disparities when requested
  if (compareData == "" && comparedTo != "") {
    compareData = paste(out, "eMKFdisparities", sep = "_")
    if (tolower(comparedTo) == "all") {
      comparedTo = "unspecified"
    }
  } else {
    if (compareData != "" && (comparedTo == "" || tolower(comparedTo) == "all")) {
      comparedTo = "unspecified"
    }
  }
  
  #############################################################
  # Check and re-format input dataset (eMKF macro 'reformat') #
  #############################################################
  
  # Strata in input and sort orders (when applicable) + internal integer version
  if (by != "") { 
    repsInInputOrder <- unique(localdata[, by])
    nreps <- as.integer(length(repsInInputOrder))
    names(repsInInputOrder) <- as.character(1:nreps)
    repsInSortOrder <- repsInInputOrder[order(repsInInputOrder)]
    localdata[, "_rep"] <- as.integer(apply(sapply(1:nreps, function(k) {k * (localdata[, by] == repsInSortOrder[k])}), 1, sum))
  } else {
    repsInInputOrder <- NULL
    repsInSortOrder <- NULL
    nreps <- 1L
    localdata[, "_rep"] <- 1L
  }
  
  # Groups in input and sort orders + internal integer version
  groupsInInputOrder <- unique(localdata[, group])
  g <- as.integer(length(groupsInInputOrder))
  names(groupsInInputOrder) <- as.character(1:g)
  groupsInSortOrder <- groupsInInputOrder[order(groupsInInputOrder)]
  localdata[, "_group"] <- as.integer(apply(sapply(1:g, function(i) {i * (localdata[, group] == groupsInSortOrder[i])}), 1, sum))

  # Check that specified break point is a valid timepoint
  xptFound <- FALSE
  if (!is.na(breakPoint)) {
    xptFound <- is.element(breakPoint, localdata[, time])
    if (!xptFound) {
      message(paste("Warning: The specified break point was not found in the", time, "column. Please check!"))
    }
  }

  # Times in input and sort orders + internal integer version  
  timesInInputOrder <- unique(localdata[, time])
  n <- as.integer(length(timesInInputOrder))
  names(timesInInputOrder) <- as.character(1:n)
  timesInSortOrder <- timesInInputOrder[order(timesInInputOrder)]
  localdata[, "_time"] <- as.integer(apply(sapply(1:n, function(j) {j * (localdata[, time] == timesInSortOrder[j])}), 1, sum))
  
  # Track both time segments when applicable
  s1n <- n
  s2n <- 0L
  if (!is.na(breakPoint)) {
    if (xptFound) {
      localdata[, "_s1time"] <- ifelse(localdata[, time] < breakPoint, localdata[, "_time"], 0)
      localdata[, "_s2time"] <- ifelse(localdata[, time] < breakPoint, 0, localdata[, "_time"])
      s1n <- as.integer(length(unique(localdata[localdata[, time] < breakPoint, time])))
      s2n <- n - s1n
    } else {
      localdata[, "_s1time"] <- localdata[, "_time"]
      localdata[, "_s2time"] <- 0
    }
  }
  
  # Real times to use in calculations, accounting for unequal spacing and fractional times
  localdata[, "_rtime"] <- as.numeric(localdata[, time] - min(localdata[, time]) + 1)
  rts <- as.numeric(timesInSortOrder - min(timesInSortOrder) + 1)
  
  # Track both time segments when applicable
  if (!is.na(breakPoint)) {
    if (xptFound) {
      localdata[, "_s1rtime"] <- ifelse(localdata[, time] < breakPoint, localdata[, "_rtime"], 0)
      localdata[, "_s2rtime"] <- ifelse(localdata[, time] < breakPoint, 0, localdata[, "_rtime"])
    } else {
      localdata[, "_s1rtime"] <- localdata[, "_rtime"]
      localdata[, "_s2rtime"] <- 0
    }
  }
  
  # Check for missing values in 'outcome' variable
  if (any_na(localdata[, outcome])) {
    stop(paste("Missing values are not allowed in 'outcome' variable ", outcome, ".", sep = ""))
  }
    
  # Check for unequal number of timepoints per group 
  if (nrow(localdata) != nreps*g*n) {
    stop("Unequal number of timepoints per group (and/or stratum) are not allowed in the input data.")
  }
  
  # Capture initial input order prior to sorting
  localdata[, "_inputorder"] <- 1:nrow(localdata)
  
  # Sort 'localdata' by _rep, _group, and _time
  if (nreps > 1) {
    localdata <- localdata[order(localdata[, "_rep"], localdata[, "_group"], localdata[, "_time"]), ]
  } else {
    localdata <- localdata[order(localdata[, "_group"], localdata[, "_time"]), ]
  }
  
  ###############################################
  # SE imputation (as in eMKF macro 'reformat') #
  ###############################################
  
  # Treat nonpositive SEs as missing data to be imputed
  localdata[, se] <- ifelse(localdata[, se] <= 0, NA, localdata[, se])
  
  # Imputation of SEs using averages across timepoints within strata
  localdata[, "_impute"] <- 0
  if (any_na(localdata[, se])) {
    for (k in 1:nreps) {
      kdat <- localdata[localdata[, "_rep"] == k, ]
      kmns <- sapply(1:g, function(i) { mean(kdat[kdat[, "_group"] == i, se], na.rm = TRUE) })
      for (i in 1:g) {
        localdata[(localdata[, "_rep"] == k) & (localdata[, "_group"] == i), se] <- ifelse(is.na(kdat[kdat[, "_group"] == i, se]), kmns[i], kdat[kdat[, "_group"] == i, se])
        localdata[(localdata[, "_rep"] == k) & (localdata[, "_group"] == i), "_impute"] <- ifelse(is.na(kdat[kdat[, "_group"] == i, se]), 1, 0)
      }
    }
    rm(kdat, kmns)
  }
  
  # If there are still missing SEs, try imputation using averages across strata within group
  localdata[, "_imputeb"] <- 0
  if (nreps > 1 && any_na(localdata[, se])) {
    for (i in 1:g) {
      idat <- localdata[localdata[, "_group"] == i, ]
      imns <- sapply(1:n, function(j) { mean(idat[idat[, "_time"] == j, se], na.rm = TRUE) })
      for (j in 1:n) {
        localdata[(localdata[, "_group"] == i) & (localdata[, "_time"] == j), se] <- ifelse(is.na(idat[idat[, "_time"] == j, se]), imns[j], idat[idat[, "_time"] == j, se])
        localdata[(localdata[, "_group"] == i) & (localdata[, "_time"] == j), "_imputeb"] <- ifelse(is.na(idat[idat[, "_time"] == j, se]), 1, 0)
      }
    }
    rm(idat, imns)
  }
  
  # Return error if still missing SEs
  if (any_na(localdata[, se])) {
    stop("Missing SEs could not be resolved via mean imputation: please check the input data!")
  }
  
  #################################################
  # Neff imputation (as in eMKF macro 'reformat') #
  #################################################
  if (randomVars) {
    
    # Treat nonpositive Neffs as missing data to be imputed
    localdata[, neff] <- ifelse(localdata[, neff] <= 0, NA, localdata[, neff])
    
    # Imputation of Neffs using averages across timepoints within strata
    if (any_na(localdata[, neff])) {
       for (k in 1:nreps) {
        kdat <- localdata[localdata[, "_rep"] == k, ]
        kmns <- sapply(1:g, function(i) { mean(kdat[kdat[, "_group"] == i, neff], na.rm = TRUE) })
        for (i in 1:g) {
          localdata[(localdata[, "_rep"] == k) & (localdata[, "_group"] == i), neff] <- ifelse(is.na(kdat[kdat[, "_group"] == i, neff]), kmns[i], kdat[kdat[, "_group"] == i, neff])
        }
      }
      rm(kdat, kmns)
    }
    
    # If there are still missing Neffs, try imputation using averages across strata within group
    if ((nreps > 1) && any_na(localdata[, neff])) {
      for (i in 1:g) {
        idat <- localdata[localdata[, "_group"] == i, ]
        imns <- sapply(1:n, function(j) { mean(idat[idat[, "_time"] == j, neff], na.rm = TRUE) })
        for (j in 1:n) {
          localdata[(localdata[, "_group"] == i) & (localdata[, "_time"] == j), neff] <- ifelse(is.na(idat[idat[, "_time"] == j, neff]), imns[j], idat[idat[, "_time"] == j, neff])
        }
      }
      rm(idat, imns)
    }
    
    # Return error if still missing Neffs
    if (any_na(localdata[, neff])) {
      stop("Missing Neffs could not be resolved via mean imputation: please check the input data!")
    }
    
  }
  
  ################################################
  # Sample size checks: required and recommended #
  ################################################
  
  # Required sample size thresholds
  tmpvar <- 0 
  if (ARmodel == "indep_ar") { 
    tmpvar <- ceiling((0+4*g)/g) 
  } else { 
    if (ARmodel == "common_arh") { 
      tmpvar <- ceiling((1+3*g)/g) 
    } else { 
      tmpvar <- ceiling((2+2*g)/g) 
    } 
  }
  assign(paste("minpts", ARmodel, "dropped", sep = "_"), tmpvar)
  rm(tmpvar)
  
  # Extra timepoint requirements: if segment 2 has fewer than 3 extra points, change full-break to level break
  if (s2n < 3 && breakType == "full_break") {
    message("Warning: 'full_break' option changed to 'level_break' due to insufficient number of timepoints in segment 2.")
    breakType <- "level_break"
  }
  assign(paste("xtrpts", breakType, sep="_"), 0L)
  if (breakType == "level_break") { assign(paste("xtrpts", breakType, sep="_"), 2L) }
  if (breakType == "full_break") { assign(paste("xtrpts", breakType, sep="_"), 3L) }

  # Required sample sizes for eMKF
  if (is.na(breakPoint) && n < get(paste("minpts", ARmodel, "dropped", sep = "_"))) {
    stop(paste("There are not enough timepoints to use the eMKF macro with", g, group, "groups\n",
               " and the ARmodel =", ARmodel, "specification.\n",
               " At least", get(paste("minpts", ARmodel, "dropped", sep = "_")), "timepoints are required."))
  }
  if (!is.na(breakPoint) && (s1n < get(paste("minpts", ARmodel, "dropped", sep = "_")) ||
                             s2n < get(paste("xtrpts", breakType, sep="_")))) {
    stop(paste("Specified break point either was not found or resulted in too few timepoints. Please review!\n", 
               " Segment 1 needs at least", get(paste("minpts", ARmodel, "dropped", sep = "_")), "timepoints with", g, group, "groups",
               "and the ARmodel =", ARmodel, "specification.\n", 
               " Additionally, segment 2 should have", get(paste("xtrpts", breakType, sep="_")), "or more timepoints with the", breakType, "specification."))
  }
  
  # Recommended sample sizes
  if (checkSampleSize) {
    tmpvar <- 0
    if (is.element(bayesModel, c("common_cubic", "indep_cubic", "bma_cubic"))) {
      if (ARmodel == "indep_ar") { 
        tmpvar <- ceiling((0+7*g)/g) 
      } else { 
        if (ARmodel == "common_arh") { 
          tmpvar <- ceiling((1+6*g)/g) 
        } else { 
          tmpvar <- ceiling((2+5*g)/g) 
        } 
      }
    } else {
      if (is.element(bayesModel, c("common_quad", "indep_quad", "bma_quad"))) {
        if (ARmodel == "indep_ar") { 
          tmpvar <- ceiling((0+6*g)/g) 
        } else { 
          if (ARmodel == "common_arh") { 
            tmpvar <- ceiling((1+5*g)/g) 
          } else { 
            tmpvar <- ceiling((2+4*g)/g) 
          } 
        }
      } else {
        if (is.element(bayesModel, c("common_linear", "indep_linear", "bma_linear"))) {
          if (ARmodel == "indep_ar") { 
            tmpvar <- ceiling((0+5*g)/g) 
          } else { 
            if (ARmodel == "common_arh") { 
              tmpvar <- ceiling((1+4*g)/g) 
            } else { 
              tmpvar <- ceiling((2+3*g)/g) 
            } 
          }
        }  
      }
    }
    assign(paste("minpts", ARmodel, bayesModel, sep = "_"), tmpvar)
    rm(tmpvar)
    if (is.na(breakPoint) && n < get(paste("minpts", ARmodel, bayesModel, sep = "_"))) {
      stop(paste("There are too few timepoints for a", bayesModel, "model with", g, group, "groups\n",
                 " and the ARmodel =", ARmodel, "specification.\n", 
                 " At least", get(paste("minpts", ARmodel, bayesModel, sep = "_")), "timepoints are recommended."))
    }
    if (!is.na(breakPoint) && (s1n < get(paste("minpts", ARmodel, bayesModel, sep = "_")) || 
                               s2n < get(paste("xtrpts", breakType, sep="_")))) {
      stop(paste("There are too few timepoints for a", bayesModel, "model with", g, group, "groups\n",
                 " and the ARmodel =", ARmodel, "and breakType =", breakType, "specifications.\n", 
                 " Segment 1 should have at least", get(paste("minpts", ARmodel, bayesModel, sep = "_")), "timepoints",
                 "and segment 2 at least", get(paste("xtrpts", breakType, sep="_")), "with these model specs."))
    }
  }
  
  ########################################
  # Preparing data for NIMBLE model code #
  ########################################
  
  # Logical control values based on AR model types for use in NIMBLE model code 
  if (ARmodel == "common_ar") {
    common_ar <- TRUE
    common_psi <- TRUE
    common_tau <- TRUE
  } else {
    if (ARmodel == "common_arh") {
      common_ar <- FALSE
      common_psi <- TRUE
      common_tau <- FALSE
    } else { # (ARmodel == "indep_ar")
      common_ar <- FALSE
      common_psi <- FALSE
      common_tau <- FALSE
    }
  }
  
  # Dimension for segment 1 and logical control values based on trend types for use in NIMBLE model code 
  cubic_trend <- FALSE
  quadratic_trend <- FALSE
  linear_trend <- FALSE
  dropped_trend <- FALSE
  if (is.element(bayesModel, c("common_cubic", "indep_cubic", "bma_cubic"))) {
    s1p <- 4L
    cubic_trend <- TRUE
    common_trend <- (bayesModel == "common_cubic")
  } else {
    if (is.element(bayesModel, c("common_quad", "indep_quad", "bma_quad"))) {
      s1p <- 3L
      quadratic_trend <- TRUE
      common_trend <- (bayesModel == "common_quad")
    } else {
      if (is.element(bayesModel, c("common_linear", "indep_linear", "bma_linear"))) {
        s1p <- 2L
        linear_trend <- TRUE
        common_trend <- (bayesModel == "common_linear")
      } else {
        s1p <- 1L
        dropped_trend <- TRUE
        common_trend <- TRUE  # for model init/code purposes, intercepts-only model treated as a common trend
      }
    }
  }
  
  # Overall dimensionality and additional control values for full trend break cases
  cubic_cubic_trend <- FALSE
  cubic_quadratic_trend <- FALSE
  cubic_linear_trend <- FALSE
  quadratic_quadratic_trend <- FALSE
  quadratic_linear_trend <- FALSE
  linear_linear_trend <- FALSE
  dropped_dropped_trend <- FALSE
  s2p <- 0L
  if (breakType == "full_break") {
    s2p <- min(s1p, n - s1n - 1)
    p <- s1p + s2p
    if (s1p == 4) {
      if (s2p == 4) { 
        cubic_cubic_trend <- TRUE 
        bayesModel <- paste(bayesModel, "cubic", sep="_")
      } else {
        if (s2p == 3) { 
          cubic_quadratic_trend <- TRUE 
          bayesModel <- paste(bayesModel, "quad", sep="_")
        } else {
          if (s2p == 2) { 
            cubic_linear_trend <- TRUE 
            bayesModel <- paste(bayesModel, "linear", sep="_")
          }
        }
      }
    } else {
      if (s1p == 3) {
        if (s2p == 3) { 
          quadratic_quadratic_trend <- TRUE 
          bayesModel <- paste(bayesModel, "quad", sep="_")
        } else {
          if (s2p == 2) { 
            quadratic_linear_trend <- TRUE 
            bayesModel <- paste(bayesModel, "linear", sep="_")
          }
        }
      } else {
        if (s1p == 2) {
          if (s2p == 2) { 
            linear_linear_trend <- TRUE 
            bayesModel <- paste(bayesModel, "linear", sep="_")
          }
        } else {
          dropped_dropped_trend <- TRUE
          bayesModel <- paste(bayesModel, "dropped", sep="_")
        }
      }
    }
  } else {
    if (breakType == "level_break") {
      p <- s1p + 1
    } else {
      p <- s1p   
    }
  }
  
  # Predictor matrix X is n x p (including intercept(s))
  if (breakType == "full_break") {
    X <- polyDesignMatrix(rts, xpos = s1n+1L, xmod = 2L, degree = s1p-1L, orpol = as.integer(orpoly))
  } else {
    if (breakType == "level_break") {
      X <- polyDesignMatrix(rts, xpos = s1n+1L, xmod = 1L, degree = p-2L, orpol = as.integer(orpoly))
    } else {
      X <- polyDesignMatrix(rts, degree = p-1L, orpol = as.integer(orpoly))
    }
  }
  
  # Number of mixture components in BMA to pass to NIMBLE model
  fp <- 1L
  if (bayesModel == "bma_cubic") { fp <- 7L }
  if (bayesModel == "bma_cubic_cubic") { fp <- 13L }
  if (bayesModel == "bma_cubic_quad") { fp <- 11L }
  if (bayesModel == "bma_cubic_linear") { fp <- 7L }
  if (bayesModel == "bma_quad") { fp <- 5L }
  if (bayesModel == "bma_quad_quad") { fp <- 7L }
  if (bayesModel == "bma_quad_linear") { fp <- 5L }
  if (bayesModel == "bma_linear") { fp <- 3L }
  if (bayesModel == "bma_linear_linear") { fp <- 3L }
  
  # Constants list to pass to NIMBLE
  cnst_list <- list(g = g, p = p, n = n, rts = rts, X = X)
  if (s2p > 0) { cnst_list <- c(cnst_list, list(s1n = s1n, s1p = s1p)) } # s2n = n - s1n; s2p = p - s1p
  if (fp > 1) { cnst_list <- c(cnst_list, list(fp = fp)) } 
  
  ####################################################
  # Define model and MCMC using first "_rep" stratum #
  ####################################################
  
  # Force use of custom NIMBLE samplers in the BMA scenarios
  eMKFcustomSamplersReset <- FALSE
  if ((fp > 1) && !eMKFcustomSamplers) {
    eMKFcustomSamplers <- TRUE
    eMKFcustomSamplersReset <- TRUE
  }
  
  # Register user-supplied distributions for use in NIMBLE BUGS models
  if (fp > 1) {
    registerDistributions(list(
      dmixbeta3 = list(BUGSdist = "dmixbeta3(lg, pmn, psd, ind, bma)",
                       types = c('value = double(1)', 'lg = integer(0)', 'ind = integer(0)', 'bma = integer(0)')),
      dmixbeta2 = list(BUGSdist = "dmixbeta2(lg, pmn, psd, ind, bma)",
                       types = c('value = double(1)', 'lg = integer(0)', 'ind = integer(0)', 'bma = integer(0)')),
      dmixbeta1 = list(BUGSdist = "dmixbeta1(lg, pmn, psd, ind, bma)",
                       types = c('value = double(1)', 'lg = integer(0)', 'ind = integer(0)', 'bma = integer(0)'))
    ), userEnv = .GlobalEnv)
  }
  
  # Assign 'localdata' to the dataset 'eMKFbayesdata' in the global environment and remove 'localdata'
  assign(paste(out, "eMKFbayesdata", sep="_"), localdata, envir = .GlobalEnv)
  rm(localdata)
  
  # Data list to pass to NIMBLE
  data_list <- eMKFdataList(dataName = paste(out, "eMKFbayesdata", sep="_"),
                            subset = ( get(paste(out, "eMKFbayesdata", sep="_"), envir = .GlobalEnv)[, "_rep"] == 1 ),
                            p, fp, g, n, s1p, s1n, outcome, se, neff,
                            malpha, palpha, mbeta1, pbeta1, mbeta2, pbeta2, mbeta3, pbeta3,
                            s2malpha, s2palpha, s2mbeta1, s2pbeta1, s2mbeta2, s2pbeta2, s2mbeta3, s2pbeta3,
                            mrho, prho, taul, tauu, randomVars, vshape, vscale, wshape)
  
  # Set random seed for initialization purposes (uses reverse of input seed here)
  init_seed <- as.integer(paste(rev(strsplit(as.character(seed), split="")[[1]]), collapse=""))
  
  # List of initial values for stochastic nodes to pass to NIMBLE
  init_list <- vector("list", chains)
  if (fp > 1) {
    if (breakType == "full_break") {
      for (chain in 1:chains) {
        init_list[[chain]] <- bayesBMAxptf_init(cubic_cubic_trend, cubic_quadratic_trend, cubic_linear_trend, 
                                                quadratic_quadratic_trend, quadratic_linear_trend, linear_linear_trend, 
                                                common_ar, common_psi, common_tau,
                                                randomVars, orpoly, fp, g, p, n, s1p, s1n, rts,
                                                data_list$mbetag, data_list$Dbetag, data_list$mrho, data_list$srho,
                                                data_list$taul, data_list$tauu, data_list$vshape, data_list$vscale, data_list$wshape,
                                                init_seed = init_seed + chain - 1)
      }
    } else {
      if (breakType == "level_break") {
        for (chain in 1:chains) {
          init_list[[chain]] <- bayesBMAxptl_init(cubic_trend, quadratic_trend, linear_trend, 
                                                  common_ar, common_psi, common_tau,
                                                  randomVars, orpoly, fp, g, p, n, s1n, rts,
                                                  data_list$mbetag, data_list$Dbetag, data_list$mrho, data_list$srho,
                                                  data_list$taul, data_list$tauu, data_list$vshape, data_list$vscale, data_list$wshape,
                                                  init_seed = init_seed + chain - 1)
        }
      } else {
        for (chain in 1:chains) {
          init_list[[chain]] <- bayesBMA_init(cubic_trend, quadratic_trend, linear_trend, 
                                              common_ar, common_psi, common_tau,
                                              randomVars, orpoly, fp, g, p, n, rts,
                                              data_list$mbetag, data_list$Dbetag, data_list$mrho, data_list$srho,
                                              data_list$taul, data_list$tauu, data_list$vshape, data_list$vscale, data_list$wshape,
                                              init_seed = init_seed + chain - 1)
        }
      }
    }
  } else {
    if (breakType == "full_break") {
      for (chain in 1:chains) {
        init_list[[chain]] <- bayesfitxptf_init(cubic_cubic_trend, cubic_quadratic_trend, cubic_linear_trend, quadratic_quadratic_trend, 
                                                quadratic_linear_trend, linear_linear_trend, dropped_dropped_trend, common_trend, 
                                                common_ar, common_psi, common_tau, 
                                                randomVars, orpoly, g, p, n, s1p, s1n, rts,
                                                data_list$mbetag, data_list$Dbetag, data_list$mrho, data_list$srho,
                                                data_list$taul, data_list$tauu, data_list$vshape, data_list$vscale,
                                                init_seed = init_seed + chain - 1)
      }
    } else {
      if (breakType == "level_break") {
        for (chain in 1:chains) {
          init_list[[chain]] <- bayesfitxptl_init(cubic_trend, quadratic_trend, linear_trend, dropped_trend, common_trend, 
                                                  common_ar, common_psi, common_tau, 
                                                  randomVars, orpoly, g, p, n, s1n, rts,
                                                  data_list$mbetag, data_list$Dbetag, data_list$mrho, data_list$srho,
                                                  data_list$taul, data_list$tauu, data_list$vshape, data_list$vscale,
                                                  init_seed = init_seed + chain - 1)
        }
      } else {
        for (chain in 1:chains) {
          init_list[[chain]] <- bayesfit_init(cubic_trend, quadratic_trend, linear_trend, dropped_trend, common_trend, 
                                              common_ar, common_psi, common_tau, 
                                              randomVars, orpoly, g, p, n, rts,
                                              data_list$mbetag, data_list$Dbetag, data_list$mrho, data_list$srho,
                                              data_list$taul, data_list$tauu, data_list$vshape, data_list$vscale,
                                              init_seed = init_seed + chain - 1)
        }
      }
    }
  }
  
  # Invoke applicable NIMBLE model code
  if (fp > 1) {
    if (breakType == "full_break") { 
      Rnmbl_code <- bayesBMAxptf_modelCode 
    } else {
      if (breakType == "level_break") { 
        Rnmbl_code <- bayesBMAxptl_modelCode 
      } else {
        Rnmbl_code <- bayesBMA_modelCode 
      }
    }
  } else {
    if (breakType == "full_break") { 
      Rnmbl_code <- bayesfitxptf_modelCode 
    } else {
      if (breakType == "level_break") { 
        Rnmbl_code <- bayesfitxptl_modelCode 
      } else {
        Rnmbl_code <- bayesfit_modelCode 
      }
    }
  }
  
  # Variables to monitor depend on bayesModel, breakType, ARmodel, and randomVars
  if (breakType == "full_break") { 
    Rnmbl_mntr <-  switch(bayesModel,
                          bma_cubic_cubic = c("s1flg", "s2flg", "s1ag", "s1b1g", "s1b2g", "s1b3g", "s2ag", "s2b1g", "s2b2g", "s2b3g", "etaarr"),
                          bma_cubic_quad = c("s1flg", "s2flg", "s1ag", "s1b1g", "s1b2g", "s1b3g", "s2ag", "s2b1g", "s2b2g", "etaarr"),
                          bma_cubic_linear = c("s1flg", "s2flg", "s1ag", "s1b1g", "s1b2g", "s1b3g", "s2ag", "s2b1g", "etaarr"),
                          indep_cubic_cubic = c("s1ag", "s1b1g", "s1b2g", "s1b3g", "s2ag", "s2b1g", "s2b2g", "s2b3g", "etaarr"),
                          indep_cubic_quad = c("s1ag", "s1b1g", "s1b2g", "s1b3g", "s2ag", "s2b1g", "s2b2g", "etaarr"),
                          indep_cubic_linear = c("s1ag", "s1b1g", "s1b2g", "s1b3g", "s2ag", "s2b1g", "etaarr"),
                          common_cubic_cubic = c("s1ag", "s1b1", "s1b2", "s1b3", "s2ag", "s2b1", "s2b2", "s2b3", "etaarr"),
                          common_cubic_quad = c("s1ag", "s1b1", "s1b2", "s1b3", "s2ag", "s2b1", "s2b2", "etaarr"),
                          common_cubic_linear = c("s1ag", "s1b1", "s1b2", "s1b3", "s2ag", "s2b1", "etaarr"),
                          bma_quad_quad = c("s1flg", "s2flg", "s1ag", "s1b1g", "s1b2g", "s2ag", "s2b1g", "s2b2g", "etaarr"),
                          bma_quad_linear = c("s1flg", "s2flg", "s1ag", "s1b1g", "s1b2g", "s2ag", "s2b1g", "etaarr"),
                          indep_quad_quad = c("s1ag", "s1b1g", "s1b2g", "s2ag", "s2b1g", "s2b2g", "etaarr"),
                          indep_quad_linear = c("s1ag", "s1b1g", "s1b2g", "s2ag", "s2b1g", "etaarr"),
                          common_quad_quad = c("s1ag", "s1b1", "s1b2", "s2ag", "s2b1", "s2b2", "etaarr"),
                          common_quad_linear = c("s1ag", "s1b1", "s1b2", "s2ag", "s2b1", "etaarr"),
                          bma_linear_linear = c("s1flg", "s2flg", "s1ag", "s1b1g", "s2ag", "s2b1g", "etaarr"),
                          indep_linear_linear = c("s1ag", "s1b1g", "s2ag", "s2b1g", "etaarr"),
                          common_linear_linear = c("s1ag", "s1b1", "s2ag", "s2b1", "etaarr"),
                          dropped_dropped = c("s1ag", "s2ag", "etaarr"))    
    
  } else {
    if (breakType == "level_break") {
      Rnmbl_mntr <-  switch(bayesModel,
                            bma_cubic = c("flg", "s1ag", "s2ag", "b1g", "b2g", "b3g", "etaarr"),
                            indep_cubic = c("s1ag", "s2ag", "b1g", "b2g", "b3g", "etaarr"),
                            common_cubic = c("s1ag", "s2ag", "b1", "b2", "b3", "etaarr"),
                            bma_quad = c("flg", "s1ag", "s2ag", "b1g", "b2g", "etaarr"),
                            indep_quad = c("s1ag", "s2ag", "b1g", "b2g", "etaarr"),
                            common_quad = c("s1ag", "s2ag", "b1", "b2", "etaarr"),
                            bma_linear = c("flg", "s1ag", "s2ag", "b1g", "etaarr"),
                            indep_linear = c("s1ag", "s2ag", "b1g", "etaarr"),
                            common_linear = c("s1ag", "s2ag", "b1", "etaarr"),
                            dropped = c("s1ag", "s2ag", "etaarr"))    
      
    } else {
      Rnmbl_mntr <-  switch(bayesModel,
                            bma_cubic = c("flg", "ag", "b1g", "b2g", "b3g", "etaarr"),
                            indep_cubic = c("ag", "b1g", "b2g", "b3g", "etaarr"),
                            common_cubic = c("ag", "b1", "b2", "b3", "etaarr"),
                            bma_quad = c("flg", "ag", "b1g", "b2g", "etaarr"),
                            indep_quad = c("ag", "b1g", "b2g", "etaarr"),
                            common_quad = c("ag", "b1", "b2", "etaarr"),
                            bma_linear = c("flg", "ag", "b1g", "etaarr"),
                            indep_linear = c("ag", "b1g", "etaarr"),
                            common_linear = c("ag", "b1", "etaarr"),
                            dropped = c("ag", "etaarr"))    
    }
  }
  Rnmbl_mntr <- switch(ARmodel,
                       common_ar = c("rho", "tausq", Rnmbl_mntr),
                       common_arh = c("rho", "tausqarr", Rnmbl_mntr),
                       indep_ar = c("spsi", "mpsi", "rhoarr", "tausqarr", Rnmbl_mntr))
  if (randomVars) { 
    Rnmbl_mntr <- c(Rnmbl_mntr, "varr") 
  }
  
  # Set some global NIMBLE options
  vprint <- nimbleOptions("verbose")
  nimbleOptions(verbose = modelPrint)
  
  cat("\n")
  message("Setting up and compiling applicable MKF model and MCMC using the 'nimble' package")
  message("  [Note] This process usually takes a few minutes.")
  if (!modelPrint) { message("  [Note] To monitor compilation progress or view the MCMC's configuration, set modelPrint = TRUE.")}
  if (eMKFcustomSamplersReset) { message("  [Note] Although eMKFcustomSamplers = FALSE, custom samplers are required and will be used.") }
  message("")
  
  # Create NIMBLE model (initializing using 1st chain for model checking only)
  Rnmbl_model <- nimbleModel(Rnmbl_code, constants = cnst_list, data = data_list, inits = init_list[[1]], check = TRUE)
  
  # Configure NIMBLE model (turn off conjugacy for eMKF custom samplers)
  Rnmbl_conf <- configureMCMC(Rnmbl_model, monitors = Rnmbl_mntr, thin = thin, useConjugacy = !eMKFcustomSamplers, print = FALSE)
  
  # Use reflective random walk samplers for AR(1) parameters so proposals remain within specified bounds
  assign('sampler_reflective_RW', sampler_RW, envir = .GlobalEnv)
  if (common_ar) {
    Rnmbl_conf$removeSamplers(c('psi'))
    Rnmbl_conf$addSampler(target = c('psi'), type = 'sampler_reflective_RW', silent=TRUE, control = list(reflective=TRUE))
    Rnmbl_conf$removeSamplers(c('tau'))
    Rnmbl_conf$addSampler(target = c('tau'), type = 'sampler_reflective_RW', silent=TRUE, control = list(reflective=TRUE))
  } else {
    if (!common_psi) {
      Rnmbl_conf$removeSamplers(c('mpsi'))
      Rnmbl_conf$addSampler(target = c('mpsi'), type = 'sampler_reflective_RW', silent=TRUE, control = list(reflective=TRUE))
      Rnmbl_conf$removeSamplers(c('spsi'))
      Rnmbl_conf$addSampler(target = c('spsi'), type = 'sampler_reflective_RW', silent=TRUE, control = list(reflective=TRUE))
      Rnmbl_conf$removeSamplers(c('psiarr'))
      Rnmbl_conf$addSampler(target = c('psiarr'), type = 'sampler_reflective_RW', targetByNode=TRUE, silent=TRUE, control = list(reflective=TRUE))
    } else { # common_psi
      Rnmbl_conf$removeSamplers(c('psi'))
      Rnmbl_conf$addSampler(target = c('psi'), type = 'sampler_reflective_RW', silent=TRUE, control = list(reflective=TRUE))
    }
    if (!common_tau) {
      Rnmbl_conf$removeSamplers(c('tauarr'))
        Rnmbl_conf$addSampler(target = c('tauarr'), type = 'sampler_reflective_RW', targetByNode=TRUE, silent=TRUE, control = list(reflective=TRUE))
    } else { # common_tau
      Rnmbl_conf$removeSamplers(c('tau'))
      Rnmbl_conf$addSampler(target = c('tau'), type = 'sampler_reflective_RW', silent=TRUE, control = list(reflective=TRUE))
    }
  }
  
  # Custom eMKF samplers
  if (eMKFcustomSamplers) {
    
    # Custom Gibbs samplers for regression coefficients (and model flags in the BMA cases)
    if (fp > 1) {
      if (breakType == "full_break") {
        if (s1p == 4 && s2p == 4) {
          assign('sampler_eMKF_model_indicator', sampler_FP_xptf_bmac_bmac, envir = .GlobalEnv)
          Rnmbl_conf$removeSamplers(c('flg'))
          Rnmbl_conf$addSampler(target = c('flg'), type = 'sampler_eMKF_model_indicator', silent = TRUE)
          assign('sampler_eMKF_coefficient', sampler_CPb_xptf_bmac_bmac, envir = .GlobalEnv)
          Rnmbl_conf$removeSamplers(c('s1b1g', 's1b2g', 's1b3g', 's2b1g', 's2b2g', 's2b3g'))
          Rnmbl_conf$addSampler(target = c('s1b1g', 's1b2g', 's1b3g', 's2b1g', 's2b2g', 's2b3g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
          assign('sampler_eMKF_intercept', sampler_CPa_xptf_bmac_bmac, envir = .GlobalEnv)
          Rnmbl_conf$removeSamplers(c('s1ag', 's2ag'))
          Rnmbl_conf$addSampler(target = c('s1ag', 's2ag'), type = 'sampler_eMKF_intercept', silent = TRUE)
        } else {
          if (s1p == 4 && s2p == 3) {
            assign('sampler_eMKF_model_indicator', sampler_FP_xptf_bmac_bmaq, envir = .GlobalEnv)
            Rnmbl_conf$removeSamplers(c('flg'))
            Rnmbl_conf$addSampler(target = c('flg'), type = 'sampler_eMKF_model_indicator', silent = TRUE)
            assign('sampler_eMKF_coefficient', sampler_CPb_xptf_bmac_bmaq, envir = .GlobalEnv)
            Rnmbl_conf$removeSamplers(c('s1b1g', 's1b2g', 's1b3g', 's2b1g', 's2b2g'))
            Rnmbl_conf$addSampler(target = c('s1b1g', 's1b2g', 's1b3g', 's2b1g', 's2b2g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
            assign('sampler_eMKF_intercept', sampler_CPa_xptf_bmac_bmaq, envir = .GlobalEnv)
            Rnmbl_conf$removeSamplers(c('s1ag', 's2ag'))
            Rnmbl_conf$addSampler(target = c('s1ag', 's2ag'), type = 'sampler_eMKF_intercept', silent = TRUE)
          } else {
            if (s1p == 4 && s2p == 2) {
              assign('sampler_eMKF_model_indicator', sampler_FP_xptf_bmac_bmal, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('flg'))
              Rnmbl_conf$addSampler(target = c('flg'), type = 'sampler_eMKF_model_indicator', silent = TRUE)
              assign('sampler_eMKF_coefficient', sampler_CPb_xptf_bmac_bmal, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('s1b1g', 's1b2g', 's1b3g', 's2b1g'))
              Rnmbl_conf$addSampler(target = c('s1b1g', 's1b2g', 's1b3g', 's2b1g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
              assign('sampler_eMKF_intercept', sampler_CPa_xptf_bmac_bmal, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('s1ag', 's2ag'))
              Rnmbl_conf$addSampler(target = c('s1ag', 's2ag'), type = 'sampler_eMKF_intercept', silent = TRUE)
            } else {
              if (s1p == 3 && s2p == 3) {
                assign('sampler_eMKF_model_indicator', sampler_FP_xptf_bmaq_bmaq, envir = .GlobalEnv)
                Rnmbl_conf$removeSamplers(c('flg'))
                Rnmbl_conf$addSampler(target = c('flg'), type = 'sampler_eMKF_model_indicator', silent = TRUE)
                assign('sampler_eMKF_coefficient', sampler_CPb_xptf_bmaq_bmaq, envir = .GlobalEnv)
                Rnmbl_conf$removeSamplers(c('s1b1g', 's1b2g', 's2b1g', 's2b2g'))
                Rnmbl_conf$addSampler(target = c('s1b1g', 's1b2g', 's2b1g', 's2b2g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                assign('sampler_eMKF_intercept', sampler_CPa_xptf_bmaq_bmaq, envir = .GlobalEnv)
                Rnmbl_conf$removeSamplers(c('s1ag', 's2ag'))
                Rnmbl_conf$addSampler(target = c('s1ag', 's2ag'), type = 'sampler_eMKF_intercept', silent = TRUE)
              } else {
                if (s1p == 3 && s2p == 2) {
                  assign('sampler_eMKF_model_indicator', sampler_FP_xptf_bmaq_bmal, envir = .GlobalEnv)
                  Rnmbl_conf$removeSamplers(c('flg'))
                  Rnmbl_conf$addSampler(target = c('flg'), type = 'sampler_eMKF_model_indicator', silent = TRUE)
                  assign('sampler_eMKF_coefficient', sampler_CPb_xptf_bmaq_bmal, envir = .GlobalEnv)
                  Rnmbl_conf$removeSamplers(c('s1b1g', 's1b2g', 's2b1g'))
                  Rnmbl_conf$addSampler(target = c('s1b1g', 's1b2g', 's2b1g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                  assign('sampler_eMKF_intercept', sampler_CPa_xptf_bmaq_bmal, envir = .GlobalEnv)
                  Rnmbl_conf$removeSamplers(c('s1ag', 's2ag'))
                  Rnmbl_conf$addSampler(target = c('s1ag', 's2ag'), type = 'sampler_eMKF_intercept', silent = TRUE)
                } else { # (s1p == 2 && s2p == 2)
                  assign('sampler_eMKF_model_indicator', sampler_FP_xptf_bmal_bmal, envir = .GlobalEnv)
                  Rnmbl_conf$removeSamplers(c('flg'))
                  Rnmbl_conf$addSampler(target = c('flg'), type = 'sampler_eMKF_model_indicator', silent = TRUE)
                  assign('sampler_eMKF_coefficient', sampler_CPb_xptf_bmal_bmal, envir = .GlobalEnv)
                  Rnmbl_conf$removeSamplers(c('s1b1g', 's2b1g'))
                  Rnmbl_conf$addSampler(target = c('s1b1g', 's2b1g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                  assign('sampler_eMKF_intercept', sampler_CPa_xptf_bmal_bmal, envir = .GlobalEnv)
                  Rnmbl_conf$removeSamplers(c('s1ag', 's2ag'))
                  Rnmbl_conf$addSampler(target = c('s1ag', 's2ag'), type = 'sampler_eMKF_intercept', silent = TRUE)
                }
              }
            }
          }
        }
      } else {
        if (breakType == "level_break") {
          if (p == 5) {
            assign('sampler_eMKF_model_indicator', sampler_FP_xptl_bmac, envir = .GlobalEnv)
            Rnmbl_conf$removeSamplers(c('flg'))
            Rnmbl_conf$addSampler(target = c('flg'), type = 'sampler_eMKF_model_indicator', silent = TRUE)
            assign('sampler_eMKF_coefficient', sampler_CPb_xptl_bmac, envir = .GlobalEnv)
            Rnmbl_conf$removeSamplers(c('b1g', 'b2g', 'b3g'))
            Rnmbl_conf$addSampler(target = c('b1g', 'b2g', 'b3g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
            assign('sampler_eMKF_intercept', sampler_CPa_xptl_bmac, envir = .GlobalEnv)
            Rnmbl_conf$removeSamplers(c('s1ag', 's2ag'))
            Rnmbl_conf$addSampler(target = c('s1ag', 's2ag'), type = 'sampler_eMKF_intercept', silent = TRUE)
          } else {
            if (p == 4) {
              assign('sampler_eMKF_model_indicator', sampler_FP_xptl_bmaq, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('flg'))
              Rnmbl_conf$addSampler(target = c('flg'), type = 'sampler_eMKF_model_indicator', silent = TRUE)
              assign('sampler_eMKF_coefficient', sampler_CPb_xptl_bmaq, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('b1g', 'b2g'))
              Rnmbl_conf$addSampler(target = c('b1g', 'b2g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
              assign('sampler_eMKF_intercept', sampler_CPa_xptl_bmaq, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('s1ag', 's2ag'))
              Rnmbl_conf$addSampler(target = c('s1ag', 's2ag'), type = 'sampler_eMKF_intercept', silent = TRUE)
            } else {
              assign('sampler_eMKF_model_indicator', sampler_FP_xptl_bmal, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('flg'))
              Rnmbl_conf$addSampler(target = c('flg'), type = 'sampler_eMKF_model_indicator', silent = TRUE)
              assign('sampler_eMKF_coefficient', sampler_CPb_xptl_bmal, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('b1g'))
              Rnmbl_conf$addSampler(target = c('b1g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
              assign('sampler_eMKF_intercept', sampler_CPa_xptl_bmal, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('s1ag', 's2ag'))
              Rnmbl_conf$addSampler(target = c('s1ag', 's2ag'), type = 'sampler_eMKF_intercept', silent = TRUE)
            }
          }   
        } else {
          if (p == 4) {
            assign('sampler_eMKF_model_indicator', sampler_FP_bmac, envir = .GlobalEnv)
            Rnmbl_conf$removeSamplers(c('flg'))
            Rnmbl_conf$addSampler(target = c('flg'), type = 'sampler_eMKF_model_indicator', silent = TRUE)
            assign('sampler_eMKF_coefficient', sampler_CPb_bmac, envir = .GlobalEnv)
            Rnmbl_conf$removeSamplers(c('b1g', 'b2g', 'b3g'))
            Rnmbl_conf$addSampler(target = c('b1g', 'b2g', 'b3g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
            assign('sampler_eMKF_intercept', sampler_CPa_bmac, envir = .GlobalEnv)
            Rnmbl_conf$removeSamplers(c('ag'))
            Rnmbl_conf$addSampler(target = c('ag'), type = 'sampler_eMKF_intercept', silent = TRUE)
          } else {
            if (p == 3) {
              assign('sampler_eMKF_model_indicator', sampler_FP_bmaq, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('flg'))
              Rnmbl_conf$addSampler(target = c('flg'), type = 'sampler_eMKF_model_indicator', silent = TRUE)
              assign('sampler_eMKF_coefficient', sampler_CPb_bmaq, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('b1g', 'b2g'))
              Rnmbl_conf$addSampler(target = c('b1g', 'b2g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
              assign('sampler_eMKF_intercept', sampler_CPa_bmaq, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('ag'))
              Rnmbl_conf$addSampler(target = c('ag'), type = 'sampler_eMKF_intercept', silent = TRUE)
            } else {
              assign('sampler_eMKF_model_indicator', sampler_FP_bmal, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('flg'))
              Rnmbl_conf$addSampler(target = c('flg'), type = 'sampler_eMKF_model_indicator', silent = TRUE)
              assign('sampler_eMKF_coefficient', sampler_CPb_bmal, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('b1g'))
              Rnmbl_conf$addSampler(target = c('b1g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
              assign('sampler_eMKF_intercept', sampler_CPa_bmal, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('ag'))
              Rnmbl_conf$addSampler(target = c('ag'), type = 'sampler_eMKF_intercept', silent = TRUE)
            }
          }
        }
      }
    } else {
      if (breakType == "full_break") {
        if (bayesModel == "indep_cubic_cubic") {
          assign('sampler_eMKF_coefficient', sampler_CP_xptf_bgc_bgc, envir = .GlobalEnv)
          Rnmbl_conf$removeSamplers(c('s1ag', 's1b1g', 's1b2g', 's1b3g', 's2ag', 's2b1g', 's2b2g', 's2b3g'))
          Rnmbl_conf$addSampler(target = c('s1ag', 's1b1g', 's1b2g', 's1b3g', 's2ag', 's2b1g', 's2b2g', 's2b3g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
        } else {
          if (bayesModel == "indep_cubic_quad") {
            assign('sampler_eMKF_coefficient', sampler_CP_xptf_bgc_bgq, envir = .GlobalEnv)
            Rnmbl_conf$removeSamplers(c('s1ag', 's1b1g', 's1b2g', 's1b3g', 's2ag', 's2b1g', 's2b2g'))
            Rnmbl_conf$addSampler(target = c('s1ag', 's1b1g', 's1b2g', 's1b3g', 's2ag', 's2b1g', 's2b2g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
          } else {
            if (bayesModel == "indep_cubic_linear") {
              assign('sampler_eMKF_coefficient', sampler_CP_xptf_bgc_bgl, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('s1ag', 's1b1g', 's1b2g', 's1b3g', 's2ag', 's2b1g'))
              Rnmbl_conf$addSampler(target = c('s1ag', 's1b1g', 's1b2g', 's1b3g', 's2ag', 's2b1g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
            } else {
              if (bayesModel == "indep_quad_quad") {
                assign('sampler_eMKF_coefficient', sampler_CP_xptf_bgq_bgq, envir = .GlobalEnv)
                Rnmbl_conf$removeSamplers(c('s1ag', 's1b1g', 's1b2g', 's2ag', 's2b1g', 's2b2g'))
                Rnmbl_conf$addSampler(target = c('s1ag', 's1b1g', 's1b2g', 's2ag', 's2b1g', 's2b2g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
              } else {
                if (bayesModel == "indep_quad_linear") {
                  assign('sampler_eMKF_coefficient', sampler_CP_xptf_bgq_bgl, envir = .GlobalEnv)
                  Rnmbl_conf$removeSamplers(c('s1ag', 's1b1g', 's1b2g', 's2ag', 's2b1g'))
                  Rnmbl_conf$addSampler(target = c('s1ag', 's1b1g', 's1b2g', 's2ag', 's2b1g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                } else {
                  if (bayesModel == "indep_linear_linear") {
                    assign('sampler_eMKF_coefficient', sampler_CP_xptf_bgl_bgl, envir = .GlobalEnv)
                    Rnmbl_conf$removeSamplers(c('s1ag', 's1b1g', 's2ag', 's2b1g'))
                    Rnmbl_conf$addSampler(target = c('s1ag', 's1b1g', 's2ag', 's2b1g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                  } else {
                    if (bayesModel == "dropped_dropped") {
                      assign('sampler_eMKF_coefficient', sampler_CP_xptf_b0_b0, envir = .GlobalEnv)
                      Rnmbl_conf$removeSamplers(c('s1ag', 's2ag'))
                      Rnmbl_conf$addSampler(target = c('s1ag', 's2ag'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                    } else {
                      if (bayesModel == "common_cubic_cubic") {
                        assign('sampler_eMKF_coefficient', sampler_CP_xptf_b1c_b1c, envir = .GlobalEnv)
                        Rnmbl_conf$removeSamplers(c('s1ag', 's1b1', 's1b2', 's1b3', 's2ag', 's2b1', 's2b2', 's2b3'))
                        Rnmbl_conf$addSampler(target = c('s1ag', 's1b1', 's1b2', 's1b3', 's2ag', 's2b1', 's2b2', 's2b3'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                      } else {
                        if (bayesModel == "common_cubic_quad") {
                          assign('sampler_eMKF_coefficient', sampler_CP_xptf_b1c_b1q, envir = .GlobalEnv)
                          Rnmbl_conf$removeSamplers(c('s1ag', 's1b1', 's1b2', 's1b3', 's2ag', 's2b1', 's2b2'))
                          Rnmbl_conf$addSampler(target = c('s1ag', 's1b1', 's1b2', 's1b3', 's2ag', 's2b1', 's2b2'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                        } else {
                          if (bayesModel == "common_cubic_linear") {
                            assign('sampler_eMKF_coefficient', sampler_CP_xptf_b1c_b1l, envir = .GlobalEnv)
                            Rnmbl_conf$removeSamplers(c('s1ag', 's1b1', 's1b2', 's1b3', 's2ag', 's2b1'))
                            Rnmbl_conf$addSampler(target = c('s1ag', 's1b1', 's1b2', 's1b3', 's2ag', 's2b1'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                          } else {
                            if (bayesModel == "common_quad_quad") {
                              assign('sampler_eMKF_coefficient', sampler_CP_xptf_b1q_b1q, envir = .GlobalEnv)
                              Rnmbl_conf$removeSamplers(c('s1ag', 's1b1', 's1b2', 's2ag', 's2b1', 's2b2'))
                              Rnmbl_conf$addSampler(target = c('s1ag', 's1b1', 's1b2', 's2ag', 's2b1', 's2b2'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                            } else {
                              if (bayesModel == "common_quad_linear") {
                                assign('sampler_eMKF_coefficient', sampler_CP_xptf_b1q_b1l, envir = .GlobalEnv)
                                Rnmbl_conf$removeSamplers(c('s1ag', 's1b1', 's1b2', 's2ag', 's2b1'))
                                Rnmbl_conf$addSampler(target = c('s1ag', 's1b1', 's1b2', 's2ag', 's2b1'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                              } else {
                                assign('sampler_eMKF_coefficient', sampler_CP_xptf_b1l_b1l, envir = .GlobalEnv)
                                Rnmbl_conf$removeSamplers(c('s1ag', 's1b1', 's2ag', 's2b1'))
                                Rnmbl_conf$addSampler(target = c('s1ag', 's1b1', 's2ag', 's2b1'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                              }
                            }
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        }
      } else {
        if (breakType == "level_break") {
          if (bayesModel == "indep_cubic") {
            assign('sampler_eMKF_coefficient', sampler_CP_xptl_bgc, envir = .GlobalEnv)
            Rnmbl_conf$removeSamplers(c('s1ag', 's2ag', 'b1g', 'b2g', 'b3g'))
            Rnmbl_conf$addSampler(target = c('s1ag', 's2ag', 'b1g', 'b2g', 'b3g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
          } else {
            if (bayesModel == "indep_quad") {
              assign('sampler_eMKF_coefficient', sampler_CP_xptl_bgq, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('s1ag', 's2ag', 'b1g', 'b2g'))
              Rnmbl_conf$addSampler(target = c('s1ag', 's2ag', 'b1g', 'b2g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
            } else {
              if (bayesModel == "indep_linear") {
                assign('sampler_eMKF_coefficient', sampler_CP_xptl_bgl, envir = .GlobalEnv)
                Rnmbl_conf$removeSamplers(c('s1ag', 's2ag', 'b1g'))
                Rnmbl_conf$addSampler(target = c('s1ag', 's2ag', 'b1g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
              } else {
                if (bayesModel == "dropped") {
                  assign('sampler_eMKF_coefficient', sampler_CP_xptl_b0, envir = .GlobalEnv)
                  Rnmbl_conf$removeSamplers(c('s1ag', 's2ag'))
                  Rnmbl_conf$addSampler(target = c('s1ag', 's2ag'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                } else {
                  if (bayesModel == "common_cubic") {
                    assign('sampler_eMKF_coefficient', sampler_CP_xptl_b1c, envir = .GlobalEnv)
                    Rnmbl_conf$removeSamplers(c('s1ag', 's2ag', 'b1', 'b2', 'b3'))
                    Rnmbl_conf$addSampler(target = c('s1ag', 's2ag', 'b1', 'b2', 'b3'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                  } else {
                    if (bayesModel == "common_quad") {
                      assign('sampler_eMKF_coefficient', sampler_CP_xptl_b1q, envir = .GlobalEnv)
                      Rnmbl_conf$removeSamplers(c('s1ag', 's2ag', 'b1', 'b2'))
                      Rnmbl_conf$addSampler(target = c('s1ag', 's2ag', 'b1', 'b2'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                    } else {
                      assign('sampler_eMKF_coefficient', sampler_CP_xptl_b1l, envir = .GlobalEnv)
                      Rnmbl_conf$removeSamplers(c('s1ag', 's2ag', 'b1'))
                      Rnmbl_conf$addSampler(target = c('s1ag', 's2ag', 'b1'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                    }
                  }
                }
              }
            }
          }          
        } else {
          if (bayesModel == "indep_cubic") {
            assign('sampler_eMKF_coefficient', sampler_CP_bgc, envir = .GlobalEnv)
            Rnmbl_conf$removeSamplers(c('ag', 'b1g', 'b2g', 'b3g'))
            Rnmbl_conf$addSampler(target = c('ag', 'b1g', 'b2g', 'b3g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
          } else {
            if (bayesModel == "indep_quad") {
              assign('sampler_eMKF_coefficient', sampler_CP_bgq, envir = .GlobalEnv)
              Rnmbl_conf$removeSamplers(c('ag', 'b1g', 'b2g'))
              Rnmbl_conf$addSampler(target = c('ag', 'b1g', 'b2g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
            } else {
              if (bayesModel == "indep_linear") {
                assign('sampler_eMKF_coefficient', sampler_CP_bgl, envir = .GlobalEnv)
                Rnmbl_conf$removeSamplers(c('ag', 'b1g'))
                Rnmbl_conf$addSampler(target = c('ag', 'b1g'), type = 'sampler_eMKF_coefficient', silent = TRUE)
              } else {
                if (bayesModel == "dropped") {
                  assign('sampler_eMKF_coefficient', sampler_CP_b0, envir = .GlobalEnv)
                  Rnmbl_conf$removeSamplers(c('ag'))
                  Rnmbl_conf$addSampler(target = c('ag'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                } else {
                  if (bayesModel == "common_cubic") {
                    assign('sampler_eMKF_coefficient', sampler_CP_b1c, envir = .GlobalEnv)
                    Rnmbl_conf$removeSamplers(c('ag', 'b1', 'b2', 'b3'))
                    Rnmbl_conf$addSampler(target = c('ag', 'b1', 'b2', 'b3'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                  } else {
                    if (bayesModel == "common_quad") {
                      assign('sampler_eMKF_coefficient', sampler_CP_b1q, envir = .GlobalEnv)
                      Rnmbl_conf$removeSamplers(c('ag', 'b1', 'b2'))
                      Rnmbl_conf$addSampler(target = c('ag', 'b1', 'b2'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                    } else {
                      assign('sampler_eMKF_coefficient', sampler_CP_b1l, envir = .GlobalEnv)
                      Rnmbl_conf$removeSamplers(c('ag', 'b1'))
                      Rnmbl_conf$addSampler(target = c('ag', 'b1'), type = 'sampler_eMKF_coefficient', silent = TRUE)
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
    
    # Custom Gibbs sampler for true states
    assign('sampler_eMKF_true_state', sampler_EP, envir = .GlobalEnv)
    Rnmbl_conf$removeSamplers(c('etaarr'))
    Rnmbl_conf$addSampler(target = c('etaarr'), type = 'sampler_eMKF_true_state', silent = TRUE)

    # Custom Gibbs sampler for variance parameters in the random variances case
    if (randomVars) {
      assign('sampler_eMKF_variance', sampler_RP, envir = .GlobalEnv)
      Rnmbl_conf$removeSamplers(c('varr'))
      Rnmbl_conf$addSampler(target = c('varr'), type = 'sampler_eMKF_variance', silent = TRUE)
    }
    
  }
  
  # Clean-up unused custom eMKF samplers
  rm(list = ls(pattern = "^sampler_FP", envir = .GlobalEnv), envir = .GlobalEnv)
  rm(list = ls(pattern = "^sampler_CP", envir = .GlobalEnv), envir = .GlobalEnv)
  rm(sampler_EP, envir = .GlobalEnv)
  rm(sampler_RP, envir = .GlobalEnv)

  # Build uncompiled MCMC
  Rnmbl_mcmc <- buildMCMC(Rnmbl_conf, print = FALSE)
  
  # Compile NIMBLE model and MCMC
  message("")
  stime <- system.time(
    Cnmbl_mcmc <- compileNimble(Rnmbl_model, Rnmbl_mcmc, projectName = "eMKF", dirName = dllloc, resetFunctions = TRUE)
    )
  message(paste("  [Note] CPU time used for compiling model and MCMC in C++ was", round(stime[3],2), "seconds.", sep=" ")) 
  
  # Print the monitors and samplers
  if (modelPrint){
    cat("\n")
    message("MCMC overview")
    if (eMKFcustomSamplers) {
      cat("==== Monitors ====\n")
      Rnmbl_conf$printMonitors()
      cat("==== Samplers ====\n")
      Rnmbl_conf$printSamplers(byType = TRUE, type = "RW$")
      cat("eMKF samplers in execution order\n")
      Rnmbl_conf$printSamplers(executionOrder = TRUE, type = "^eMKF")
    } else {
      print(Rnmbl_conf)
    }
  }
  
  # Reset NIMBLE options to show progress bars with chain labels irrespective of modelPrint
  nimbleOptions(verbose = TRUE)
  
  # Run the MCMC and assign the posterior samples to the given variable in the global environment
  bayeslog <- paste(out, paste("postSample_rep", 1, sep=""), sep = "_")
  if (nreps > 1) { 
    cat("\n")
    message(paste("Replication 1 of", nreps)) 
  } else {
    cat("\n")
  }
  
  # Monitored nodes in the order specified in Rnmbl_mntr
  Cnmbl_mntr <- Cnmbl_mcmc[['Rnmbl_model']]$expandNodeNames(Rnmbl_mntr, returnScalarComponents = TRUE)

  # In the BMA cases, remove extraneous (g+1)st element of the coefficient vectors from the posterior sample
  if (fp > 1) {
    Cnmbl_extr <- c(paste("b1g[", g+1, "]", sep=""), paste("b2g[", g+1, "]", sep=""), paste("b3g[", g+1, "]", sep=""),
                    paste("s1b1g[", g+1, "]", sep=""), paste("s1b2g[", g+1, "]", sep=""), paste("s1b3g[", g+1, "]", sep=""),
                    paste("s2b1g[", g+1, "]", sep=""), paste("s2b2g[", g+1, "]", sep=""), paste("s2b3g[", g+1, "]", sep=""))
    Cnmbl_mntr <- setdiff(Cnmbl_mntr, Cnmbl_extr)
  }
  
  message("MCMC started")
  stime <- system.time(
    assign(bayeslog, 
           runMCMC(Cnmbl_mcmc[['Rnmbl_mcmc']],
                   nburnin = nbi, niter = nbi + nmc, thin = thin, nchains = chains, 
                   setSeed = c(seed + 1:chains - 1), inits = init_list, samplesAsCodaMCMC = TRUE)[, Cnmbl_mntr],
           envir = .GlobalEnv)
    )
  message("MCMC concluded")
  message(paste("  [Note] CPU time used for running MCMC was", round(stime[3],2), "seconds.", sep=" ")) 
  
  # Ensure 'bayeslog' remains an mcmc.list object when only one chain is used
  if (chains == 1) {
    assign(bayeslog, 
           coda::mcmc.list(chain1 = get(bayeslog, envir = .GlobalEnv)), 
           envir = .GlobalEnv)
  }
  
  # Post processing calculations of group differences/disparities at the last timepoint
  lastCols <- paste("etaarr[", seq(n, g*n, n), "]", sep="")
  stime <- system.time({
    if (comparedTo != "") {
      assign(paste(bayeslog, "eMKFdisparities", sep="_"), 
             eMKFpostDisparities(bayeslog, lastCols), 
             envir = .GlobalEnv)
      assign(paste(bayeslog, "disparities_summary", sep="_"), 
             eMKFpostProcessing(dataName = paste(bayeslog, "eMKFdisparities", sep="_"), outName = out,
                                compute.splitRhats = FALSE, compute.HPDintervals = FALSE), 
             envir = .GlobalEnv)
    }
    
    # Post processing calculations for model flags in the BMA scenarios
    if (fp > 1) {
      if (breakType == "full_break") {
        assign(paste(bayeslog, "mflg", sep="_"), 
               eMKFpostProcessing(dataName = bayeslog, colNames = c("s1flg", "s2flg"), outName = out,
                                  frequencies.only = TRUE, fp = fp, bayesModel = bayesModel), 
               envir = .GlobalEnv) 
      } else {
          assign(paste(bayeslog, "mflg", sep="_"), 
                 eMKFpostProcessing(dataName = bayeslog, colNames = c("flg"), outName = out,
                                    frequencies.only = TRUE, fp = fp, bayesModel = bayesModel), 
                 envir = .GlobalEnv)   
      }
    }
    
    # Post processing for non-categorical model parameters and generation of Gelman-Rubin diagnostics
    if (fp > 1) {
      if (breakType == "full_break") {
        paramNames <- setdiff(coda::varnames(get(bayeslog, envir = .GlobalEnv)), c("s1flg", "s2flg"))
      } else {
        paramNames <- setdiff(coda::varnames(get(bayeslog, envir = .GlobalEnv)), c("flg"))        
      }
    } else {
      paramNames <- coda::varnames(get(bayeslog, envir = .GlobalEnv))
    }
    assign(paste(bayeslog, "summary", sep="_"), 
           eMKFpostProcessing(dataName = bayeslog, colNames = paramNames, outName = out,
                              compute.HPDintervals = mcmcHPDintervals, threshold = GRthreshold), 
           envir = .GlobalEnv)
    
  })
  
  message(paste("  [Note] CPU time used for post-processing was", round(stime[3],2), "seconds.", sep=" ")) 
  
  # Update data list and initial values and re-run MCMC without re-compiling
  if (nreps > 1) { 
    for (r in 2:nreps) {
      data_list <- eMKFdataList(dataName =  paste(out, "eMKFbayesdata", sep="_"),
                                subset = ( get(paste(out, "eMKFbayesdata", sep="_"), envir = .GlobalEnv)[, "_rep"] == r ),
                                p, fp, g, n, s1p, s1n, outcome, se, neff,
                                malpha, palpha, mbeta1, pbeta1, mbeta2, pbeta2, mbeta3, pbeta3,
                                s2malpha, s2palpha, s2mbeta1, s2pbeta1, s2mbeta2, s2pbeta2, s2mbeta3, s2pbeta3,
                                mrho, prho, taul, tauu, randomVars, vshape, vscale, wshape)      
      if (fp > 1) {
        if (breakType == "full_break") {
          for (chain in 1:chains) {
            init_list[[chain]] <- bayesBMAxptf_init(cubic_cubic_trend, cubic_quadratic_trend, cubic_linear_trend, 
                                                    quadratic_quadratic_trend, quadratic_linear_trend, linear_linear_trend, 
                                                    common_ar, common_psi, common_tau,
                                                    randomVars, orpoly, fp, g, p, n, s1p, s1n, rts,
                                                    data_list$mbetag, data_list$Dbetag, data_list$mrho, data_list$srho,
                                                    data_list$taul, data_list$tauu, data_list$vshape, data_list$vscale, data_list$wshape,
                                                    init_seed = init_seed + chain - 1)
          }
        } else {
          if (breakType == "level_break") {
            for (chain in 1:chains) {
              init_list[[chain]] <- bayesBMAxptl_init(cubic_trend, quadratic_trend, linear_trend, 
                                                      common_ar, common_psi, common_tau,
                                                      randomVars, orpoly, fp, g, p, n, s1n, rts,
                                                      data_list$mbetag, data_list$Dbetag, data_list$mrho, data_list$srho,
                                                      data_list$taul, data_list$tauu, data_list$vshape, data_list$vscale, data_list$wshape,
                                                      init_seed = init_seed + chain - 1)
            }
          } else {
            for (chain in 1:chains) {
              init_list[[chain]] <- bayesBMA_init(cubic_trend, quadratic_trend, linear_trend, 
                                                  common_ar, common_psi, common_tau,
                                                  randomVars, orpoly, fp, g, p, n, rts,
                                                  data_list$mbetag, data_list$Dbetag, data_list$mrho, data_list$srho,
                                                  data_list$taul, data_list$tauu, data_list$vshape, data_list$vscale, data_list$wshape,
                                                  init_seed = init_seed + chain - 1)
            }
          }
        }
      } else {
        if (breakType == "full_break") {
          for (chain in 1:chains) {
            init_list[[chain]] <- bayesfitxptf_init(cubic_cubic_trend, cubic_quadratic_trend, cubic_linear_trend, quadratic_quadratic_trend, 
                                                    quadratic_linear_trend, linear_linear_trend, dropped_dropped_trend, common_trend, 
                                                    common_ar, common_psi, common_tau, 
                                                    randomVars, orpoly, g, p, n, s1p, s1n, rts,
                                                    data_list$mbetag, data_list$Dbetag, data_list$mrho, data_list$srho,
                                                    data_list$taul, data_list$tauu, data_list$vshape, data_list$vscale,
                                                    init_seed = init_seed + chain - 1)
          }
        } else {
          if (breakType == "level_break") {
            for (chain in 1:chains) {
              init_list[[chain]] <- bayesfitxptl_init(cubic_trend, quadratic_trend, linear_trend, dropped_trend, common_trend, 
                                                      common_ar, common_psi, common_tau, 
                                                      randomVars, orpoly, g, p, n, s1n, rts,
                                                      data_list$mbetag, data_list$Dbetag, data_list$mrho, data_list$srho,
                                                      data_list$taul, data_list$tauu, data_list$vshape, data_list$vscale,
                                                      init_seed = init_seed + chain - 1)
            }
          } else {
            for (chain in 1:chains) {
              init_list[[chain]] <- bayesfit_init(cubic_trend, quadratic_trend, linear_trend, dropped_trend, common_trend, 
                                                  common_ar, common_psi, common_tau, 
                                                  randomVars, orpoly, g, p, n, rts,
                                                  data_list$mbetag, data_list$Dbetag, data_list$mrho, data_list$srho,
                                                  data_list$taul, data_list$tauu, data_list$vshape, data_list$vscale,
                                                  init_seed = init_seed + chain - 1)
            }
          }
        }
      }
      
      # Assign new values to 'data' nodes in NIMBLE model
      Cnmbl_mcmc[['Rnmbl_model']][['Yarr']] <- data_list$Yarr
      Cnmbl_mcmc[['Rnmbl_model']][['S2arr']] <- data_list$S2arr
      Cnmbl_mcmc[['Rnmbl_model']][['mbetag']] <- data_list$mbetag
      Cnmbl_mcmc[['Rnmbl_model']][['Dbetag']] <- data_list$Dbetag
      Cnmbl_mcmc[['Rnmbl_model']][['mrho']] <- data_list$mrho
      Cnmbl_mcmc[['Rnmbl_model']][['srho']] <- data_list$srho
      Cnmbl_mcmc[['Rnmbl_model']][['taul']] <- data_list$taul
      Cnmbl_mcmc[['Rnmbl_model']][['tauu']] <- data_list$tauu
      if (randomVars) { 
        Cnmbl_mcmc[['Rnmbl_model']][['Narr']] <- data_list$Narr 
        Cnmbl_mcmc[['Rnmbl_model']][['vshape']] <- data_list$vshape 
        Cnmbl_mcmc[['Rnmbl_model']][['vscale']] <- data_list$vscale 
      }
      if (fp > 1) {
        Cnmbl_mcmc[['Rnmbl_model']][['wshape']] <- data_list$wshape 
      }
      
      cat("\n")
      message(paste("Replication", r, "of", nreps)) 
      bayeslog <- paste(out, paste("postSample_rep", r, sep=""), sep = "_")
      message("MCMC started")
      stime <- system.time(
        assign(bayeslog, 
               runMCMC(Cnmbl_mcmc[['Rnmbl_mcmc']],
                               nburnin = nbi, niter = nbi + nmc, thin = thin, nchains = chains,
                               setSeed = c(seed + 1:chains), inits = init_list, samplesAsCodaMCMC = TRUE)[, Cnmbl_mntr],
               envir = .GlobalEnv)
      )
      message("MCMC concluded")
      message(paste("  [Note] CPU time used for MCMC was", round(stime[3],2), "seconds.", sep=" ")) 
      
      # Ensure 'bayeslog' remains an mcmc.list object when only one chain is used
      if (chains == 1) {
        assign(bayeslog, 
               coda::mcmc.list(chain1 = get(bayeslog, envir = .GlobalEnv)), 
               envir = .GlobalEnv)
      }
      
      # Post processing calculations of group differences/disparities at the last timepoint
      stime <- system.time({
        if (comparedTo != "") {
          assign(paste(bayeslog, "eMKFdisparities", sep="_"), 
                 eMKFpostDisparities(bayeslog, lastCols), 
                 envir = .GlobalEnv)
          assign(paste(bayeslog, "disparities_summary", sep="_"), 
                 eMKFpostProcessing(dataName = paste(bayeslog, "eMKFdisparities", sep="_"), outName = out,
                                    compute.splitRhats = FALSE, compute.HPDintervals = FALSE), 
                 envir = .GlobalEnv)
        }
        
        # Post processing calculations for model flags in the BMA scenarios
        if (fp > 1) {
          if (breakType == "full_break") {
            assign(paste(bayeslog, "mflg", sep="_"), 
                   eMKFpostProcessing(dataName = bayeslog, colNames = c("s1flg", "s2flg"), outName = out,
                                      frequencies.only = TRUE, fp = fp, bayesModel = bayesModel), 
                   envir = .GlobalEnv) 
          } else {
            assign(paste(bayeslog, "mflg", sep="_"), 
                   eMKFpostProcessing(dataName = bayeslog, colNames = c("flg"), outName = out,
                                      frequencies.only = TRUE, fp = fp, bayesModel = bayesModel), 
                   envir = .GlobalEnv)   
          }
        }
        
        # Post processing for non-categorical model parameters and generation of Gelman-Rubin diagnostics
        if (fp > 1) {
          if (breakType == "full_break") {
            paramNames <- setdiff(coda::varnames(get(bayeslog, envir = .GlobalEnv)), c("s1flg", "s2flg"))
          } else {
            paramNames <- setdiff(coda::varnames(get(bayeslog, envir = .GlobalEnv)), c("flg"))        
          }
        } else {
          paramNames <- coda::varnames(get(bayeslog, envir = .GlobalEnv))
        }
        assign(paste(bayeslog, "summary", sep="_"), 
               eMKFpostProcessing(dataName = bayeslog, colNames = paramNames, outName = out,
                                  compute.HPDintervals = mcmcHPDintervals, threshold = GRthreshold), 
               envir = .GlobalEnv)
        
      })
      
      message(paste("  [Note] CPU time used for post-processing was", round(stime[3],2), "seconds.", sep=" ")) 

    } # end for loop over replications
  }
  
  ##################
  # Wrapping-up... #
  ##################
  
  # Combine posterior samples into single list if mcmclog == TRUE, otherwise discard
  postSample_names <- paste(out, paste("postSample_rep", 1:nreps, sep=""), sep = "_")
  if (mcmclog) {
    objsToList(postSample_names, paste(out, "postSample", sep="_"), paste("rep", 1:nreps, sep=""))
  } else {
    rm(list=postSample_names, envir = .GlobalEnv)
  }
  
  # Combine posterior sample summaries into single list
  postSample_names <- paste(out, paste("postSample_rep", 1:nreps, sep=""), "summary", sep = "_")
  objsToList(postSample_names, paste(out, "postSample_summary", sep="_"), paste("rep", 1:nreps, sep=""))
  
  # Combine posterior sample frequencies for model flag into a single list and turn result into a data matrix
  if (fp > 1) {
    postSample_names <- paste(out, paste("postSample_rep", 1:nreps, sep=""), "mflg", sep = "_")
    objsToList(postSample_names, paste(out, "postSample_mflg", sep="_"), paste("rep", 1:nreps, sep=""))
  }
  
  # Combine Gelman-Rubin statistics into single list
  postSample_names <- paste(out, paste("postSample_rep", 1:nreps, sep=""), "GR", sep = "_")
  objsToList(postSample_names, paste(out, "postSample_GR", sep="_"), paste("rep", 1:nreps, sep=""))

  # Combine posterior samples for disparities into single list
  if (comparedTo != "") {
    
    # Do not retain the posterior samples for disparities calculations; just the posterior summaries
    postSample_names <- paste(out, paste("postSample_rep", 1:nreps, sep=""), "eMKFdisparities", sep = "_")
    rm(list=postSample_names, envir = .GlobalEnv)
    
    postSample_names <- paste(out, paste("postSample_rep", 1:nreps, sep=""), "disparities_summary", sep = "_")
    objsToList(postSample_names, paste(out, "postSample_eMKFdisparities", sep="_"), paste("rep", 1:nreps, sep=""))
    
    #########################################################
    # Prepare output data for disparities at last timepoint #
    #########################################################
    
    dispNames <- row.names(get(paste(out, "postSample_eMKFdisparities", sep="_"), envir=.GlobalEnv)[[1]])
    dispType <- vector("character", length(dispNames))
    lhsGroups <- vector("character", length(dispNames))
    lhsGroups0 <- vector("character", length(dispNames))
    rhsGroups <- vector("character", length(dispNames))
    rhsGroups0 <- vector("character", length(dispNames))
    
    dn <- 0L
    dstr <- character(0L)
    for (dn in seq_along(dispNames)) {
      dstr <- strsplit(dispNames[dn], "_")[[1]]
      if (length(dstr) == 3 && (dstr[1] == "diff" || dstr[1] == "ratio")) {
        if (dstr[1] == "diff") { dispType[dn] <- "RD" } else { dispType[dn] <- "RR" }
        lhsGroups[dn] <- dstr[2]
        if (dstr[2] != "MAX") { lhsGroups0[dn] <- names(groupsInSortOrder)[as.integer(dstr[2])] } else { lhsGroups0[dn] <- dstr[2] }
        rhsGroups[dn] <- dstr[3]
        if (dstr[3] != "MIN") { rhsGroups0[dn] <- names(groupsInSortOrder)[as.integer(dstr[3])] } else { rhsGroups0[dn] <- dstr[3] }
        } else {
        if (length(dstr) == 2 && (dstr[1] == "SRD" || dstr[1] == "SRR")) {
          dispType[dn] <- dstr[1]
          if (dstr[2] == "adv") {
            lhsGroups[dn] <- "AVG_excl_MIN"
            lhsGroups0[dn] <- "AVG_excl_MIN"
            rhsGroups[dn] <- "MIN"
            rhsGroups0[dn] <- "MIN"
          } else {
            if (dstr[2] == "fav") {
              lhsGroups[dn] <- "MAX"
              lhsGroups0[dn] <- "MAX"
              rhsGroups[dn] <- "AVG_excl_MAX"
              rhsGroups0[dn] <- "AVG_excl_MAX"
            }
          }
        } else {
          if (length(dstr) == 1 && (dstr[1] == "MRD" || dstr[1] == "MRR")) {
            dispType[dn] <- dstr[1]
            lhsGroups[dn] <- "MAX"
            lhsGroups0[dn] <- "MAX"
            rhsGroups[dn] <- "MIN"
            rhsGroups0[dn] <- "MIN"
          } else{
            dispType[dn] <- dispNames[dn]
            lhsGroups[dn] <- "--"
            lhsGroups0[dn] <- "--"
            rhsGroups[dn] <- "--"
            rhsGroups0[dn] <- "--"
          }
        }
      }
    }
    
    lhsGroupNames <- lhsGroups 
    lhsGroupNames[is.element(lhsGroups, as.character(1:g))] <- c(rep(rep(groupsInSortOrder, rep(g, g)), 2), rep(groupsInSortOrder, 2))
    rhsGroupNames <- rhsGroups 
    rhsGroupNames[is.element(lhsGroups, as.character(1:g))] <- c(rep(groupsInSortOrder, 2*g), rep("MIN", 2*g))
    rhsGroupNames[lhsGroups == "MAX" & is.element(rhsGroups, as.character(1:g))] <- rep(groupsInSortOrder, 2)
    
    ratioMeasures <- c("RR", "MRR", "SRR")
    diffMeasures <- c("RD", "MRD", "SRD")
    
    dispMeasure <- dispType
    dispMeasure[is.element(dispType, ratioMeasures)] <- paste(lhsGroupNames, "/", rhsGroupNames)[is.element(dispType, ratioMeasures)]
    dispMeasure[is.element(dispType, diffMeasures)] <- paste(lhsGroupNames, "-", rhsGroupNames)[is.element(dispType, diffMeasures)]
    
    timeNames <- rep(timesInSortOrder[n], length(dispNames))
    repNames <- rep(1, length(dispNames))
    if (by != "") {
      byNames <- rep(repsInSortOrder[1], length(dispNames))
      repNames0 <- rep(names(repsInSortOrder)[1], length(dispNames))
      lastDisparities <- data.frame(repNames, repNames0, byNames, dispMeasure, dispType, timeNames, lhsGroups0, lhsGroups, lhsGroupNames, rhsGroups0, rhsGroups, rhsGroupNames,
                                    get(paste(out, "postSample_eMKFdisparities", sep="_"), envir=.GlobalEnv)[[1]], row.names=NULL, check.names = FALSE)
    } else {
      repNames0 <- rep(1, length(dispNames))
      lastDisparities <- data.frame(repNames, repNames0, dispMeasure, dispType, timeNames, lhsGroups0, lhsGroups, lhsGroupNames, rhsGroups0, rhsGroups, rhsGroupNames,
                                    get(paste(out, "postSample_eMKFdisparities", sep="_"), envir=.GlobalEnv)[[1]], row.names=NULL, check.names = FALSE)
    }
    if (nreps > 1) {
      for (r in 2:nreps) {
        repNames <- rep(r, length(dispNames))
        if (by != "") {
          byNames <- rep(repsInSortOrder[r], length(dispNames))
          repNames0 <- rep(names(repsInSortOrder)[r], length(dispNames))
          lastDisparities <- rbind(lastDisparities,
                                   data.frame(repNames, repNames0, byNames, dispMeasure, dispType, timeNames, lhsGroups0, lhsGroups, lhsGroupNames, rhsGroups0, rhsGroups, rhsGroupNames,
                                              get(paste(out, "postSample_eMKFdisparities", sep="_"), envir=.GlobalEnv)[[r]], row.names=NULL, check.names = FALSE))
        } else {
          repNames0 <- rep(r, length(dispNames))
          lastDisparities <- rbind(lastDisparities,
                                   data.frame(repNames, repNames0, dispMeasure, dispType, timeNames, lhsGroups0, lhsGroups, lhsGroupNames, rhsGroups0, rhsGroups, rhsGroupNames,
                                              get(paste(out, "postSample_eMKFdisparities", sep="_"), envir=.GlobalEnv)[[r]], row.names=NULL, check.names = FALSE))
        }
      }
    }
    if (by != "") {
      names(lastDisparities) <- c("_rep", "_repIn", by, "Measure", "Type", time, "_lhsGroupIn", "_lhsGroup", paste(group, "(lhs)"), "_rhsGroupIn", "_rhsGroup", paste(group, "(rhs)"), "MKF estimate", "RMSE")
    } else {
      names(lastDisparities) <- c("_rep", "_repIn", "Measure", "Type", time, "_lhsGroupIn", "_lhsGroup", paste(group, "(lhs)"), "_rhsGroupIn", "_rhsGroup", paste(group, "(rhs)"), "MKF estimate", "RMSE")
    }
    
    # Compute CI limits for disparities estimates
    lastDisparities[!is.element(dispType, ratioMeasures), "95% LL"] <- lastDisparities[!is.element(dispType, ratioMeasures), "MKF estimate"] - 1.959964*lastDisparities[!is.element(dispType, ratioMeasures), "RMSE"]
    lastDisparities[!is.element(dispType, ratioMeasures), "95% UL"] <- lastDisparities[!is.element(dispType, ratioMeasures), "MKF estimate"] + 1.959964*lastDisparities[!is.element(dispType, ratioMeasures), "RMSE"]
    lastDisparities[is.element(dispType, ratioMeasures), "95% LL"] <- exp(log(lastDisparities[is.element(dispType, ratioMeasures), "MKF estimate"]) - 1.959964*lastDisparities[is.element(dispType, ratioMeasures), "RMSE"]/lastDisparities[is.element(dispType, ratioMeasures), "MKF estimate"])
    lastDisparities[is.element(dispType, ratioMeasures), "95% UL"] <- exp(log(lastDisparities[is.element(dispType, ratioMeasures), "MKF estimate"]) + 1.959964*lastDisparities[is.element(dispType, ratioMeasures), "RMSE"]/lastDisparities[is.element(dispType, ratioMeasures), "MKF estimate"])
    
    # Re-sort by groups in input order
    lastDisparities0 <- lastDisparities[is.element(lastDisparities[,"_lhsGroupIn"], as.character(1:g)) & lastDisparities[,"_rhsGroupIn"] != "MIN", ]
    lastDisparities0 <- lastDisparities0[order(as.integer(lastDisparities0[,"_repIn"]), lastDisparities0[,"Type"], as.integer(lastDisparities0[,"_lhsGroupIn"]), as.integer(lastDisparities0[,"_rhsGroupIn"])), ]
    lastDisparities1 <- lastDisparities[is.element(lastDisparities[,"Type"], c("RD", "RR")) & lastDisparities[,"_lhsGroupIn"] == "MAX", ]
    lastDisparities1 <- lastDisparities1[order(as.integer(lastDisparities1[,"_repIn"]), lastDisparities1[,"Type"], as.integer(lastDisparities1[,"_rhsGroupIn"])), ]
    lastDisparities2 <- lastDisparities[is.element(lastDisparities[,"Type"], c("RD", "RR")) & lastDisparities[,"_rhsGroupIn"] == "MIN", ]
    lastDisparities2 <- lastDisparities2[order(as.integer(lastDisparities2[,"_repIn"]), lastDisparities2[,"Type"], as.integer(lastDisparities2[,"_lhsGroupIn"])), ]
    lastDisparities3 <- lastDisparities[!is.element(lastDisparities[,"Type"], c("RD", "RR")), ]
    lastDisparities3 <- lastDisparities3[order(as.integer(lastDisparities3[,"_repIn"]), lastDisparities3[,"Type"]), ]
    
    lastDisparities[is.element(lastDisparities[,"_lhsGroupIn"], as.character(1:g)) & lastDisparities[,"_rhsGroupIn"] != "MIN", ] <- lastDisparities0
    lastDisparities[is.element(lastDisparities[,"Type"], c("RD", "RR")) & lastDisparities[,"_lhsGroupIn"] == "MAX", ] <- lastDisparities1
    lastDisparities[is.element(lastDisparities[,"Type"], c("RD", "RR")) & lastDisparities[,"_rhsGroupIn"] == "MIN", ] <- lastDisparities2
    lastDisparities[!is.element(lastDisparities[,"Type"], c("RD", "RR")), ] <- lastDisparities3
    rm(lastDisparities0, lastDisparities1, lastDisparities2, lastDisparities3)
    
    # Finalize and clean up
    rm(repNames, dispNames, dispMeasure, dispType, timeNames, lhsGroups0, lhsGroups, lhsGroupNames, rhsGroups0, rhsGroups, rhsGroupNames)
    if (by != "") { rm(byNames) }
    rm(list=paste(out, "postSample_eMKFdisparities", sep="_"), envir = .GlobalEnv)
    
  }
  
  #########################################################
  # Prepare output data for predictions at last timepoint #
  #########################################################
  
  # Isolate the posterior summaries for just the last timepoint
  lastTimepoint <- data.frame(get(paste(out, "postSample_summary", sep="_"), envir=.GlobalEnv)[[1]][lastCols,, drop=FALSE], row.names=NULL, check.names = FALSE)
  if (nreps > 1) {
    for (r in 2:nreps) {
      lastTimepoint <- rbind(lastTimepoint,
                             data.frame(get(paste(out, "postSample_summary", sep="_"), envir=.GlobalEnv)[[r]][lastCols,, drop=FALSE], row.names = NULL, check.names = FALSE))
    }
  }

  # Compute Wald CI limits for MKF estimates
  lastTimepoint[, "MKF Wald 95% LL"] <- lastTimepoint[, "MKF estimate"] - 1.959964*lastTimepoint[, "MKF SE"]
  lastTimepoint[, "MKF Wald 95% UL"] <- lastTimepoint[, "MKF estimate"] + 1.959964*lastTimepoint[, "MKF SE"]
  
  # Get data values from eMKFbayesdata
  lastRows <- get(paste(out, "eMKFbayesdata", sep="_"), envir = .GlobalEnv)[, "_time"] == n
  dataCols <- c(outcome, se)
  if (by != "") {
    dataCols <- c(by, group, time, "_rep", "_group", "_time", "_rtime", "_impute", "_imputeb", "_inputorder", dataCols)
  } else {
    dataCols <- c(group, time, "_rep", "_group", "_time", "_rtime", "_impute", "_imputeb", "_inputorder", dataCols)
  }
  lastDatapoint <- get(paste(out, "eMKFbayesdata", sep="_"), envir = .GlobalEnv)[lastRows, dataCols]
  names(lastDatapoint)[names(lastDatapoint) == outcome] <- "Sample estimate"
  names(lastDatapoint)[names(lastDatapoint) == se] <- "Sample SE"
  
  # Compute Wald CI limits for sample estimates
  lastDatapoint[, "Sample Wald 95% LL"] <- lastDatapoint[, "Sample estimate"] - 1.959964*lastDatapoint[, "Sample SE"]
  lastDatapoint[, "Sample Wald 95% UL"] <- lastDatapoint[, "Sample estimate"] + 1.959964*lastDatapoint[, "Sample SE"]
  
  # Compute standardized differences and relative RMSEs
  lastTimepoint[, "Std. Diff"] <- (lastTimepoint[, "MKF estimate"] - lastDatapoint[, "Sample estimate"])/lastDatapoint[, "Sample SE"]
  lastTimepoint[, "Rel. RMSE"] <- lastTimepoint[, "MKF SE"]/lastDatapoint[, "Sample SE"]
  
  # Combine into single data frame
  lastTimepoint <- cbind(lastDatapoint, lastTimepoint)
  rm(lastDatapoint)
  
  # Re-sort using initial/input order
  lastTimepoint <- lastTimepoint[order(lastTimepoint[, "_inputorder"]), ]
  row.names(lastTimepoint) <- NULL
  
  # Print predictions output table if requested
  if (finalPrint) {
    
    if (by != "") {
      labelCols <- c(by, group, time)
    } else {
      labelCols <- c(group, time)
    }
    
    tmp1 <- lastTimepoint[, c("_inputorder", labelCols)]
    tmp1[, "Source"] <- rep("Sample", nrow(tmp1))
    tmp1 <- cbind(tmp1, lastTimepoint[, c("Sample estimate", "Sample SE", "Sample Wald 95% LL", "Sample Wald 95% UL", "Std. Diff", "Rel. RMSE")])
    tmp1[, "Std. Diff"] <- 0
    tmp1[, "Rel. RMSE"] <- 1
    names(tmp1)[substr(names(tmp1), 1, 6) == "Sample"] <- c("Estimate", "RMSE", "95% LL", "95% UL")
    
    tmp2 <- lastTimepoint[, c("_inputorder", labelCols)]
    tmp2[, "Source"] <- rep("Model", nrow(tmp2))
    tmp2 <- cbind(tmp2, lastTimepoint[, c("MKF estimate", "MKF SE", "MKF Wald 95% LL", "MKF Wald 95% UL", "Std. Diff", "Rel. RMSE")])
    tmp2[, labelCols] <- ""
    names(tmp2)[substr(names(tmp2), 1, 3) == "MKF"] <- c("Estimate", "RMSE", "95% LL", "95% UL")
    
    tmp <- rbind(tmp1, tmp2)
    rm(tmp1, tmp2)
    tmp <- tmp[order(tmp[,"_inputorder"]),]
    tmp <- tmp[,-1]
    
    tmptitle <- ""
    
    if (fp > 1) {
      tmptitle <- paste(tmptitle, "Bayesian model average")
    } else {
      tmptitle <- paste(tmptitle, "Bayesian")
    }
    
    if (breakType == "full_break") {
      tmptitle <- switch(bayesModel,
                         bma_cubic_cubic = paste(tmptitle, " up to the unconstrained cubic trend before and after ", breakPoint, ", with", sep=""),
                         bma_cubic_quad = paste(tmptitle, " up to the unconstrained cubic trend before, and quadratic trend after, ", breakPoint, ", with", sep=""),
                         bma_cubic_linear = paste(tmptitle, " up to the unconstrained cubic trend before, and linear trend after, ", breakPoint, ", with", sep=""),
                         indep_cubic_cubic = paste(tmptitle, " independent cubic trends before and after ", breakPoint, ", with", sep=""),
                         indep_cubic_quad = paste(tmptitle, " independent cubic trends before, and quadratic trends after, ", breakPoint, ", with", sep=""),
                         indep_cubic_linear = paste(tmptitle, " independent cubic trends before, and linear trends after, ", breakPoint, ", with", sep=""),
                         common_cubic_cubic = paste(tmptitle, " common cubic trend before and after ", breakPoint, ", with", sep=""),
                         common_cubic_quad = paste(tmptitle, " common cubic trend before, and quadratic trend after, ", breakPoint, ", with", sep=""),
                         common_cubic_linear = paste(tmptitle, " common cubic trend before, and linear trend after, ", breakPoint, ", with", sep=""),
                         bma_quad_quad = paste(tmptitle, " up to the unconstrained quadratic trend before and after ", breakPoint, ", with", sep=""),
                         bma_quad_linear = paste(tmptitle, " up to the unconstrained quadratic trend before, and linear trend after, ", breakPoint, ", with", sep=""),
                         indep_quad_quad =  paste(tmptitle, " independent quadratic trends before and after ", breakPoint, ", with", sep=""),
                         indep_quad_linear =  paste(tmptitle, " independent quadratic trends before, and linear trends after, ", breakPoint, ", with", sep=""),
                         common_quad_quad = paste(tmptitle, " common quadratic trend before and after ", breakPoint, ", with", sep=""),
                         common_quad_linear = paste(tmptitle, " common quadratic trend before, and linear trend after, ", breakPoint, ", with", sep=""),
                         bma_linear_linear = paste(tmptitle, " up to the unconstrained linear trend before and after ", breakPoint, ", with", sep=""),
                         indep_linear_linear =  paste(tmptitle, " independent linear trends before and after ", breakPoint, ", with", sep=""),
                         common_linear_linear = paste(tmptitle, " common linear trend before and after ", breakPoint, ", with", sep=""),
                         dropped_dropped = paste(tmptitle, " intercepts-only model before and after ", breakPoint, ", with", sep=""))
    } else {
      if (breakType == "level_break") {
        tmptitle <- switch(bayesModel,
                           bma_cubic = paste(tmptitle, " up to the unconstrained cubic trend, with level shifts in ", breakPoint, ", and", sep=""),
                           indep_cubic = paste(tmptitle, " independent cubic trends, with level shifts in ", breakPoint, ", and", sep=""),
                           common_cubic = paste(tmptitle, " common cubic trend, with level shift(s) in ", breakPoint, ", and", sep=""),
                           bma_quad = paste(tmptitle, " up to the unconstrained quadratic trend, with level shifts in ", breakPoint, ", and", sep=""),
                           indep_quad =  paste(tmptitle, " independent quadratic trends, with level shifts in ", breakPoint, ", and", sep=""),
                           common_quad = paste(tmptitle, " common quadratic trend, with level shift(s) in ", breakPoint, ", and", sep=""),
                           bma_linear = paste(tmptitle, " up to the unconstrained linear trend, with level shifts in ", breakPoint, ", and", sep=""),
                           indep_linear =  paste(tmptitle, " independent linear trends, with level shifts in ", breakPoint, ", and", sep=""),
                           common_linear = paste(tmptitle, " common linear trend, with level shift(s) in ", breakPoint, ", and", sep=""),
                           dropped = paste(tmptitle, " intercepts-only model, with level shift(s) in ", breakPoint, ", and", sep=""))
      } else {
        tmptitle <- switch(bayesModel,
                           bma_cubic = paste(tmptitle, "up to the unconstrained cubic trend, with"),
                           indep_cubic = paste(tmptitle, "independent cubic trends, with"),
                           common_cubic = paste(tmptitle, "common cubic trend, with"),
                           bma_quad = paste(tmptitle, "up to the unconstrained quadratic trend, with"),
                           indep_quad =  paste(tmptitle, "independent quadratic trends, with"),
                           common_quad = paste(tmptitle, "common quadratic trend, with"),
                           bma_linear = paste(tmptitle, "up to the unconstrained linear trend, with"),
                           indep_linear =  paste(tmptitle, "independent linear trends, with"),
                           common_linear = paste(tmptitle, "common linear trend, with"),
                           dropped = paste(tmptitle, "intercepts-only model, with"))
      }
    }
    
    tmptitle <- switch(ARmodel,
                         common_ar = paste(tmptitle, "common autoregressive parameters,"),
                         common_arh = paste(tmptitle, "common autocorrelation parameter,"),
                         indep_ar =  paste(tmptitle, "independent autoregressive parameters,"))
    
    if (randomVars) { 
      tmptitle <- paste(tmptitle, "and random") 
    } else {
      tmptitle <- paste(tmptitle, "and fixed") 
    }
    
    tmptitle <- paste(tmptitle, "sampling variances across", group, "groups, for the outcome", outcome)
    
    tmpfile <- paste(outloc, paste(out, "eMKFpredictions.html", sep="_"), sep="\\") 

    # Generate HTML file without printing to console    
    tmpstar <- capture.output(stargazer::stargazer(tmp, title=paste("eMKF predictions: ", tmptitle, ".", sep=""), 
                                                   type="html", out=tmpfile, summary=FALSE, rownames=FALSE, 
                                                   digits=pdigit, decimal.mark=".", digit.separator=""))
    
    # Generate text version for console
    stargazer::stargazer(tmp, title=paste("eMKF predictions: ", tmptitle, ".", sep=""), 
                         type="text", out=NULL, summary=FALSE, rownames=FALSE, 
                         digits=pdigit, decimal.mark=".", digit.separator="")

    cat(paste("An HTML version of this table is available from:\n", tmpfile, "\n", sep=""))
    
    rm(tmp, tmpstar, tmpfile, labelCols)
    
  }
  
  # Trim disparities dataset depending on requested reference group and prepare output table(s)
  if (comparedTo != "") {
    
    if (!is.element(tolower(comparedTo), c("min", "max", tolower(groupsInSortOrder)))) {
      cat("\n")
      if (tolower(comparedTo) == "unspecified") {
        cat("eMKF disparities: Reference ('comparedTo') for disparities was unspecified. ")
      } else {
        cat(paste("eMKF disparities: Reference ('comparedTo') for disparities was not 'MIN', 'MAX', or an existing", group, "name. "))
      }
      if (finalPrint) {
        cat(paste("An output table will not be generated; however, all comparisons will be saved to the dataset ", compareData, ".\n", sep=""))
      } else {
        cat(paste("All comparisons will be saved to the dataset ", compareData, ".\n", sep=""))
      }
    } else {
      if (tolower(comparedTo) == "min") {
        lastDisparities <- lastDisparities[lastDisparities[,"_rhsGroup"] == "MIN" | 
                                             is.element(lastDisparities[,"Type"], c("MAX", "AVG_excl_MIN", "MIN")), ]
        lastDisparities <- lastDisparities[, !is.element(names(lastDisparities), c("_rhsGroupIn", "_rhsGroup", paste(group, "(rhs)")))]
      } else {
        if (tolower(comparedTo) == "max") {
          lastDisparities <- lastDisparities[lastDisparities[,"_lhsGroup"] == "MAX" | 
                                               is.element(lastDisparities[,"Type"], c("MAX", "AVG_excl_MAX", "MIN")), ]
          lastDisparities <- lastDisparities[, !is.element(names(lastDisparities), c("_lhsGroupIn", "_lhsGroup", paste(group, "(lhs)")))]
        } else {
          lastDisparities <- lastDisparities[(tolower(lastDisparities[, paste(group, "(rhs)")]) == tolower(comparedTo)) & 
                                               lastDisparities[,"_lhsGroup"] != "MAX", ]
        }
      }
    }
    
    # Print disparities output table if requested
    if (finalPrint && is.element(tolower(comparedTo), c("min", "max", tolower(groupsInSortOrder)))) {
      if (by != "") {
        tmp1 <- lastDisparities[is.element(lastDisparities[, "Type"], diffMeasures), 
                               c(by, "Measure", time, "MKF estimate", "RMSE", "95% LL", "95% UL")]
        tmp2 <- lastDisparities[is.element(lastDisparities[, "Type"], ratioMeasures), 
                                c(by, "Measure", time, "MKF estimate", "RMSE", "95% LL", "95% UL")]
        tmp1[duplicated(tmp1[, by]), by] <- ""
        tmp2[duplicated(tmp2[, by]), by] <- ""
        names(tmp1)[2] <- "Difference Measure"
        names(tmp2)[2] <- "Ratio Measure"
      } else {
        tmp1 <- lastDisparities[is.element(lastDisparities[, "Type"], diffMeasures), 
                                c("Measure", time, "MKF estimate", "RMSE", "95% LL", "95% UL")]
        tmp2 <- lastDisparities[is.element(lastDisparities[, "Type"], ratioMeasures), 
                                c("Measure", time, "MKF estimate", "RMSE", "95% LL", "95% UL")]
        names(tmp1)[1] <- "Difference Measure"
        names(tmp2)[1] <- "Ratio Measure"
      }
      
      # Make sure 'pdigit' argument does not apply to time variable
      tmp1[, time] <- as.character(tmp1[, time])
      tmp2[, time] <- as.character(tmp2[, time])
      
      # Generate HTML file without printing to console
      tmpfile <- paste(outloc, paste(out, "eMKFdisparities.html", sep="_"), sep="\\")
      tmpstar <- capture.output(stargazer::stargazer(list(tmp1,tmp2), title=c(paste("eMKF disparities: ", tmptitle, ".", sep=""), ""),
                                                     type="html", out=tmpfile, summary=FALSE, rownames=FALSE,
                                                     digits=pdigit, decimal.mark=".", digit.separator=""))
  
      # Generate text version for console
      stargazer::stargazer(list(tmp1,tmp2), title=c(paste("eMKF disparities: ", tmptitle, ".", sep=""), ""),
                           type="text", out=NULL, summary=FALSE, rownames=FALSE,
                           digits=pdigit, decimal.mark=".", digit.separator="")
      
      cat(paste("An HTML version of these disparities tables is available from:\n", tmpfile, "\n", sep=""))
      
      rm(tmp1, tmp2, tmpstar, tmpfile)
      rm(tmptitle)
    }
    
  }
  
  # Assign to datasets in global environment and clean up
  assign(paste(out, "eMKFpredictions", sep="_"), lastTimepoint, envir = .GlobalEnv)
  rm(lastTimepoint)
  if (comparedTo != "") {
    assign(compareData, lastDisparities, envir = .GlobalEnv)
    rm(lastDisparities)
  }
  
  ###################################################################################################
  # Turn lists of posterior summaries into single data matrices for consistency with SAS eMKF macro #
  ###################################################################################################
  
  tmptab <- get(paste(out, "postSample_summary", sep="_"), envir = .GlobalEnv)[[1]]
  tmptab <- cbind("_rep"=rep(1, nrow(tmptab)), "Variable"=row.names(tmptab), tmptab) 
  if (nreps > 1) {
    for (r in 2:nreps) {
      tmptabr <- get(paste(out, "postSample_summary", sep="_"), envir = .GlobalEnv)[[r]]
      tmptabr <- cbind("_rep"=rep(r, nrow(tmptabr)), "Variable"=row.names(tmptabr), tmptabr) 
      tmptab <- rbind(tmptab, tmptabr)
      rm(tmptabr)
    }
  }
  row.names(tmptab) <- NULL
  etaarrVariables <- (substr(tmptab[, "Variable"], 1, 6) == 'etaarr')
  tmpdata <- tmptab[etaarrVariables, -c(1:2), drop=FALSE]
  
  # Also merge posterior estimates for true states into 'eMKFbayesdata' and write *.csv to file
  tmpdata <- cbind(get(paste(out, "eMKFbayesdata", sep="_"), envir = .GlobalEnv), tmpdata)
  write.csv(x = tmpdata, 
            file = paste(outloc, paste(out, "eMKFbayesdata.csv", sep="_"), sep="\\"),
            row.names = FALSE)
  assign(paste(out, "eMKFbayesdata", sep="_"), tmpdata, envir = .GlobalEnv)
  rm(tmpdata)
  rm(list=paste(out, "postSample_summary", sep="_"), envir = .GlobalEnv)
  assign(paste(out, "postSample_coefs", sep="_"), tmptab[!etaarrVariables,, drop=FALSE], envir = .GlobalEnv)
  rm(tmptab)
  
  if (fp > 1) {
    tmptab <- get(paste(out, "postSample_mflg", sep="_"), envir = .GlobalEnv)[[1]]
    tmptab <- cbind("_rep"=rep(1, nrow(tmptab)), "Model"=row.names(tmptab), tmptab) 
    if (nreps > 1) {
      for (r in 2:nreps) {
        tmptabr <- get(paste(out, "postSample_mflg", sep="_"), envir = .GlobalEnv)[[r]]
        tmptabr <- cbind("_rep"=rep(r, nrow(tmptabr)), "Model"=row.names(tmptabr), tmptabr) 
        tmptab <- rbind(tmptab, tmptabr)
        rm(tmptabr)
      }
    }
    row.names(tmptab) <- NULL
    rm(list=paste(out, "postSample_mflg", sep="_"), envir = .GlobalEnv)
    assign(paste(out, "postSample_mflg", sep="_"), tmptab, envir = .GlobalEnv)
    rm(tmptab)
  }
  
  tmptab <- get(paste(out, "postSample_GR", sep="_"), envir = .GlobalEnv)[[1]]
  tmptab <- cbind("_rep"=rep(1, nrow(tmptab)), "Variable"=row.names(tmptab), tmptab) 
  if (nreps > 1) {
    for (r in 2:nreps) {
      tmptabr <- get(paste(out, "postSample_GR", sep="_"), envir = .GlobalEnv)[[r]]
      tmptabr <- cbind("_rep"=rep(r, nrow(tmptabr)), "Variable"=row.names(tmptabr), tmptabr) 
      tmptab <- rbind(tmptab, tmptabr)
      rm(tmptabr)
    }
  }
  row.names(tmptab) <- NULL
  rm(list=paste(out, "postSample_GR", sep="_"), envir = .GlobalEnv)
  assign(paste(out, "postSample_GR", sep="_"), tmptab, envir = .GlobalEnv)
  rm(tmptab)
  
  ############
  # Clean-up #
  ############
  
  # Clean user environment 
  rm(list = ls(pattern = "^str.", envir = .GlobalEnv), envir = .GlobalEnv)
  rm(list = ls(pattern = "^sampler_eMKF", envir = .GlobalEnv), envir = .GlobalEnv)
  rm(list = ls(pattern = "^sampler_reflective", envir = .GlobalEnv), envir = .GlobalEnv)
  rm(list = ls(pattern = "^rmixbeta", envir = .GlobalEnv), envir = .GlobalEnv)
  rm(list = ls(pattern = "^dmixbeta", envir = .GlobalEnv), envir = .GlobalEnv)
  rm(list = ls(pattern = "^ar1_", envir = .GlobalEnv), envir = .GlobalEnv)
  rm(list = ls(pattern = "^eMKF", envir = .GlobalEnv), envir = .GlobalEnv)
  rm(rankNorm, envir = .GlobalEnv)
  rm(objsToList, envir = .GlobalEnv)
  rm(polyDesignMatrix, envir = .GlobalEnv)
  rm(list = ls(pattern = "^bayesfit", envir = .GlobalEnv), envir = .GlobalEnv)
  rm(list = ls(pattern = "^bayesBMA", envir = .GlobalEnv), envir = .GlobalEnv)

  # Change NIMBLE option to suppress de-registration message irrespective of modelPrint
  nimbleOptions(verbose = FALSE)
  
  # De-register custom eMKF distributions
  if (fp > 1) {
    deregisterDistributions(c('dmixbeta3', 'dmixbeta2', 'dmixbeta1'), userEnv = .GlobalEnv)
  }

  # Remove uncompiled NIMBLE model and MCMC
  rm(list = ls(pattern = "^Rnmbl_"))

  # Remove compiled NIMBLE model and MCMC
  rm(Cnmbl_mcmc)

  # Reset NIMBLE options to what they were before Rmkf was called
  nimbleOptions(verbose = vprint)
  
  # Function to reset finalizer when R session has not yet ended
  resetCompiledCode <- function(ee) {
    print("This message should not appear prior to the end of this R session!")
  }
  
  # Function to clean-up DLLs and related C++ files on R session exit
  clearCompiledCode <- function(ee) {

    # Unload and remove shared libraries and C++ files related to model code
    DLLs <- Sys.glob(file.path(dllloc, "eMKF_Rnmbl_code*.dll"))
    DLLs <- gsub("\\\\", "/", DLLs)
    DLLs <- gsub("//", "/", DLLs)
    dll_list <- getLoadedDLLs()
    for (l in seq_along(DLLs)) {
      for (dll_info in dll_list) {
        if (dll_info[["dynamicLookup"]] == TRUE) { 
          if (dll_info[["path"]] == DLLs[l]) { dyn.unload(DLLs[l]) }
        }
      }
      file.remove(DLLs[l])
    }
    file.remove(Sys.glob(file.path(dllloc, "eMKF_Rnmbl_code*")))
    
    # Unload and remove shared libraries and C++ files related to mcmc code
    DLLs <- Sys.glob(file.path(dllloc, "eMKF_MCMC*.dll"))
    DLLs <- gsub("\\\\", "/", DLLs)
    DLLs <- gsub("//", "/", DLLs)
    dll_list <- getLoadedDLLs()
    for (l in seq_along(DLLs)) {
      for (dll_info in dll_list) {
        if (dll_info[["dynamicLookup"]] == TRUE) { 
          if (dll_info[["path"]] == DLLs[l]) { dyn.unload(DLLs[l]) }
        }
      }
      file.remove(DLLs[l])
    }
    file.remove(Sys.glob(file.path(dllloc, "eMKF_MCMC*")))
    
    # Unload and remove shared libraries and C++ files related to dynamic registrations
    DLLs <- Sys.glob(file.path(dllloc, "dynamicRegistrations*.dll"))
    DLLs <- gsub("\\\\", "/", DLLs)
    DLLs <- gsub("//", "/", DLLs)
    dll_list <- getLoadedDLLs()
    for (l in seq_along(DLLs)) {
      for (dll_info in dll_list) {
        if (dll_info[["dynamicLookup"]] == TRUE) { 
          if (dll_info[["path"]] == DLLs[l]) { dyn.unload(DLLs[l]) }
        }
      }
      file.remove(DLLs[l])
    }
    file.remove(Sys.glob(file.path(dllloc, "dynamicRegistrations*")))
    file.remove(Sys.glob(file.path(dllloc, "MakeVars.win")))
    
  }
  
  gg0 <- function(xx) {
    ee <- globalenv()
    reg.finalizer(ee, resetCompiledCode, onexit = TRUE)
  }
  
  gg <- function(xx) {
    ee <- globalenv()
    reg.finalizer(ee, clearCompiledCode, onexit = TRUE)
  }
  
  # Final user-facing messages
  cat("\n")
  message("Rmkf concluded\n")
  message(paste(" - Model-based predictions at the last timepoint are in dataset", 
                paste("'", paste(out, "eMKFpredictions", sep="_"), "'", sep = "")))
  message(paste("   -- The full suite of predictions (all data points) is in", 
                paste("'", paste(out, "eMKFbayesdata", sep="_"), "'", sep = "")))
  message(paste("   -- A comma-delimited version of the latter dataset is also available from:\n     ", 
                paste(outloc, paste(out, "eMKFbayesdata.csv", sep="_"), sep="\\")))
  if (comparedTo != "") {
    message(paste(" - Model-based estimates of disparities at the last timepoint are in", 
                  paste("'", paste(out, "eMKFdisparities", sep="_"), "'", sep = "")))
  }
  if (fp > 1) {
    message(paste(" - Posterior frequencies (BMA weights) for the model indicators are in", 
                  paste("'", paste(out, "postSample_mflg", sep="_"), "'", sep = "")))
  }
  message(paste(" - Posterior summaries for model coefficients are in", 
                paste("'", paste(out, "postSample_coefs", sep="_"), "'", sep = "")))
  message(paste("\n - Gelman-Rubin split R-hat diagnostics are in", paste("'", paste(out, "postSample_GR", sep="_"), "'", sep = "")))
  if (mcmclog) {
    message(paste(" - Full posterior samples are in", paste("'", paste(out, "postSample", sep="_"), "'", sep = "")))
  }
  message("\n**Notes on Rmkf nomenclature**\n")
  message(paste(strwrap(paste("Users who wish to examine MCMC diagnostics or posterior summaries for model",
                "coefficients should reference the dataset", paste("'", paste(out, "eMKFbayesdata", sep="_"), "'", sep = ""),
                "for a crosswalk of the internal Rmkf variables '_rep', '_group', and '_time' with the corresponding variables", 
                "in the user-supplied data.  Group-specific model coefficients (intercepts 'ag'; linear 'b1g',", 
                "quadratic 'b2g', and cubic 'b3g' regression coefficients; variances 'varr'; and AR(1) parameters", 
                "'rhoarr' and 'tausqarr', where applicable) are indexed by '_group'.  True state predictions 'etaarr'",
                "are indexed by '_group' and '_time'; for example, the 'etaarr[(i-1)*n+1]' through 'etaarr[i*n]' are the",
                "n ordered timepoints for '_group' i.  Notation for model parameters that are shared across groups is as follows:",
                "'rho' and 'tausq' for the AR(1) parameters when they are shared across groups; 'mpsi' and 'spsi' for the",
                "mean and standard deviation of the normal hyperprior of the group-specific 'psiarr = ln[(1+rhoarr)/(1-rhoarr)]',",
                "when applicable; 'b1' for the linear, 'b2' for the quadratic, and 'b3' for the cubic regression coefficients", 
                "when shared across groups; and 'flg' for the model indicator flag in the BMA cases.  In the trend-break cases,",
                "labels for regression coefficients and model indicator flags, where applicable, include the prefix 's1' or 's2'", 
                "for segment-specific parameters.")), collapse = "\n"))
  
  # Register finalizer(s) and perform garbage collection
  cat("\n")
  message(paste("Compiled DLLs and C++ code are available from:\n", dllloc, sep=""))
  gg0()
  invisible(gc())
  if (clearDLLs) { 
    message("  [Note] By default, all DLLs and related C++ files will be removed when this R session exits.")
    gg() 
  }
  message("----------------------------------------------------------------------------------------------\n")
  
  invisible(return(NULL))
  
} # Rmkf

######################
# citation('nimble') #
######################
#
# de Valpine, P., D. Turek, C.J. Paciorek, C. Anderson-Bergman, D. Temple Lang, and R. Bodik. 2017. Programming with models: 
# writing statistical algorithms for general model structures with NIMBLE. Journal of Computational and Graphical Statistics 
# 26: 403-413. <DOI:10.1080/10618600.2016.1172487>.
# 
# de Valpine P, Paciorek C, Turek D, Michaud N, Anderson-Bergman C, Obermeyer F, Wehrhahn Cortes C, Rodrguez A, Temple Lang D, 
# Paganin S (2025). _NIMBLE: MCMC, Particle Filtering, and Programmable Hierarchical Modeling_. doi:10.5281/zenodo.1211190 
# <https://doi.org/10.5281/zenodo.1211190>, R package version 1.4.0, <https://cran.r-project.org/package=nimble>.
# 
# de Valpine P, Paciorek C, Turek D, Michaud N, Anderson-Bergman C, Obermeyer F, Wehrhahn Cortes C, Rodrguez A, Temple Lang D, 
# Paganin S (2025). _NIMBLE User Manual_. doi:10.5281/zenodo.1211190 <https://doi.org/10.5281/zenodo.1211190>, R package manual 
# version 1.4.0, <https://r-nimble.org>.
#
###################
# eMKF references #
###################
#
# Talih M, Patel P, Rossen LM. An R-NIMBLE implementation of the enhanced modified Kalman filter (eMKF) tool for small domain 
# estimation (version 2.4 2026-01-30). National Center for Health Statistics. 2026. https://github.com/CDCgov/eMKF.
#
# Talih M, Patel P, Rossen LM. The enhanced modified Kalman filter (eMKF) tool for small domain estimation 
# (version 2.4 2026-01-30). National Center for Health Statistics. 2026. https://github.com/CDCgov/eMKF. 
# 
# Talih M, Rossen LM, Patel P, Earp M, Parker JD. The enhanced modified Kalman filter (eMKF) tool for small domain estimation 
# (version 1.4 2024-08-10). National Center for Health Statistics. 2024. https://github.com/CDCgov/eMKF.
# 
# Talih M, Rossen L, Patel P, Earp M, Parker J. Technical guidance for using the modified Kalman filter in small domain  
# estimation at the National Center for Health Statistics. Vital Health Stat 2(209). 2024. DOI: 10.15620/cdc/157496.
# 
# Rossen L, Talih M, Patel P, Earp M, Parker J. Evaluation of a modified Kalman filter approach for estimating health outcomes 
# in small subpopulations. Vital Health Stat 2(208). 2024. DOI: 10.15620/cdc/157497.
#

